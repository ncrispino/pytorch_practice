{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0] # Celsius\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4] # unknown\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 9, 0, 6, 1, 3, 4, 5, 7]), tensor([ 8, 10]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples) # random permutation of ints from 0 to n_samples - 1\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t_u = t_u[train_indices].unsqueeze(1)\n",
    "train_t_c = t_c[train_indices].unsqueeze(1)\n",
    "\n",
    "val_t_u = t_u[val_indices].unsqueeze(1)\n",
    "val_t_c = t_c[val_indices].unsqueeze(1)\n",
    "\n",
    "train_t_un = 0.1 * train_t_u\n",
    "val_t_un = 0.1 * val_t_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks\n",
    "Note: saturated when changes in input result in little/no changes to output.\n",
    "\n",
    "PyTorch has building blocks called modules. A PyTorch module is a class derived from nn.Module and can have one or more Parameter instances as attributes, which are tensors whose value is optimized during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.9563]],\n",
       "\n",
       "        [[-6.9069]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Linear is a subclass of nn.Module, which means it has a __call__ method defined. Use it.\n",
    "linear_model = nn.Linear(1, 1) # args are input size, output size, bias(=True default)\n",
    "linear_model(val_t_un.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.9753]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.2360], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2112], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(1)\n",
    "linear_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any module in ```nn``` written to produce output for a batch of multiple inputs. The first dimension of the input is the batch dim.\n",
    "\n",
    "Need tensor of size B x Nin where B is size of batch and Nin is number of inputs. For example, for 10 samples need 10 x 1 input.\n",
    "\n",
    "Output is tensor of size B x Nout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2112],\n",
       "        [-1.2112],\n",
       "        [-1.2112],\n",
       "        [-1.2112],\n",
       "        [-1.2112],\n",
       "        [-1.2112],\n",
       "        [-1.2112],\n",
       "        [-1.2112],\n",
       "        [-1.2112],\n",
       "        [-1.2112]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(10, 1)\n",
    "linear_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change prev input to be of desired input form B x Nin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([11, 1]), torch.Size([11, 1]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_c = t_c.unsqueeze(1)\n",
    "t_u = t_u.unsqueeze(1)\n",
    "t_c.shape, t_u.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(1, 1)\n",
    "optimizer = optim.SGD(linear_model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.4801]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.6367], requires_grad=True)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(linear_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create training loop with model passed in instead of individual params. Also use loss functions from ```torch.nn``` directly, which are subclasses of ```nn.Module``` so need to be created as an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val, t_c_train, t_c_val, print_vals=True):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p_train = model(t_u_train)\n",
    "        train_loss = loss_fn(t_p_train, t_c_train)\n",
    "        \n",
    "        t_p_val = model(t_u_val)\n",
    "        val_loss = loss_fn(t_p_val, t_c_val)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if print_vals:\n",
    "            if (epoch == 1 or epoch % 1000 == 0) and (epoch != n_epochs):\n",
    "                print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                      f\" Validation loss {val_loss.item():.4f}\")\n",
    "    print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "              f\" Validation loss {val_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 310.1073, Validation loss 416.4771\n",
      "Epoch 1000, Training loss 2.9976, Validation loss 7.4397\n",
      "Epoch 2000, Training loss 2.4491, Validation loss 5.7089\n",
      "Epoch 3000, Training loss 2.4376, Validation loss 5.4824\n",
      "Parameter containing:\n",
      "tensor([[5.2247]], requires_grad=True) Parameter containing:\n",
      "tensor([-16.4597], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "linear_model = nn.Linear(1, 1)\n",
    "optimizer = optim.SGD(linear_model.parameters(), lr=1e-2)\n",
    "\n",
    "training_loop(3000, optimizer, linear_model, nn.MSELoss(), train_t_un, val_t_un, train_t_c, val_t_c)\n",
    "\n",
    "print(linear_model.weight, linear_model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending model to have one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=13, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=13, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model = nn.Sequential(nn.Linear(1, 13), \n",
    "                          nn.Tanh(), \n",
    "                          nn.Linear(13, 1))\n",
    "seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([13, 1]), torch.Size([13]), torch.Size([1, 13]), torch.Size([1])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[param.shape for param in seq_model.parameters()] # weight and bias from both first and second linear modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight torch.Size([13, 1])\n",
      "0.bias torch.Size([13])\n",
      "2.weight torch.Size([1, 13])\n",
      "2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in seq_model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also explicitly name layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (hidden_linear): Linear(in_features=1, out_features=8, bias=True)\n",
       "  (hidden_activation): Tanh()\n",
       "  (output_linear): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "seq_model = nn.Sequential(OrderedDict([('hidden_linear', nn.Linear(1, 8)),\n",
    "                                       ('hidden_activation', nn.Tanh()),\n",
    "                                       ('output_linear', nn.Linear(8, 1))\n",
    "                                      ]))\n",
    "seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_linear.weight torch.Size([8, 1])\n",
      "hidden_linear.bias torch.Size([8])\n",
      "output_linear.weight torch.Size([1, 8])\n",
      "output_linear.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in seq_model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1219, -0.0828,  0.2277,  0.0074, -0.1700,  0.2736,  0.3079, -0.1365]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model.output_linear.weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0998], requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model.output_linear.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training loop with sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 4.1099, Validation loss 10.6521\n",
      "Epoch 1000, Training loss 3.5550, Validation loss 8.6022\n",
      "Epoch 2000, Training loss 3.3147, Validation loss 7.4831\n",
      "Epoch 3000, Training loss 3.1881, Validation loss 6.8195\n",
      "Epoch 4000, Training loss 3.1079, Validation loss 6.3686\n",
      "Epoch 5000, Training loss 3.0483, Validation loss 6.0199\n",
      "output tensor([[ 9.2495],\n",
      "        [22.2165]], grad_fn=<AddmmBackward0>)\n",
      "answer tensor([[ 6.],\n",
      "        [21.]])\n",
      "hidden tensor([[ 1.1032e-05],\n",
      "        [-3.7549e-02],\n",
      "        [ 4.5048e-03],\n",
      "        [ 1.6936e-04],\n",
      "        [ 1.2288e-06],\n",
      "        [ 3.2008e-04],\n",
      "        [-2.3273e-05],\n",
      "        [ 9.0582e-03]])\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(seq_model.parameters(), 1e-3)\n",
    "\n",
    "training_loop(5000, optimizer, seq_model, nn.MSELoss(), train_t_un, val_t_un, train_t_c, val_t_c)\n",
    "\n",
    "print('output', seq_model(val_t_un))\n",
    "print('answer', val_t_c)\n",
    "print('hidden', seq_model.hidden_linear.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x264c37bed48>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "t_range = torch.arange(20, 90).unsqueeze(1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.set(xlabel=\"Fahrenheit\", ylabel=\"Celsius\")\n",
    "ax.plot(t_u.numpy(), t_c.numpy(), 'o') # correct labels\n",
    "ax.plot(t_range.numpy(), seq_model(0.1 * t_range).detach().numpy(), 'c-') # whole curve (behavior between samples)\n",
    "ax.plot(t_u.numpy(), seq_model(0.1 * t_u).detach().numpy(), 'kx') # fitted values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "**1. Changing number of hidden neurons and learning rate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(n_neurons, learning_rate=1e-3, print_vals=False, print_weights=False):\n",
    "    seq_model = nn.Sequential(nn.Linear(1, n_neurons), \n",
    "                              nn.Tanh(), \n",
    "                              nn.Linear(n_neurons, 1))\n",
    "\n",
    "    optimizer = optim.SGD(seq_model.parameters(), learning_rate)\n",
    "\n",
    "    training_loop(5000, optimizer, seq_model, nn.MSELoss(), train_t_un, val_t_un, train_t_c, val_t_c, print_vals)\n",
    "    \n",
    "    print('output', seq_model(val_t_un))\n",
    "    if print_weights:        \n",
    "        print('answer', val_t_c)\n",
    "        print('hidden', seq_model[2].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 10.0914, Validation loss 1.8650\n",
      "output tensor([[ 6.1014],\n",
      "        [19.0175]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "2 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 2.3857, Validation loss 2.4569\n",
      "output tensor([[ 7.8936],\n",
      "        [20.8368]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "3 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 2.0353, Validation loss 1.9894\n",
      "output tensor([[ 7.9727],\n",
      "        [20.7040]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "4 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 2.0147, Validation loss 1.5253\n",
      "output tensor([[ 7.7362],\n",
      "        [20.8093]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "5 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.9676, Validation loss 1.6000\n",
      "output tensor([[ 7.7601],\n",
      "        [20.6800]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "6 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.9258, Validation loss 1.8986\n",
      "output tensor([[ 7.9039],\n",
      "        [20.5842]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "7 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.9294, Validation loss 1.6417\n",
      "output tensor([[ 7.7704],\n",
      "        [20.6137]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "8 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.9053, Validation loss 1.8057\n",
      "output tensor([[ 7.8296],\n",
      "        [20.4859]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "9 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.9191, Validation loss 2.1187\n",
      "output tensor([[ 7.9091],\n",
      "        [20.2297]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "10 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.8925, Validation loss 2.0165\n",
      "output tensor([[ 7.8774],\n",
      "        [20.2867]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for n in range(1, 11):\n",
    "    print(f'\\n{n} neurons in hidden layer:\\n')\n",
    "    run_model(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.9013, Validation loss 2.2206\n",
      "output tensor([[ 7.9991],\n",
      "        [20.3329]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "20 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.9983, Validation loss 2.5862\n",
      "output tensor([[ 8.2691],\n",
      "        [20.4212]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "30 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.9625, Validation loss 2.9274\n",
      "output tensor([[ 7.7849],\n",
      "        [19.4174]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "40 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.8491, Validation loss 2.8336\n",
      "output tensor([[ 8.0515],\n",
      "        [19.7934]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "50 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 2.0364, Validation loss 3.3962\n",
      "output tensor([[ 7.7974],\n",
      "        [19.1674]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "60 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.8564, Validation loss 3.1094\n",
      "output tensor([[ 8.1321],\n",
      "        [19.7067]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "70 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.8327, Validation loss 2.9874\n",
      "output tensor([[ 8.0841],\n",
      "        [19.7228]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "80 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.8466, Validation loss 3.1803\n",
      "output tensor([[ 8.1468],\n",
      "        [19.6765]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "90 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.8524, Validation loss 3.2794\n",
      "output tensor([[ 8.1771],\n",
      "        [19.6513]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "100 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.8584, Validation loss 3.3605\n",
      "output tensor([[ 8.2014],\n",
      "        [19.6309]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for n in range(10, 110, 10):\n",
    "    print(f'\\n{n} neurons in hidden layer:\\n')\n",
    "    run_model(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in this case increasing the number of hidden neurons increases validation loss in general. Now change the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning Rate: 0.1\n",
      "Epoch 5000, Training loss nan, Validation loss nan\n",
      "output tensor([[nan],\n",
      "        [nan]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.010000000000000002\n",
      "Epoch 5000, Training loss 1.4629, Validation loss 8.3590\n",
      "output tensor([[ 8.1255],\n",
      "        [17.8401]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.0010000000000000002\n",
      "Epoch 5000, Training loss 1.8595, Validation loss 2.4561\n",
      "output tensor([[ 8.0162],\n",
      "        [20.0795]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.00010000000000000002\n",
      "Epoch 5000, Training loss 13.4294, Validation loss 17.2636\n",
      "output tensor([[ 9.3713],\n",
      "        [16.1887]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 1.0000000000000003e-05\n",
      "Epoch 5000, Training loss 78.2484, Validation loss 91.7353\n",
      "output tensor([[7.2144],\n",
      "        [7.5102]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 1.0000000000000004e-06\n",
      "Epoch 5000, Training loss 170.8897, Validation loss 231.6279\n",
      "output tensor([[0.2423],\n",
      "        [0.2614]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 1.0000000000000004e-07\n",
      "Epoch 5000, Training loss 164.3224, Validation loss 222.4874\n",
      "output tensor([[0.5972],\n",
      "        [0.6092]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 1.0000000000000005e-08\n",
      "Epoch 5000, Training loss 170.1628, Validation loss 230.5421\n",
      "output tensor([[0.2244],\n",
      "        [0.3185]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 1.0000000000000005e-09\n",
      "Epoch 5000, Training loss 192.3890, Validation loss 260.5979\n",
      "output tensor([[-0.8239],\n",
      "        [-0.7860]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 1.0000000000000006e-10\n",
      "Epoch 5000, Training loss 164.3827, Validation loss 222.4956\n",
      "output tensor([[0.5651],\n",
      "        [0.6173]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for lr in [0.1**i for i in range(1, 11)]:\n",
    "    print(f'\\nLearning Rate: {lr}')\n",
    "    run_model(13, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation loss is generally increasing as the learning rate decreases (past a certain point). Let's try learning rates around 1e-3, which seems to work best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning Rate: 0.001\n",
      "Epoch 5000, Training loss 1.8702, Validation loss 2.5303\n",
      "output tensor([[ 8.0367],\n",
      "        [20.0447]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.002\n",
      "Epoch 5000, Training loss 1.8598, Validation loss 3.0534\n",
      "output tensor([[ 8.3240],\n",
      "        [20.1154]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.003\n",
      "Epoch 5000, Training loss 1.8531, Validation loss 4.4645\n",
      "output tensor([[ 8.0806],\n",
      "        [18.8226]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.004\n",
      "Epoch 5000, Training loss 1.8412, Validation loss 4.6165\n",
      "output tensor([[ 8.0424],\n",
      "        [18.5552]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.005\n",
      "Epoch 5000, Training loss 1.8509, Validation loss 4.1345\n",
      "output tensor([[ 7.7810],\n",
      "        [18.5680]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.006\n",
      "Epoch 5000, Training loss 1.7305, Validation loss 4.7539\n",
      "output tensor([[ 7.8901],\n",
      "        [18.2598]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.007\n",
      "Epoch 5000, Training loss 1.7572, Validation loss 5.7353\n",
      "output tensor([[ 8.5758],\n",
      "        [19.2062]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.008\n",
      "Epoch 5000, Training loss 1.6295, Validation loss 7.1129\n",
      "output tensor([[ 8.3004],\n",
      "        [18.6318]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.009000000000000001\n",
      "Epoch 5000, Training loss 1.5338, Validation loss 7.4839\n",
      "output tensor([[ 8.1446],\n",
      "        [18.2910]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for lr in [1e-3*i for i in range(1, 10)]:\n",
    "    print(f'\\nLearning Rate: {lr}')\n",
    "    run_model(13, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning Rate: 0.0001\n",
      "Epoch 5000, Training loss 10.9058, Validation loss 13.0131\n",
      "output tensor([[ 9.1022],\n",
      "        [16.9512]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.0002\n",
      "Epoch 5000, Training loss 4.7291, Validation loss 3.5992\n",
      "output tensor([[ 8.0446],\n",
      "        [19.2638]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.00030000000000000003\n",
      "Epoch 5000, Training loss 3.2471, Validation loss 4.2588\n",
      "output tensor([[ 8.4801],\n",
      "        [19.4621]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.0004\n",
      "Epoch 5000, Training loss 2.4046, Validation loss 2.6131\n",
      "output tensor([[ 8.0476],\n",
      "        [19.9837]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.0005\n",
      "Epoch 5000, Training loss 2.1379, Validation loss 2.1915\n",
      "output tensor([[ 7.9133],\n",
      "        [20.1504]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.0006000000000000001\n",
      "Epoch 5000, Training loss 2.0220, Validation loss 2.4350\n",
      "output tensor([[ 8.0090],\n",
      "        [20.0870]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.0007\n",
      "Epoch 5000, Training loss 1.9636, Validation loss 2.0909\n",
      "output tensor([[ 7.8723],\n",
      "        [20.1778]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.0008\n",
      "Epoch 5000, Training loss 1.8986, Validation loss 2.0758\n",
      "output tensor([[ 7.8899],\n",
      "        [20.2384]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.0009000000000000001\n",
      "Epoch 5000, Training loss 1.9157, Validation loss 2.2449\n",
      "output tensor([[ 7.9038],\n",
      "        [20.0826]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for lr in [1e-4*i for i in range(1, 10)]:\n",
    "    print(f'\\nLearning Rate: {lr}')\n",
    "    run_model(13, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it appears the best learning rate according to validation loss is around 0.0008."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Create new model for wine data. Copying normalization from Ch.4 notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "wineq_pd = pd.read_csv('data/p1ch4/tabular-wine/winequality-white.csv', sep=';')\n",
    "wineq = torch.from_numpy(wineq_pd.to_numpy(dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4898, 12]), torch.float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wineq.shape, wineq.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now have a floating point tensor with all the columns including the last, which refers to the quality score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7.0000,  0.2700,  0.3600,  ...,  3.0000,  0.4500,  8.8000],\n",
       "         [ 6.3000,  0.3000,  0.3400,  ...,  3.3000,  0.4900,  9.5000],\n",
       "         [ 8.1000,  0.2800,  0.4000,  ...,  3.2600,  0.4400, 10.1000],\n",
       "         ...,\n",
       "         [ 6.5000,  0.2400,  0.1900,  ...,  2.9900,  0.4600,  9.4000],\n",
       "         [ 5.5000,  0.2900,  0.3000,  ...,  3.3400,  0.3800, 12.8000],\n",
       "         [ 6.0000,  0.2100,  0.3800,  ...,  3.2600,  0.3200, 11.8000]]),\n",
       " torch.Size([4898, 11]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = wineq[:, :-1] # excluding score\n",
    "data, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6., 6., 6.,  ..., 6., 7., 6.]), torch.Size([4898]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = wineq[:, -1]\n",
    "target, target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6., 6., 6.,  ..., 6., 7., 6.])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_onehot = torch.zeros(target.shape[0], 10)\n",
    "target_onehot.scatter_(1, target.unsqueeze(1).long(), 1.0) # for each row take idx corresp to element in target and set to 1.0 in target_onehot\n",
    "# first arg is dim along which the following two args are specified, \n",
    "# second arg is a column tensor indicating idx of element to scatter,\n",
    "# third arg is the elements/single scalar to scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.8548e+00, 2.7824e-01, 3.3419e-01, 6.3914e+00, 4.5772e-02, 3.5308e+01,\n",
       "        1.3836e+02, 9.9403e-01, 3.1883e+00, 4.8985e-01, 1.0514e+01])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mean = torch.mean(data, dim=0)\n",
    "data_mean # gets mean in every column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.1211e-01, 1.0160e-02, 1.4646e-02, 2.5726e+01, 4.7733e-04, 2.8924e+02,\n",
       "        1.8061e+03, 8.9455e-06, 2.2801e-02, 1.3025e-02, 1.5144e+00])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_var = torch.var(data, dim=0)\n",
    "data_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normalized = (data - data_mean)/torch.sqrt(data_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now back to new analysis. Use normalized data and one-hot encoding of target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4898, 11]), torch.Size([4898, 10]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_normalized.shape, target_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = data_normalized.shape[1]\n",
    "n_outputs = target_onehot.shape[1]\n",
    "n_hidden_neurons = 10\n",
    "seq_model = nn.Sequential(nn.Linear(n_inputs, n_hidden_neurons),\n",
    "                          nn.Tanh(),\n",
    "                          nn.Linear(n_hidden_neurons, n_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.random_split()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learn",
   "language": "python",
   "name": "deep_learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
