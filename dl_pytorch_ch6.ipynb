{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0] # Celsius\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4] # unknown\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1,  6,  9,  8,  5,  3,  7,  0, 10]), tensor([2, 4]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples) # random permutation of ints from 0 to n_samples - 1\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t_u = t_u[train_indices].unsqueeze(1)\n",
    "train_t_c = t_c[train_indices].unsqueeze(1)\n",
    "\n",
    "val_t_u = t_u[val_indices].unsqueeze(1)\n",
    "val_t_c = t_c[val_indices].unsqueeze(1)\n",
    "\n",
    "train_t_un = 0.1 * train_t_u\n",
    "val_t_un = 0.1 * val_t_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks\n",
    "Note: saturated when changes in input result in little/no changes to output.\n",
    "\n",
    "PyTorch has building blocks called modules. A PyTorch module is a class derived from nn.Module and can have one or more Parameter instances as attributes, which are tensors whose value is optimized during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.1361]],\n",
       "\n",
       "        [[-3.9751]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Linear is a subclass of nn.Module, which means it has a __call__ method defined. Use it.\n",
    "linear_model = nn.Linear(1, 1) # args are input size, output size, bias(=True default)\n",
    "linear_model(val_t_un.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.8473]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.7953], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0520], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(1)\n",
    "linear_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any module in ```nn``` written to produce output for a batch of multiple inputs. The first dimension of the input is the batch dim.\n",
    "\n",
    "Need tensor of size B x Nin where B is size of batch and Nin is number of inputs. For example, for 10 samples need 10 x 1 input.\n",
    "\n",
    "Output is tensor of size B x Nout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0520],\n",
       "        [-0.0520],\n",
       "        [-0.0520],\n",
       "        [-0.0520],\n",
       "        [-0.0520],\n",
       "        [-0.0520],\n",
       "        [-0.0520],\n",
       "        [-0.0520],\n",
       "        [-0.0520],\n",
       "        [-0.0520]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(10, 1)\n",
    "linear_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change prev input to be of desired input form B x Nin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([11, 1]), torch.Size([11, 1]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_c = t_c.unsqueeze(1)\n",
    "t_u = t_u.unsqueeze(1)\n",
    "t_c.shape, t_u.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(1, 1)\n",
    "optimizer = optim.SGD(linear_model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.4061]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2528], requires_grad=True)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(linear_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create training loop with model passed in instead of individual params. Also use loss functions from ```torch.nn``` directly, which are subclasses of ```nn.Module``` so need to be created as an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val, t_c_train, t_c_val, print_vals=True):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p_train = model(t_u_train)\n",
    "        train_loss = loss_fn(t_p_train, t_c_train)\n",
    "        \n",
    "        t_p_val = model(t_u_val)\n",
    "        val_loss = loss_fn(t_p_val, t_c_val)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if print_vals:\n",
    "            if (epoch == 1 or epoch % 1000 == 0) and (epoch != n_epochs):\n",
    "                print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                      f\" Validation loss {val_loss.item():.4f}\")\n",
    "    print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "              f\" Validation loss {val_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 90.9806, Validation loss 62.7878\n",
      "Epoch 1000, Training loss 3.5465, Validation loss 2.5739\n",
      "Epoch 2000, Training loss 3.0434, Validation loss 2.5045\n",
      "Epoch 3000, Training loss 3.0355, Validation loss 2.4961\n",
      "Parameter containing:\n",
      "tensor([[5.3719]], requires_grad=True) Parameter containing:\n",
      "tensor([-17.2280], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "linear_model = nn.Linear(1, 1)\n",
    "optimizer = optim.SGD(linear_model.parameters(), lr=1e-2)\n",
    "\n",
    "training_loop(3000, optimizer, linear_model, nn.MSELoss(), train_t_un, val_t_un, train_t_c, val_t_c)\n",
    "\n",
    "print(linear_model.weight, linear_model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending model to have one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=13, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=13, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model = nn.Sequential(nn.Linear(1, 13), \n",
    "                          nn.Tanh(), \n",
    "                          nn.Linear(13, 1))\n",
    "seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([13, 1]), torch.Size([13]), torch.Size([1, 13]), torch.Size([1])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[param.shape for param in seq_model.parameters()] # weight and bias from both first and second linear modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight torch.Size([13, 1])\n",
      "0.bias torch.Size([13])\n",
      "2.weight torch.Size([1, 13])\n",
      "2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in seq_model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also explicitly name layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (hidden_linear): Linear(in_features=1, out_features=8, bias=True)\n",
       "  (hidden_activation): Tanh()\n",
       "  (output_linear): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "seq_model = nn.Sequential(OrderedDict([('hidden_linear', nn.Linear(1, 8)),\n",
    "                                       ('hidden_activation', nn.Tanh()),\n",
    "                                       ('output_linear', nn.Linear(8, 1))\n",
    "                                      ]))\n",
    "seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_linear.weight torch.Size([8, 1])\n",
      "hidden_linear.bias torch.Size([8])\n",
      "output_linear.weight torch.Size([1, 8])\n",
      "output_linear.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in seq_model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2599,  0.2669,  0.1643,  0.3208,  0.0530,  0.0124,  0.1690, -0.0409]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model.output_linear.weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.2942], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model.output_linear.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training loop with sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 203.1317, Validation loss 189.2796\n",
      "Epoch 1000, Training loss 4.7918, Validation loss 2.5514\n",
      "Epoch 2000, Training loss 3.6606, Validation loss 4.9500\n",
      "Epoch 3000, Training loss 2.1354, Validation loss 1.9012\n",
      "Epoch 4000, Training loss 1.9113, Validation loss 1.8788\n",
      "Epoch 5000, Training loss 1.8342, Validation loss 1.9448\n",
      "output tensor([[13.3735],\n",
      "        [12.1154]], grad_fn=<AddmmBackward0>)\n",
      "answer tensor([[15.],\n",
      "        [11.]])\n",
      "hidden tensor([[ 0.0101],\n",
      "        [-0.0186],\n",
      "        [-0.0200],\n",
      "        [-0.0232],\n",
      "        [-0.0178],\n",
      "        [ 0.0113],\n",
      "        [-0.0074],\n",
      "        [ 0.0266]])\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(seq_model.parameters(), 1e-3)\n",
    "\n",
    "training_loop(5000, optimizer, seq_model, nn.MSELoss(), train_t_un, val_t_un, train_t_c, val_t_c)\n",
    "\n",
    "print('output', seq_model(val_t_un))\n",
    "print('answer', val_t_c)\n",
    "print('hidden', seq_model.hidden_linear.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f506089888>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAArWklEQVR4nO3deXxU1f3/8deHBEjCFvaEEMJSCPsiiVq11rjUBRFcoFq31rbYVn9qRRFUXLBu1KW1tVpQW5eqjQiIK3WJdvm6ELaELRCWQBIgYQ2QBTI5vz8yYECWBDK5k5n38/HIY2bu3Jl5A+F85p577jnmnENERMJPE68DiIiIN1QARETClAqAiEiYUgEQEQlTKgAiImEq0usAddGhQwfXvXt3r2OIiDQq8+fP3+Kc63jo9kZVALp3705mZqbXMUREGhUzyzvcdnUBiYiEKRUAEZEwpQIgIhKmVABERMJUwAuAmUWZ2TdmttjMlprZg/7t7czsYzNb5b9tG+gsIiLyrYY4AqgAznbODQGGAheY2anAROBT51xv4FP/YxERaSABLwCu2m7/w6b+HweMAl72b38ZGB3oLCIi8q0GOQdgZhFmtggoAj52zn0NdHbObQTw33Y6wmvHmVmmmWUWFxc3RFwRkaCwr6qKf+/YwcTVqymsqKj392+QC8Gccz5gqJnFArPMbGAdXjsNmAaQkpKixQtEJKQV793LB9u28f7Wrfxr2zZ2+nxEmnFGmzZ0ad68Xj+rQa8Eds7tMLPPgQuAzWYW75zbaGbxVB8diIiEnbVlZczesoVZW7bwv507qQLimjXj8o4dGdG+Pee2bUvryPpvrgNeAMysI7DP3/hHA+cCjwNzgOuBx/y37wQ6i4hIsFhTVsY/i4pILy5m0e7q06SDWrTg3qQkRnXowNCWLWliFtAMDXEEEA+8bGYRVJ9zSHfOvWdmXwLpZvZzYD0wpgGyiIh4pqCigjeLinizqIjMXbsAOLV1a57o1YvRHTrQKzq6QfMEvAA457KAYYfZvhU4J9CfLyLipT0+H7OKi3ll82Y+2b4dBwxv2ZKpPXsytlMnkqKiPMvWqGYDFRFpDJxzfFVSwvSNG3mruJjdPh/do6KYnJTENZ070zsmxuuIgAqAiEi92b5vH69u3sz0jRtZsmcPLSMi+HHHjlwfF8fpbdoEvE+/rlQARESOYvbCAn4/N4fCHWV0iY3mzvOTGT0s4aB9Fu3axZ8KCni9qIjyqipSW7Viep8+XNmpEy0DMHqnvgRvMhERj81eWMCkmdmU7fMBULCjjEkzswEYMSSeWVu28KeCAv67cycxTZpwfefO/KpLF4a2auVl7Foz5xrPtVUpKSlOK4KJSEPpfuE4ymN7EJU0+MC20g1ZVO5bS8vf/owNFRX0jIripoQEfhYXR9umTT1Me2RmNt85l3Lodk0HLSJyBOWxPSh+5zHK87LwNTOKfUspfv9xtn+/Hz2iopgzcCArTzmF2xMTg7bxPxp1AYmIHEGvIafgazaRolmP40ZdAu/Oofn/u5c+kUP4Yth3Rrc3OioAIiKHsa6sjJZnxrLl5FOh6hJ47VVa/vBKujKQKWckex2vXqgLSESkhvzycsbl5ND7m2/4fN8uUpeuInLWHNqcdiXlmR/yk8SS74wCaqx0BCAiAmzZu5dH16/n2YICqoBfdenCD9as4aYp9/Cvd2eRlpZGRkYGY8eO5YzeHUlLS/M68gnTEYCIhLU9Ph9T1q2j59df84f8fK7q3JlVp5zCn3r3Zt3ixaSnpx9o7NPS0khPT2fevHkep64fGgYqImHJ5xwvb9rEvWvXsnHvXi7r0IHf9ehBvxYtvI5W7440DFRdQCISdj7dvp3xubks3rOHU1u3ZsaAAZzWpo3XsRqcCoCIhI11ZWX8dvVqZm/ZQlLz5rzZvz9jO3bEgmyOnoaiAiAiIa/c52Pqhg08un49TYCHe/Tg9q5diYqI8Dqap1QARCSkvb91K7esWsWa8nLGduzIE716kejhHPzBRAVARELSxooKbsnNZUZxMf1iYvh0yBDObtvW61hBRQVAREJKlXNMKyzkrjVrqKiq4uEePbgjMZFmTTTq/VAqACISMlbs2cPPc3L4v5ISzo6N5fk+fYJm9a1gpAIgIo2ezzme2rCByWvX0iIigr/37ct1nTuH7eie2gr4MZGZJZpZhpktN7OlZnarf/sDZlZgZov8PxcFOouIhJ4Ve/Zw+oIFTFizhgvbt2dpairXx8Wp8a+FhjgCqATGO+cWmFkrYL6Zfex/7mnn3BMNkEFEQkyVczydn889a9bQIiKC1/v148pOndTw10HAC4BzbiOw0X9/l5ktB0JjKj0R8cSG8nKuX7GCjB07GNW+Pc/36UNc8+Zex2p0GvS0uJl1B4YBX/s33WxmWWb2kpkddnyWmY0zs0wzyywuLm6oqCISpN7YvJlB8+bxTUkJLyQnM2vgQDX+x6nBCoCZtQTeBm5zzpUAzwG9gKFUHyE8ebjXOeemOedSnHMpHTt2bKi4IhJkdlZWcvWyZfxk+XL6t2jB4tRUfh4fry6fE9Ago4DMrCnVjf8/nHMzAZxzm2s8Px14ryGyiEjjM6+khCuXLSOvvJwp3bszqVs3IjWu/4QFvABYdXl+EVjunHuqxvZ4//kBgEuBJYHOIiKNy/4TvRPXrKFLs2Z8MWwYp4fhrJ2B0hBHAKcD1wLZZrbIv+1u4CozGwo4YB1wYwNkEZFGonjvXq5fsYIPt23j0g4deDE5mbZNm3odK6Q0xCig/wKH66T7INCfLSKN05c7dzJ22TKK9+7l2d69+XWXLurrDwBdCSwiQcM5x58KChi/ejWJzZvzfyedxEmtWnkdK2SpAIhIUNhVWckvcnJILy7mkvbt+XvfvuryCTAVABHxXE5pKaOXLGFlaSmP9+zJHYmJNFGXT8CpAIiIp+Zs2cK1y5fTvEkTPhkyhDTN2d9gNJBWRDxR5RwPrlvHqCVL6B0dTebw4Wr8G5iOAESkwZVUVnLt8uXM2bqV6zp35vk+fYgO8/V5vaACICINak1ZGSOzs8kpLeWZ732PmxMSNMTTIyoAItJgPt++ncuXLsUB/9IavZ7TOQARaRDTCgs5LyuLTs2a8c1JJ6nxDwI6AhCRgPI5x+25uTxTUMAF7drxZv/+tIlU0xMM9K8gIgGzq7KSq5Yt4/1t27ita1ee6NWLCPX3Bw0VABEJiPzyci7OzmbJnj38pXdvfp2ghQCDjQqAiNS7Bbt2MTI7m10+H+8NGsQF7dt7HUkOQwVAROrV+1u38uOlS2nftCn/GzaMQS1beh1JjkAFQETqbPbCAn4/N4fCHWV0iY3mzvOTGT0sgWmFhfx65UqGtmzJ+4MGaa3eIKcCICJ1MnthAZNmZlO2zwdAwY4yJs7M5rXyLbxdsZ2L2rXjn/3701IjfYKe/oVEpE5+PzfnQOMP4Azy+0WSU7GdcfHxPNu7t9brbST0ryQidbJ87quU52UBUBUBRcOj2LM5m6g/vcbzffqo8W9E9C8lInXS5XsDKX7nMfYUZrHplCjK8xZjkx+kV7t+mtOnkVEXkIjUye9+cyW/cT42vvUIVF2CzX6Hrpfeze9+c6XX0aSOAn4EYGaJZpZhZsvNbKmZ3erf3s7MPjazVf5bTQwi0ggk9GrFnmu/j428BP7xKgmnXMIzt1/D6GG60KuxaYguoEpgvHOuH3AqcJOZ9QcmAp8653oDn/ofi0gQ+3jbNtIWLSJmSRaxcz9k8uTJlGd9RJsdK72OJsch4AXAObfRObfAf38XsBxIAEYBL/t3exkYHegsInL8/llUxIjsbOKWLmXvAw/wdno6U6ZMIT09nbFjx5KRkeF1RKmjBj0JbGbdgWHA10Bn59xGqC4SQKcjvGacmWWaWWZxcXGDZRWRb/2loICrli3j1NatuXbHDmakp5OWlgZAWloa6enpzJs3z+OUUlfmnGuYDzJrCXwBPOycm2lmO5xzsTWe3+6cO+p5gJSUFJeZmRngpCKyn/Ov2/tgXh6XtG/Pm/37a+nGRsjM5jvnUg7d3iBHAGbWFHgb+IdzbqZ/82Yzi/c/Hw8UNUQWEamdKuf4f6tW8WBeHj+Li+PtAQPU+IeYhhgFZMCLwHLn3FM1npoDXO+/fz3wTqCziEjt7Kuq4trly3m2sJA7EhN5MTlZF3iFoIa4DuB04Fog28wW+bfdDTwGpJvZz4H1wJgGyCIix1Dm8zFm6VLe37aNR3v04K5u3XSBV4gKeAFwzv0XONJvzzmB/nwRqb2dlZWMzM7mvzt38nyfPtzYpYvXkSSAdCWwiABQtHcvF2RlsWTPHt7o358fdzrswDwJISoAIsKG8nLOW7yY9RUVvDNwIBdqBa+woAIgEuZWlZZy7uLF7KisZO7gwfwgNtbrSNJAVABEwljW7t38aPFifEDG0KGc1KqV15GkAWlcl0iY+mrnTn64aBGRZvxHjX9YUgEQCUOfbt/OuYsX0z4ykv8OG0bfFi28jiQeUAEQCTPvbtnCiKwsekRH859hw+geHe11JPGICoBIGHl982YuXbKEwS1b8vnQocQ3b+51JPGQCoBImJhWWMg1y5dzRps2fDpkCO2bNvU6knhMBUAkDDyxfj03rlzJhe3a8eHgwbSK1ABAUQEQCWnOOe5bu5Y716xhbMeOzBo4UDN6ygH6GiASoqqc4/bcXP5YUMDP4+L4a3IyEZrUTWpQARAJQT7nGJeTw0ubNnFb16481auXZvSU71ABEAkxe6uquHr5cmYUF3NfUhIPdO+uxl8OSwVAJISU+nxcvnQpH23bxpO9enF7YqLXkSSIqQCIhIiSykou9s/lP71PH36hufzlGFQARELAFv9c/os1l7/UgQqASCNXUFHBeYsXs7a8nNkDBzJCc/lLLakAiDRiuaWlnJeVxdZ9+/ho8GB+qLn8pQ5UAEQaqezduzlv8WIqnSNj6FCGazpnqaOAXwlsZi+ZWZGZLamx7QEzKzCzRf6fiwKdQyRYzV5YwOmPfUaPie9z+mOfMXthwTFfc9Bc/sOGqfGX41LnAmBmTcysdR1e8nfggsNsf9o5N9T/80Fdc4iEgtkLC5g0M5uCHWU4oGBHGZNmZh9UBKZOnUpGRsaBxx9v20ba3/6Gvfkm/x02jH6ay1+OU60KgJm9bmatzawFsAzIMbM7a/Na59y/gW0nkFEkZP1+bg5l+3wHbSvb5+P3c3MOPE5NTWXs2LFkZGTwVlERF77yCvsefJBpI0dqLn85IbU9AujvnCsBRgMfAN2Aa0/ws282syx/F1HbI+1kZuPMLNPMMouLi0/wI0WCS+GOsmNuT0tLIz09nUuuuIKxd9yBPfggs9LTufz88xsqpoSo2haApmbWlOoC8I5zbh/gTuBznwN6AUOBjcCTR9rROTfNOZfinEvp2LHjCXykSPDpEnv4b/A1tzvn+LpnT3aPGAGvvsodN93EyPPOa6iIEsJqWwD+CqwDWgD/NrMkoOR4P9Q5t9k553POVQHTgZOP971EGrM7z08muunB0zNHN43gzvOTgeoZPe9YvZpJM2fS/L33uPvee3nhr3896JyAyPGq1TBQ59wzwDM1NuWZWdrxfqiZxTvnNvofXgosOdr+Io3R7IUF/H5uDoU7yugSG82d5yczeljCQfvsf3y4/fZVVXFDTg6vffQRUQ89xHszZnDO2Wdz7tlnM3bsWNLT00lLO+7/hiK1KwBmdt8RnppSi9e+AZwFdDCzfOB+4CwzG0p1N9I64Mba5BBpLPaP7tl/gnf/6B7gsEXg0G17fD7GLF3Kh9u2cX5RERNmzODss88Gvj0nMG/ePBUAOSHm3LG78s1sfI2HUcDFwHLn3A2BCnY4KSkpLjMzsyE/UuS4nP7YZxQc5gRvQmw0/5t49lFfu23fPi7OzubrkhKe69OHcZrUTU6Qmc13zqUcur22XUAHnaQ1syeAOfWUTSTk1GZ0z+GsLy/ngqws1pSV8daAAVymgQ8SQMd7JXAM0LM+g4iEktqM7oGDL/Jasns3py1YwPovv+T6zz9X4y8BV9sLwbL9Y/azzGwpkAP8MbDRRBqvY43u2W//RV5/ePddzli4kPL582n20ENceeaZDRlXwlRtJ4O7uMb9SmCzc64yAHlEQsLRRvfUlJaWxs3TpvHba6+l/eWX4+bM4e233vrOyd3ajCgSqaujFgAza+2/AnjXIU+1NjOcc5riQeQIDje651B/zM/nwbZtSRgzhoIXXmDy5MmHbfxrO6JIpC6O1QX0uv92PpDpv51f47GIHIcq5xifm8ttubmcvmYNFbNnM3nyZJ577rnvXORVm/mCRI7HUY8AnHMX+297NEwckdBX7vNx3YoVvFVczKUbNvCfu+46cFFXWlrady7yOt4RRSLHUtuTwKf7ZwLFzK4xs6fMrFtgo4mEnm379nFeVhZvFRfzZK9enFJYeFBjX/Mir/1qO6JIpK5qeyFYFjAEGAy8CrwIXOac+2Fg4x1MF4JJYzJ16lRSU1MPNO65paWk/f3vbMzK4vUpUxhby4XbDz0HANUjih69bJDOAUitHOlCsNpeB1DpqivFKOCPzrk/AlqCSOQIpk6dSmRk5IF5/P+3cydD7rqL/PHjeWrEiFo3/lB9ovfRywaREBuNUX01sRp/qQ+1HQa6y8wmAdcAZ5pZBNA0cLFEGrf94/snTZrEqCuuYHdKCu7jj5n0yCPcMnJknd+vNiOKROqqtl1AccBPgHnOuf/4+//Pcs69EuiANakLSBqTzz77jJFjxlCakADZ2Yy5+mrSX3vN61gShk6oC8g5t8k595Rz7j/+x+sbuvEXaUzKfT5eioujdPhwyM7m9DPOIGPuXM3jL0HlqAXAzHaZWclhfnaZ2XEvCCMSyjbv3cvZixfzjz//GT75hGuuuYacFSuYNGnSgXMCIsHgWNcB6ESvSB1k797NxdnZbPr6a5q9/DI/+e29rOx4FhGV/bnrvilcc+MtmsdfgkatZwM1szPM7Gf++x3MTBeHidQwZ8sWTlu4kErnuLGkhHuf/htfxZxGwY4yopIG037kXby/qIA+513tdVQRoPYXgt0P3AVM8m9qBuhslgjVi7Y/nJfH6CVL6BsTwzfDh/PMfffx0bYOB43dj0oaTEzqZZrCQYJGbYeBXgoMAxYAOOcKzUzdQxL29vh83LBiBenFxVzdqRPTk5OJjqieBlpTOEiwq20B2Oucc2bmAPZPCyESztaXlzN6yRIW7d7N1J49uSMxETM78HyX2OjDLgupKRwkWNT2HEC6mf0ViDWzXwKfANMDF0skuGVs387w+fNZXVbGe4MGcWe3bgc1/lD7RWFEvHKs9QC+B3R2zj1hZucBJUAy8CHwQW0+wMxeonpBmSLn3ED/tnbAP4HuwDpgrHNu+3H+GUQajHOOp/PzmbB6NX1iYpg1cCDJMTGH3be2i8KIeOWoVwKb2XvA3c65rEO2pwD3O+eOeU27mZ0J7AZeqVEApgLbnHOPmdlEoK1z7q5jvZeuBBYv7fH5+EVODm8WFXFZhw78vW9fWkXWthdVxDvHeyVw90MbfwDnXCbV396PyTn3b+DQlcNGAS/7778MjK7Ne4l4Jbe0lNMWLOCfRUU80qMHMwYMUOMvjd6xfoOjjvLciZzJ6uyc2wjgnNtoZkecGtHMxgHjALp10xIE0vBmFxdz/YoVRJrxwaBBXNC+vdeRROrFsY4A5vlP+h7EzH5O9bKQAeecm+acS3HOpXTs2LEhPlIEgMqqKu5avZpLly6lT0wM84cPV+MvIeVYRwC3AbPM7Gq+bfBTqL4Q7NIT+NzNZhbv//YfDxSdwHuJ1LtNFRVcuWwZX+zcya+6dOEP3/sezZvU+sJ5kUbhWHMBbQZOM7M0YKB/8/vOuc9O8HPnANcDj/lv3znB9xOpN59u387Vy5ZR4vPxSt++XBsX53UkkYCo1Vks51wGcFxTGJrZG8BZQAczywfup7rhT/d3Ja0HxhzPe4vUJ59z/C4vjwfXrSM5JoZPhgxhYMuWXscSCZiAD2Nwzl11hKfOCfRni9TWpooKrl6+nM927OC6zp15tndvWmqUj4Q4/YZL2Ju7bRvXLV/OLp+PvyUn89P4eK8jiTQIFQAJW3urqrhn7Vqe2LCBATExfKouHwkzKgASlnJLS7lq+XIyd+3i11268GSvXgdm8RQJFyoAEtJmLyw4aC6eO37Uh11dIrl51SoizHh7wAAu0/UlEqZUACRkXX/rPWRsa02ThOoRzOt3l3H17H9StnklP/jVr3itXz+6RR3tYneR0KYrWyRkzS9tR8Hbj1Cel0V5uyYUtlxB2TMP0aldHzKGDlXjL2FPRwASUqZOnUpqaippaWnsad+XDqMnsnnWw7AyGXJX03bMRGJcXyIOmbtfJBzpCEBCSmpqKmPHjiUjI4M2XaLZOqAZuEpYsIDWgy6kdYdBWpFLxE9HABJS0tLSeOPNN7n4iisoGzECN3MmWARtTruSXQs+oE2Podz542u8jikSFFQAJKSsLC3lvrZtKR0xAl59lWbNo+h97e/Y074vXfunsGHGI7S54WRAq3KJqAtIQoLPOZ7esIEhmZlk//e/tHr/fc455xyio5rzp5+cxNrHRrBk+nhmz5zBvHnzvI4rEhRUAKTRW1Vayg8XLuT21asZtmoVzR56iHdmzOCTTz5h1qxZB84JQHUX0YQJEzxOLBIcVACk0fI5xx/83/qXlpbySt++jNqyhRnp6aSlpQHVDX56erq+9YscxlEXhQ82WhRe9lu+Zw835OTwVUkJI9q1Y1pyMl2aN/c6lkhQOtKi8DoJLI3Kvqoqpm7YwJR162gVEcFr/frxk06dMI3rF6kzFQBpNDJLSvjlypUs2r2bsR078qfevenUrNl39jt0/p87z09m9DCN+hE5lAqABL09Ph/3rV3LH/Lz6dysGTMHDODSI0zgNnthAZNmZlO2zwdAwY4yJs3MBlAREDmETgJLUJu7bRsD583jqfx8fhkfz7LU1CM2/gC/n5tzoPHfr2yfj9/PzQl0VJFGR0cAEpQ2VVRw++rVvFFURN+YGP49dCg/iI095usKd5TVabtIOFMBkKBS5RzTN25k4po1lPp83J+UxKSkJJo3qd3BapfYaAoO09hr/h+R7/K0C8jM1plZtpktMjON7wxz2bt3c8bChfxq5UqGtWxJVmoqD/ToUevGH+DO85OJbnrwyl7RTSO48/zk+o4r0ugFwxFAmnNui9chxDu7Kyt5MC+PpzdsIDYykpf79uXazp2Pa2jn/hO9GgUkcmzBUAAkTDnnmLllC7fl5pJfUcEv4+N5tGdP2jdtekLvO3pYghp8kVrwugA44F9m5oC/OuemHbqDmY0DxgF069atgeNJoOSWlnJLbi4fbtvGkBYtSO/fn++3aeN1LJGw4nUBON05V2hmnYCPzWyFc+7fNXfwF4VpUD0VhBchpf6U+nw8un49U9evp3mTJjzdqxc3JyQQWYd+fhGpH54WAOdcof+2yMxmAScD/z76q6Qxcs4xe8sWfpubS15FBdd07szUnj2J1/w9Ip7xrACYWQugiXNul//+j4ApXuWRwFmxZw+35eYyd/t2BrZowRf9+nFmLcb0i0hgeXkE0BmY5R/pEQm87pz7yMM8Us9KKit5KC+PP+TnE+Pv7rkpIYGm6u4RCQqeFQDn3BpgiFefL4FT5Rz/2LyZCWvWsGnvXm6Ii+PRnj0PO3GbiHjH65PAEmK+KSnh1txcviop4eRWrXhn4EBObt3a61gichgqAFIvNlZUcPfatfx90yY6N23K35KTuS4ujiaap18kaKkAyAkp9/n4Y0EBv8vLo6KqigmJidyTlETrSP1qiQQ7/S8NQQ2xIMr+q3jvXL2ateXljGzfnid79aJ3TEy9fo6IBI4KQIhpiAVRFu7axW9zc/li504GtmjBx4MHc267dvXy3iLScDQeL8QEckGUjRUV3LBiBcPnz2dpaSnP9e7NwuHD1fiLNFI6AggxgVgQpdTn48kNG3h8/Xr2Osf4xETu6daN2BOctE1EvKUCEGLqc0GU/eP57167lvyKCi7v0IHHe/WiV7QWVxEJBeoCCjH1tSDK59u3kzp/PtetWEHnpk35YuhQZgwcqMZfJIToCCDEnOiCKDmlpUxYvZo5W7eS2Lw5r/Xrx1WdOmk8v0gIUgEIQcezIErR3r08sG4d0woLiYmI4JEePbita1eiIyKO/WIRaZTUBRRmpk6dSkZGxoHHpT4fN7zxBom33sq0wkJu7NKF3FNOYVJSkhp/kRCnAhBmUlNTGTt2LJ989hkvbtxIt7/8hb/9+tekpqay9OSTebZPH03aJhIm1AUUZs466yxunz6dCy6/HN/IkUS++y5/ePVVbh050utoItLAdAQQRr4pKSFt0SLujo2l9WWXwauvMvHmm9X4i4QpFYAQc2gfP8CrH37I4PHjOWXBApaVlnJLURERc+YwefJknn/++e/sLyLhQQUgxOzv48/IyGBTRQWjXnmF6666ilWJidyflMTfysp4/aabSE9PZ8qUKaSnpx/YX0TCiwpAiElLS+OlN97g4iuuIHHcOObccgujnnmGdb/5DQ/06MHSBQtIT08nLS3twP7p6enMmzfP4+Qi0tB0EjiElPt8PFtYyCNRUZSOGAGvvMJNEyfy5+uuO7DPhAkTmL2wgNMf++ygC8UmTEjzMLmIeEFHACGgsqqKFzdupM8333DH6tX0XLGC2A8+YPLkyfzzhRcO6t7ZP110wY4yHN9OFz17YYF3fwAR8YSnBcDMLjCzHDPLNbOJXmZpjKqc462iIgbOm8cvcnKIb9aMJ0tKWDdpEjPfeuuwffyBnC5aRBoXzwqAmUUAzwIXAv2Bq8ysv1d5GhPnHHO3bSN1/nzGLltGhBmzBgzgq5NOonLFiqP28QdiumgRaZy8PAdwMpDrnFsDYGZvAqOAZR5mCnr/2bGDe9au5T87d9I9KoqX+/bl6s6difBP1jZhwoTvvCYtLe1AQajP6aJFpHHzsgsoAdhQ43G+f5scxvxdu7gwK4szFy1iVVkZz/buTc7JJ3NdXNyBxr826mu6aBFp/Lw8Ajhcq+W+s5PZOGAcQLdu3QKdKegs2b2b+9etY+aWLbSLjGRqz57clJBAzHFO1Hai00WLSOjwsgDkA4k1HncFCg/dyTk3DZgGkJKS8p0CEapWlZbywLp1vFFURMuICB7o3p3bunalTeSJ/5Mdz3TRIhJ6vCwA84DeZtYDKACuBH7iYZ6gsLasjIfy8nhl0yaaN2nCXd26cUdiIu21/q6I1DPPCoBzrtLMbgbmAhHAS865pV7l8dr68nIezsvjpU2biABuTkhgUlISnTU1s4gEiKdXAjvnPgA+8DKD1/LLy3l0/Xqmb9yIATfGxzMpKYmE5s29jiYiIU5TQXgkv7ycx/wNfxVwQ1wc9yQl0S0qyutoIhImVAAaWH55OY9v2MC0wkKqgJ/FxXF3t250j9Y4fBFpWCoADWSD/xv/C/5v/D+Ni+MeNfwi4iEVgADLKy/nUf/JXaj+xj9JDb+IBAEVgABZXVbGo3l5vLx5Mwb8PD6eid26kaQ+fhEJEioA9SyntJSH8/J4ffNmIs34VZcuTEhMJFENv4gEGRWAerJo1y4eWb+eGcXFRDdpwm1duzI+MZF4DecUkSClAnCCvtq5k4fXr+e9rVtpHRHBxG7d+G3XrnTUBVwiEuRUAI6Dc45Pt2/n0fXr+WzHDtpHRvJQ9+7cnJBArKZsEJFGQgWgDqqc450tW3hk/Xoyd+0ivlkznujVixvj42lZD5O0iYg0JLVatbC3qorXN29m6oYNLC8tpVdUFNP69OG6uDiaN9GyyiLSOKkAHMXuykqmb9zIU/n55FdUMKRFC17v148xHTsSqYZfRBq5sC4AU6dOJTU19cByiQAZGRlkfPklXHUVfy4oYHtlJWfFxjK9Tx/Ob9cOq8PqWyIiwSysv8ampqYyduxYMjIyAHjtww+56PLLeSwmht/l5fHD2Fi+HDaMjKFDuaB9ezX+IhJSwvoIIC0tjfT0dC4bM4b4MWNY/sYbRD7wAD+78ELGJyaSHBPjdUQRkYAJ2wJQ5Rzvbt3KpMhodlx0ETuef55Wo6/ljz+4lJ8lJ3kdT0Qk4MKuAJT6fLyyaRNP5+ezsqyMJl8twGbNodXpV7J77jvc33IAbW+/RmvmikjIC5sCsHnvXp4tKOAvBQVsraxkeMuWdH13PoXPTaHTqIlEJQ0mOnEwBW8/wr1RkYyePt7ryCIiARUWBeChdet4OC+Pvc4xsn17xicm8oM2bWh/57N09Df+AFFJg+k4aiKFuUs8TiwiEnhhUQCSoqK4IT6e27p2pU+NE7v9zr+Wgh1lB+0blTSYXkNOaeiIIiINLiyGgV4XF8df+vQ5qPEHuPP8ZKKbRhy0LbppBHeen9yQ8UREPOFJATCzB8yswMwW+X8u8iLH6GEJPHrZIBJiozEgITaaRy8bpBPAIhIWvOwCeto594SHnw9UFwE1+CISjsKiC0hERL7LywJws5llmdlLZtb2SDuZ2TgzyzSzzOLi4obMJyIS0sw5F5g3NvsEiDvMU/cAXwFbAAc8BMQ752441numpKS4zMzMes0pIhLqzGy+cy7l0O0BOwfgnDu3NvuZ2XTgvUDlEBGRw/NqFFB8jYeXArrySkSkgXk1CmiqmQ2lugtoHXCjRzlERMJWwM4BBIKZFQN5x/nyDlSfd2gslDfwGltm5Q2sUM6b5JzreOjGRlUAToSZZR7uJEiwUt7Aa2yZlTewwjGvrgMQEQlTKgAiImEqnArANK8D1JHyBl5jy6y8gRV2ecPmHICIiBwsnI4ARESkBhUAEZEwFZIFwMwSzSzDzJab2VIzu9W/vZ2ZfWxmq/y3R5yEriGZWZSZfWNmi/15H/RvD8q8AGYWYWYLzew9/+OgzQpgZuvMLNu//kSmf1vQZjazWDObYWYr/L/H3w/WvGaWXGNtj0VmVmJmtwVrXgAz+63//9oSM3vD/38wmPPe6s+61Mxu82874bwhWQCASmC8c64fcCpwk5n1ByYCnzrnegOf+h8HgwrgbOfcEGAocIGZnUrw5gW4FVhe43EwZ90vzTk3tMbY6WDO/EfgI+dcX2AI1X/XQZnXOZfj/3sdCgwHSoFZBGleM0sAbgFSnHMDgQjgSoI370Dgl8DJVP8uXGxmvamPvM65kP8B3gHOA3KonnkUIB7I8TrbYbLGAAuAU4I1L9DV/wt3NvCef1tQZq2ReR3Q4ZBtQZkZaA2sxT9II9jzHpLxR8D/gjkvkABsANpRPR3Oe/7cwZp3DPBCjceTgQn1kTdUjwAOMLPuwDDga6Czc24jgP+2k4fRDuLvUlkEFAEfO+eCOe8fqP4FrKqxLViz7ueAf5nZfDMb598WrJl7AsXA3/zdbC+YWQuCN29NVwJv+O8HZV7nXAHwBLAe2AjsdM79iyDNS/VkmWeaWXsziwEuAhKph7whXQDMrCXwNnCbc67E6zxH45zzuepD6K7Ayf7DvqBjZhcDRc65+V5nqaPTnXMnARdS3SV4pteBjiISOAl4zjk3DNhDkHRHHI2ZNQMuAd7yOsvR+PvKRwE9gC5ACzO7xttUR+acWw48DnwMfAQsprqb+4SFbAEws6ZUN/7/cM7N9G/evH8qav9tkVf5jsQ5twP4HLiA4Mx7OnCJma0D3gTONrPXCM6sBzjnCv23RVT3T59M8GbOB/L9R4EAM6guCMGad78LgQXOuc3+x8Ga91xgrXOu2Dm3D5gJnEbw5sU596Jz7iTn3JnANmAV9ZA3JAuAmRnwIrDcOfdUjafmANf7719P9bkBz5lZRzOL9d+PpvoXdAVBmNc5N8k519U5153qw/3PnHPXEIRZ9zOzFmbWav99qvt7lxCkmZ1zm4ANZpbs33QOsIwgzVvDVXzb/QPBm3c9cKqZxfjbinOoPskerHkxs07+227AZVT/PZ94Xq9PcATopMkZVPf5ZgGL/D8XAe2pPnm5yn/bzuus/ryDgYX+vEuA+/zbgzJvjdxn8e1J4KDNSnWf+mL/z1LgnkaQeSiQ6f+dmA20DfK8McBWoE2NbcGc90Gqv2QtAV4Fmgd53v9Q/SVgMXBOff39aioIEZEwFZJdQCIicmwqACIiYUoFQEQkTKkAiIiEKRUAEZEwpQIgIc/MfIfMVtn9CPt1N7MlAcrwUzP7cx1f84J/EkPM7O5A5JLwFul1AJEGUOaqp9moF2YW6Zyrl0vxj8Y594saD+8GHgn0Z0p40RGAhB0za2lmn5rZAv8aAaNqPB1hZtP9867/y39lNmb2uZk9YmZfALea2XAz+8I/udzcGpfkf25mj1v1+g4rzewHNd67i5l95J+/fWqNPD8ysy/9ed7yz2G1/71SzOwxINp/9PKPwP8NSbhQAZBwsL/xXGRms4By4FJXPTlcGvCkf0oAgN7As865AcAO4PIa7xPrnPsh8AzwJ+AK59xw4CXg4Rr7RTrnTgZuA+6vsX0o8GNgEPBjq164qANwL3CuP08mcHvN8M65ifiPYpxzV5/g34XIAeoCknBwUBeQf6LAR/wzglZRPT98Z//Ta51zi/z35wPda7zPP/23ycBA4GN/3Yigelrh/fZPPnjo6z91zu30Z1gGJAGxQH/gf/73agZ8eTx/SJG6UgGQcHQ10BEY7pzb55/ZNMr/XEWN/XxAdI3He/y3Bix1zn3/CO+//z18HPx/7ND3jvS/18fOuavq+ocQOVHqApJw1IbqNQ32mVka1d/E6yIH6Ghm34fqIwozG3CcWb4CTjez7/nfK8bM+hxmv33+IxeReqMCIOHoH0CKVS8OfzXVs0LWmnNuL3AF8LiZLaZ6ttnTjieIc64Y+CnwhpllUV0Q+h5m12lAlk4CS33SbKAiImFKRwAiImFKBUBEJEypAIiIhCkVABGRMKUCICISplQARETClAqAiEiY+v8PgEEa8GtCkwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "t_range = torch.arange(20, 90).unsqueeze(1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.set(xlabel=\"Fahrenheit\", ylabel=\"Celsius\")\n",
    "ax.plot(t_u.numpy(), t_c.numpy(), 'o') # correct labels\n",
    "ax.plot(t_range.numpy(), seq_model(0.1 * t_range).detach().numpy(), 'c-') # whole curve (behavior between samples)\n",
    "ax.plot(t_u.numpy(), seq_model(0.1 * t_u).detach().numpy(), 'kx') # fitted values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "**1. Changing number of hidden neurons and learning rate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(n_neurons, learning_rate=1e-3, print_vals=False, print_weights=False):\n",
    "    seq_model = nn.Sequential(nn.Linear(1, n_neurons), \n",
    "                              nn.Tanh(), \n",
    "                              nn.Linear(n_neurons, 1))\n",
    "\n",
    "    optimizer = optim.SGD(seq_model.parameters(), learning_rate)\n",
    "\n",
    "    training_loop(5000, optimizer, seq_model, nn.MSELoss(), train_t_un, val_t_un, train_t_c, val_t_c, print_vals)\n",
    "    \n",
    "    print('output', seq_model(val_t_un))\n",
    "    if print_weights:        \n",
    "        print('answer', val_t_c)\n",
    "        print('hidden', seq_model[2].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 13.6701, Validation loss 29.5779\n",
      "output tensor([[10.7769],\n",
      "        [ 8.8936]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "2 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 2.9871, Validation loss 5.9727\n",
      "output tensor([[14.3782],\n",
      "        [12.7827]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "3 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.8457, Validation loss 2.2080\n",
      "output tensor([[13.5465],\n",
      "        [12.2392]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "4 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.8269, Validation loss 1.9150\n",
      "output tensor([[13.2912],\n",
      "        [11.9555]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "5 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.7999, Validation loss 1.7881\n",
      "output tensor([[13.1286],\n",
      "        [11.8232]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "6 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.8492, Validation loss 1.7803\n",
      "output tensor([[12.9010],\n",
      "        [11.6277]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "7 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.7754, Validation loss 1.9388\n",
      "output tensor([[13.3180],\n",
      "        [12.0239]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "8 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.8701, Validation loss 1.8307\n",
      "output tensor([[12.8605],\n",
      "        [11.6316]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "9 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.7731, Validation loss 1.9505\n",
      "output tensor([[13.3162],\n",
      "        [12.0324]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "10 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.8391, Validation loss 2.2180\n",
      "output tensor([[13.7190],\n",
      "        [12.4070]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for n in range(1, 11):\n",
    "    print(f'\\n{n} neurons in hidden layer:\\n')\n",
    "    run_model(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.8191, Validation loss 1.8084\n",
      "output tensor([[13.0271],\n",
      "        [11.7614]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "20 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.8232, Validation loss 1.9429\n",
      "output tensor([[13.4234],\n",
      "        [12.1833]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "30 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.9030, Validation loss 2.1139\n",
      "output tensor([[13.7308],\n",
      "        [12.4965]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "40 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.9273, Validation loss 2.1165\n",
      "output tensor([[13.7663],\n",
      "        [12.5594]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "50 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.9289, Validation loss 2.0075\n",
      "output tensor([[13.5408],\n",
      "        [12.3732]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "60 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.9191, Validation loss 2.0091\n",
      "output tensor([[13.5254],\n",
      "        [12.3579]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "70 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.9399, Validation loss 2.0278\n",
      "output tensor([[13.5379],\n",
      "        [12.3848]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "80 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 2.4286, Validation loss 2.5413\n",
      "output tensor([[12.6615],\n",
      "        [11.5345]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "90 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.9543, Validation loss 2.0348\n",
      "output tensor([[13.5510],\n",
      "        [12.4035]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "100 neurons in hidden layer:\n",
      "\n",
      "Epoch 5000, Training loss 1.9308, Validation loss 2.0306\n",
      "output tensor([[13.5199],\n",
      "        [12.3677]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for n in range(10, 110, 10):\n",
    "    print(f'\\n{n} neurons in hidden layer:\\n')\n",
    "    run_model(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in this case increasing the number of hidden neurons increases validation loss in general. Now change the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning Rate: 0.1\n",
      "Epoch 5000, Training loss nan, Validation loss nan\n",
      "output tensor([[nan],\n",
      "        [nan]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.010000000000000002\n",
      "Epoch 5000, Training loss 1.3872, Validation loss 3.2450\n",
      "output tensor([[13.1144],\n",
      "        [12.4255]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.0010000000000000002\n",
      "Epoch 5000, Training loss 1.8569, Validation loss 2.1651\n",
      "output tensor([[13.7125],\n",
      "        [12.4424]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.00010000000000000002\n",
      "Epoch 5000, Training loss 10.2521, Validation loss 2.7754\n",
      "output tensor([[13.8992],\n",
      "        [13.0831]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 1.0000000000000003e-05\n",
      "Epoch 5000, Training loss 86.2192, Validation loss 33.6671\n",
      "output tensor([[7.5685],\n",
      "        [7.5231]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 1.0000000000000004e-06\n",
      "Epoch 5000, Training loss 165.4729, Validation loss 140.7174\n",
      "output tensor([[1.3110],\n",
      "        [1.3028]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 1.0000000000000004e-07\n",
      "Epoch 5000, Training loss 185.4850, Validation loss 166.5322\n",
      "output tensor([[0.2524],\n",
      "        [0.2495]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 1.0000000000000005e-08\n",
      "Epoch 5000, Training loss 179.0813, Validation loss 159.0341\n",
      "output tensor([[0.5520],\n",
      "        [0.5442]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 1.0000000000000005e-09\n",
      "Epoch 5000, Training loss 196.5513, Validation loss 181.1682\n",
      "output tensor([[-0.3100],\n",
      "        [-0.3111]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 1.0000000000000006e-10\n",
      "Epoch 5000, Training loss 198.4434, Validation loss 183.1247\n",
      "output tensor([[-0.3840],\n",
      "        [-0.3834]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for lr in [0.1**i for i in range(1, 11)]:\n",
    "    print(f'\\nLearning Rate: {lr}')\n",
    "    run_model(13, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation loss is generally increasing as the learning rate decreases (past a certain point). Let's try learning rates around 1e-3, which seems to work best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning Rate: 0.001\n",
      "Epoch 5000, Training loss 1.8866, Validation loss 2.2317\n",
      "output tensor([[13.7997],\n",
      "        [12.5216]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.002\n",
      "Epoch 5000, Training loss 1.8277, Validation loss 2.3767\n",
      "output tensor([[13.6764],\n",
      "        [12.3676]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.003\n",
      "Epoch 5000, Training loss 1.7798, Validation loss 1.9546\n",
      "output tensor([[12.8456],\n",
      "        [11.6816]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.004\n",
      "Epoch 5000, Training loss 1.5803, Validation loss 2.0687\n",
      "output tensor([[13.0278],\n",
      "        [11.9628]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.005\n",
      "Epoch 5000, Training loss 1.5748, Validation loss 2.0267\n",
      "output tensor([[13.1364],\n",
      "        [12.0113]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.006\n",
      "Epoch 5000, Training loss 1.3573, Validation loss 2.2179\n",
      "output tensor([[13.4203],\n",
      "        [12.4018]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.007\n",
      "Epoch 5000, Training loss 1.3641, Validation loss 2.2125\n",
      "output tensor([[13.3833],\n",
      "        [12.3452]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.008\n",
      "Epoch 5000, Training loss 1.3288, Validation loss 2.4337\n",
      "output tensor([[13.7701],\n",
      "        [12.7856]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.009000000000000001\n",
      "Epoch 5000, Training loss 1.5403, Validation loss 2.4833\n",
      "output tensor([[13.0599],\n",
      "        [12.1458]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for lr in [1e-3*i for i in range(1, 10)]:\n",
    "    print(f'\\nLearning Rate: {lr}')\n",
    "    run_model(13, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning Rate: 0.0001\n",
      "Epoch 5000, Training loss 11.5025, Validation loss 2.5825\n",
      "output tensor([[13.7046],\n",
      "        [12.8673]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.0002\n",
      "Epoch 5000, Training loss 4.2693, Validation loss 2.7351\n",
      "output tensor([[14.3205],\n",
      "        [13.2379]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.00030000000000000003\n",
      "Epoch 5000, Training loss 2.5528, Validation loss 2.0953\n",
      "output tensor([[13.9933],\n",
      "        [12.7824]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.0004\n",
      "Epoch 5000, Training loss 2.0873, Validation loss 1.9220\n",
      "output tensor([[13.7665],\n",
      "        [12.5239]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.0005\n",
      "Epoch 5000, Training loss 1.9692, Validation loss 1.8714\n",
      "output tensor([[13.7054],\n",
      "        [12.4376]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.0006000000000000001\n",
      "Epoch 5000, Training loss 1.9409, Validation loss 1.8815\n",
      "output tensor([[13.6668],\n",
      "        [12.4091]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.0007\n",
      "Epoch 5000, Training loss 1.8732, Validation loss 1.8824\n",
      "output tensor([[13.5482],\n",
      "        [12.2873]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.0008\n",
      "Epoch 5000, Training loss 1.8034, Validation loss 1.9107\n",
      "output tensor([[13.3981],\n",
      "        [12.1205]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Learning Rate: 0.0009000000000000001\n",
      "Epoch 5000, Training loss 1.8468, Validation loss 1.8864\n",
      "output tensor([[13.4714],\n",
      "        [12.1984]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for lr in [1e-4*i for i in range(1, 10)]:\n",
    "    print(f'\\nLearning Rate: {lr}')\n",
    "    run_model(13, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it appears the best learning rate according to validation loss is around 0.0008."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Create new model for wine data. Copying normalization from Ch.4 notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wineq_pd = pd.read_csv('data/p1ch4/tabular-wine/winequality-white.csv', sep=';')\n",
    "wineq = torch.from_numpy(wineq_pd.to_numpy(dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4898, 12]), torch.float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wineq.shape, wineq.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now have a floating point tensor with all the columns including the last, which refers to the quality score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7.0000,  0.2700,  0.3600,  ...,  3.0000,  0.4500,  8.8000],\n",
       "         [ 6.3000,  0.3000,  0.3400,  ...,  3.3000,  0.4900,  9.5000],\n",
       "         [ 8.1000,  0.2800,  0.4000,  ...,  3.2600,  0.4400, 10.1000],\n",
       "         ...,\n",
       "         [ 6.5000,  0.2400,  0.1900,  ...,  2.9900,  0.4600,  9.4000],\n",
       "         [ 5.5000,  0.2900,  0.3000,  ...,  3.3400,  0.3800, 12.8000],\n",
       "         [ 6.0000,  0.2100,  0.3800,  ...,  3.2600,  0.3200, 11.8000]]),\n",
       " torch.Size([4898, 11]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = wineq[:, :-1] # excluding score\n",
    "data, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6., 6., 6.,  ..., 6., 7., 6.]), torch.Size([4898]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = wineq[:, -1]\n",
    "target, target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6., 6., 6.,  ..., 6., 7., 6.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_onehot = torch.zeros(target.shape[0], 10)\n",
    "target_onehot.scatter_(1, target.unsqueeze(1).long(), 1.0) # for each row take idx corresp to element in target and set to 1.0 in target_onehot\n",
    "# first arg is dim along which the following two args are specified, \n",
    "# second arg is a column tensor indicating idx of element to scatter,\n",
    "# third arg is the elements/single scalar to scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.8548e+00, 2.7824e-01, 3.3419e-01, 6.3914e+00, 4.5772e-02, 3.5308e+01,\n",
       "        1.3836e+02, 9.9403e-01, 3.1883e+00, 4.8985e-01, 1.0514e+01])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mean = torch.mean(data, dim=0)\n",
    "data_mean # gets mean in every column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.1211e-01, 1.0160e-02, 1.4646e-02, 2.5726e+01, 4.7733e-04, 2.8924e+02,\n",
       "        1.8061e+03, 8.9455e-06, 2.2801e-02, 1.3025e-02, 1.5144e+00])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_var = torch.var(data, dim=0)\n",
    "data_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normalized = (data - data_mean)/torch.sqrt(data_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now back to new analysis. Use normalized data and one-hot encoding of target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4898, 11]), torch.Size([4898, 10]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_normalized.shape, target_onehot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([8, 1, 9], [4, 6, 3, 0, 2, 5, 7])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = torch.utils.data.random_split(range(10), [3, 7])\n",
    "[i for i in x], [i for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(data_normalized.shape[0]*0.8)\n",
    "val_size = data_normalized.shape[0] - train_size\n",
    "train_set, val_set = torch.utils.data.random_split(data_normalized, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [this to get training and validation split](https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(data_normalized.shape[0]))\n",
    "np.random.shuffle(indices)\n",
    "split_idx = int(data_normalized.shape[0]*0.8)\n",
    "train_idxs, val_idxs = indices[:split_idx], indices[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = data_normalized[train_idxs]\n",
    "val_set = data_normalized[val_idxs]\n",
    "train_output = target_onehot[train_idxs]\n",
    "val_output = target_onehot[val_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create and run a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = data_normalized.shape[1]\n",
    "n_outputs = target_onehot.shape[1]\n",
    "n_hidden_neurons = 10\n",
    "seq_model = nn.Sequential(nn.Linear(n_inputs, n_hidden_neurons),\n",
    "                          nn.Tanh(),\n",
    "                          nn.Linear(n_hidden_neurons, n_outputs),\n",
    "                          nn.Softmax(dim=1)) # to represent probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust loss to work with one-hot target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(inp, out):\n",
    "    return torch.mean(-1*torch.sum(torch.tensor(range(1, inp.shape[1] + 1))*torch.log(inp), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(129.9237, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(seq_model(train_set[:2]), train_output[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 130.6483, Validation loss 130.6815\n",
      "Epoch 1000, Training loss 118.3246, Validation loss 118.3244\n",
      "Epoch 2000, Training loss 118.3227, Validation loss 118.3226\n",
      "Epoch 3000, Training loss 118.3219, Validation loss 118.3219\n",
      "Epoch 4000, Training loss 118.3215, Validation loss 118.3215\n",
      "Epoch 5000, Training loss 118.3213, Validation loss 118.3212\n",
      "Epoch 6000, Training loss 118.3211, Validation loss 118.3211\n",
      "Epoch 7000, Training loss 118.3210, Validation loss 118.3210\n",
      "Epoch 8000, Training loss 118.3209, Validation loss 118.3209\n",
      "Epoch 9000, Training loss 118.3208, Validation loss 118.3208\n",
      "Epoch 10000, Training loss 118.3208, Validation loss 118.3208\n",
      "output tensor([[0.0182, 0.0362, 0.0545,  ..., 0.1454, 0.1641, 0.1820],\n",
      "        [0.0180, 0.0364, 0.0544,  ..., 0.1457, 0.1632, 0.1822],\n",
      "        [0.0181, 0.0367, 0.0544,  ..., 0.1460, 0.1634, 0.1808],\n",
      "        ...,\n",
      "        [0.0179, 0.0362, 0.0543,  ..., 0.1454, 0.1636, 0.1822],\n",
      "        [0.0181, 0.0364, 0.0545,  ..., 0.1454, 0.1635, 0.1818],\n",
      "        [0.0183, 0.0361, 0.0548,  ..., 0.1451, 0.1633, 0.1821]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "answer tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "hidden tensor([[-2.5322e-05, -3.9898e-05, -1.6148e-04,  1.7938e-05,  1.0677e-04,\n",
      "         -2.6026e-04, -1.1441e-04, -1.1240e-04,  5.4110e-05, -1.1325e-04],\n",
      "        [-1.8010e-04,  5.3938e-05, -6.1569e-05, -9.3573e-05,  1.3299e-04,\n",
      "          8.6274e-05, -1.3930e-05,  4.2693e-05, -6.4485e-06,  9.8219e-05],\n",
      "        [ 4.3099e-05, -4.0456e-05, -4.9929e-05,  5.4121e-05,  7.7952e-05,\n",
      "         -4.4591e-05,  3.0560e-05, -2.4246e-05,  3.1980e-06,  8.0167e-05],\n",
      "        [ 6.4705e-05,  1.1587e-04,  1.7850e-04,  7.9117e-05, -5.0539e-05,\n",
      "          1.6649e-04,  2.2882e-05,  1.4004e-04, -2.9567e-05,  3.5981e-05],\n",
      "        [ 1.4381e-04, -9.4042e-05,  5.5887e-05,  1.5087e-04, -1.0454e-04,\n",
      "          1.3867e-04, -4.6846e-05,  1.7908e-04,  3.2355e-05,  5.8368e-05],\n",
      "        [-2.3649e-05, -5.4486e-05,  2.4453e-04, -8.5010e-05, -1.2732e-04,\n",
      "         -7.5008e-05,  1.2941e-04,  1.3371e-04, -4.6432e-05, -4.9530e-05],\n",
      "        [ 2.0681e-04, -1.6587e-04,  1.0474e-04,  6.1233e-05, -5.9963e-05,\n",
      "         -9.8134e-05,  1.1222e-04, -2.9130e-05,  8.5657e-06,  2.0303e-05],\n",
      "        [-2.4275e-04, -1.3965e-05, -2.9805e-04, -1.7145e-04,  4.3793e-05,\n",
      "         -6.7738e-05, -8.2492e-05, -1.2793e-04,  1.7667e-05,  3.4745e-06],\n",
      "        [ 1.0343e-04,  6.9012e-05, -6.9756e-05,  1.9966e-04,  7.9178e-05,\n",
      "          1.7242e-04, -1.9681e-04,  4.9925e-05,  3.3898e-05,  7.8127e-05],\n",
      "        [-9.0150e-05,  1.6995e-04,  5.7242e-05, -2.1305e-04, -9.8552e-05,\n",
      "         -1.8077e-05,  1.5955e-04, -2.5182e-04, -6.6766e-05, -2.1203e-04]])\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(seq_model.parameters(), learning_rate)\n",
    "\n",
    "training_loop(10000, optimizer, seq_model, cross_entropy, train_set, val_set, train_output, val_output)\n",
    "\n",
    "print('output', seq_model(val_set))\n",
    "print('answer', val_output)\n",
    "print('hidden', seq_model[2].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come back to this once I have a bit more understanding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learn",
   "language": "python",
   "name": "deep_learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
