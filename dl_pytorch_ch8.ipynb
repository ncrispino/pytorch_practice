{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "cifar2 = pickle.load(open(\"cifar2.p\", \"rb\"))\n",
    "cifar2_val = pickle.load(open(\"cifar2_val.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions\n",
    "Example: We have a 3 x 3 kernel stored as a tensor. The weight matrix will be 3 x 3 x 3 as we have a multichannel image (3 channels, one for R, G, and B). The same kernel is used throughout the whole image, as we want to pay attention to the same relative position of input and output pixels. So, the derivative of the loss w.r.t a convolution weight includes contributions from the whole image.\n",
    "\n",
    "Note that now the number of parameters depends on the size of the convolution kernel and the number of filter used, not the whole image.\n",
    "\n",
    "We will use 2D convolutions with the same size kernel across all dimensions of the image (as they all have the same size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 input per pixel, 16 channels in the output meaning 16 different feature maps, \n",
    "# each defined by a different set of shared weights\n",
    "conv = nn.Conv2d(3, 16, kernel_size=3) \n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape, conv.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is because we have 16 different kernels, each 3 x 3 x 3 (one 3 x 3 kernel for each input channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the model (note the weights will be initialized randomly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, _ = cifar2[0]\n",
    "output = conv(img.unsqueeze(0)) # need to add a dimension at beginning because function takes B x C x H x W and we only have one image so B = 1\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display output for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output image has smaller dimensions than inputs because when applying the convolutional kernels, we only consider when every pixel has a neighbor. We can change this by adding a padding of ghost zeros around the image. Use padding when we want tensors to be of comparable size after a few convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 1, 32, 32]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting weights & bias by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    conv.bias.zero_()\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv.weight.fill_(1.0 / 9.0) # avg of pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = conv(img.unsqueeze(0))\n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use an edge-detection kernel--measures difference in intensity between left and right pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv.weight[:] = torch.tensor([[-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0]])\n",
    "    conv.bias.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = conv(img.unsqueeze(0))\n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though kernels are useful, we still need to consider the big picture. Use downsampling to accomplish this.\n",
    "\n",
    "Some different downsampling methods:\n",
    "- Average pooling\n",
    "- Max pooling (popular but discards all but 1 pixel)\n",
    "- Strided convolution (every Nth pixel calculated)\n",
    "\n",
    "When features are found corresponding to the estimated kernel, they tend to have high magnitude. So, max pooling will keep these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.MaxPool2d(2) # 2 is neighborhood over which to operate the pooling operation\n",
    "output = pool(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition of using pooling with convolutions: first set of kernels operates on small neighborhoods while second set of kernels (applied after pooling) operates on wider neighborhoods, providing features that are compositons of previous features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining it together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1), # output is 16 channel\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2), # 16 x 32 x 32 pooled down to 16 x 16 x 16\n",
    "            nn.Conv2d(16, 8, kernel_size=3, padding=1), # output is 8 channel\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2), # 8 x 8 x 8\n",
    "            # WARNING: to get this to work, need to reshape the above layer to fit as below\n",
    "            nn.Linear(8 * 8 * 8, 32), # 'flattens' the prev output into a 1D vector\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x8 and 512x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3140\\1747208490.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x8 and 512x32)"
     ]
    }
   ],
   "source": [
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to reshape the output of our last pooling layer before we can input it into the linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subclassing nn.Module\n",
    "Need to define a forward function. Define submodules in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # always need\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        out = self.fc1(out.view(-1, 8 * 8 * 8)) # B x N vector\n",
    "        out = self.fc2(self.act3(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of intermediate values generally shrinking (note that number of channels can increase in some models, but comes with more pooling--a decrease in other values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For layers with no model parameters (output value is solely determined by input values), use ```torch.nn.functional``` (or ```torch``` in general for basics like activation functions if they're there)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)                \n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)                \n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)        \n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8) # B x N vector\n",
    "        out = torch.tanh(self.fc1(out))        \n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1386,  0.1147]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOT IN TEXT:** See how many layers in each model (use new package [torchinfo](https://github.com/TylerYep/torchinfo)). Note the resulting feature maps are summed up elementwise, e.g., for input 64 x 3 x 32 x 32, each kernel is 3 x 3 x 3, then the result from convolution of each kernel layer with each 3 x 32 x 32 image is 3 x 32 x 32 with zero-pooling. Then, sum this layer to get a 32 x 32 result. Repeat for each feature map desired for the output. If there are 16 desired, this will get us 16 x 32 x 32, which is what we get as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Net                                      [64, 2]                   --\n",
       "├─Conv2d: 1-1                            [64, 16, 32, 32]          448\n",
       "├─Conv2d: 1-2                            [64, 8, 16, 16]           1,160\n",
       "├─Linear: 1-3                            [64, 32]                  16,416\n",
       "├─Linear: 1-4                            [64, 2]                   66\n",
       "==========================================================================================\n",
       "Total params: 18,090\n",
       "Trainable params: 18,090\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 49.42\n",
       "==========================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 9.45\n",
       "Params size (MB): 0.07\n",
       "Estimated Total Size (MB): 10.31\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(64, 3, 32, 32)) # batch size is 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop (Warning: may take a while on cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader: # imgs is B x C x H x W so 64 x 3 x 32 x 32; labels is just a vector of dim 64\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item() # use item() to ignore gradient\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(f'{datetime.datetime.now()} Epoch {epoch}: Training Loss {loss_train/len(train_loader)}') # avg loss per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-08 00:15:01.124213 Epoch 1: Training Loss 0.5674498413398767\n",
      "2022-06-08 00:15:49.154168 Epoch 10: Training Loss 0.3255568215042163\n",
      "2022-06-08 00:16:42.845024 Epoch 20: Training Loss 0.29290795629950844\n",
      "2022-06-08 00:17:37.148855 Epoch 30: Training Loss 0.2671018430761471\n",
      "2022-06-08 00:18:31.143343 Epoch 40: Training Loss 0.2533294121930554\n",
      "2022-06-08 00:19:24.525692 Epoch 50: Training Loss 0.23610241549789526\n",
      "2022-06-08 00:20:09.347111 Epoch 60: Training Loss 0.22224653308178968\n",
      "2022-06-08 00:20:49.961938 Epoch 70: Training Loss 0.20748088784088756\n",
      "2022-06-08 00:21:26.820576 Epoch 80: Training Loss 0.19423881628710754\n",
      "2022-06-08 00:22:02.313284 Epoch 90: Training Loss 0.17889096706536164\n",
      "2022-06-08 00:22:38.271812 Epoch 100: Training Loss 0.16903056339568392\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net()\n",
    "n_epochs = 100\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(100, optimizer, model, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use dataloader because nn can't process the whole dataset at once\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "def validate(model, train_loader, test_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", test_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1) # index of highest value\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "        print(f\"Accuracy {name}: {correct/total:.2f}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.93\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "validate(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model (saves weights and biases, not the structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = ''\n",
    "torch.save(model.state_dict(), data_path + 'birds_vs_airplanes.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Net() # make sure to keep same definition of net\n",
    "loaded_model.load_state_dict(torch.load(data_path + 'birds_vs_airplanes.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on the GPU\n",
    "Will not work on my device. Move to Google Colab to try.\n",
    "\n",
    "Can move Tensors to GPU (will make a copy) by using ```Tensor.to``` and all model parameters to GPU (module instance is modified) by using ```Module.to```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f'Training on device {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefine training loop and validation for measuring accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            output = model(imgs)\n",
    "            loss = loss_fn(output, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(f'{datetime.datetime.now()} Epoch {epoch}, Training Loss {loss_train/len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-04 00:29:18.954691 Epoch 1, Training Loss 0.5606246351436445\n",
      "2022-06-04 00:30:09.509139 Epoch 10, Training Loss 0.32791446766276267\n",
      "2022-06-04 00:31:07.229105 Epoch 20, Training Loss 0.2964612185765224\n",
      "2022-06-04 00:32:03.882893 Epoch 30, Training Loss 0.27087216074489484\n",
      "2022-06-04 00:32:59.461303 Epoch 40, Training Loss 0.2494400188231924\n",
      "2022-06-04 00:33:53.695188 Epoch 50, Training Loss 0.23261178076077418\n",
      "2022-06-04 00:34:48.195049 Epoch 60, Training Loss 0.2194747927177484\n",
      "2022-06-04 00:35:42.548840 Epoch 70, Training Loss 0.20334107922330782\n",
      "2022-06-04 00:36:38.006351 Epoch 80, Training Loss 0.18753219334183224\n",
      "2022-06-04 00:37:33.358813 Epoch 90, Training Loss 0.17486945088881595\n",
      "2022-06-04 00:38:28.400128 Epoch 100, Training Loss 0.16154151172584788\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), 1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "training_loop(100, optimizer, model, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "def validate(model, train_loader, test_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"test\", test_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to(device=device)\n",
    "                labels = labels.to(device=device)\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())       \n",
    "        print(f'Accuracy {name}: {correct/total:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.93\n",
      "Accuracy test: 0.89\n"
     ]
    }
   ],
   "source": [
    "validate(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that PyTorch will attempt to load the weight to the same device it was saved from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Net().to(device=device)\n",
    "loaded_model.load_state_dict(torch.load(data_path + 'birds_vs_airplanes.pt', map_location=device)) # will override device info abt where to place weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design\n",
    "Not much in this book, but look in PyTorch hub or ```torchvision```--many loss functions and architectures are there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase width -- parameterize number of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1= n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1//2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(n_chans1//2 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2) # B x 3 x 32 x 32 -> B x 32 x 32 x 32 -> B x 32 x 16 x 16\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2) # B x 32 x 16 x 16 -> B x 16 x 16 x 16 -> B x 16 x 8 x 8\n",
    "        out = out.view(-1, n_chans1//2 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18090"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38386"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wide = NetWidth(32)\n",
    "sum(p.numel() for p in model_wide.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increased capacity of the model. But note this may lead to overfitting. Lets look at regularization to combat this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 regularization\n",
    "We'll implement l2 regularization. Note SGD optimizer already has a ```weight_decay``` parameter that I can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn, train_loader, l2_lambda=0.001):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            \n",
    "            out = model(imgs)\n",
    "            loss = loss_fn(out, labels)\n",
    "            \n",
    "            l2_norm = sum(p.pow(2.0).sum() for p in model.parameters()) # sums every weight squared across each layer\n",
    "            loss += l2_lambda * l2_norm # add a regularization term to the loss function, which is lambda * sum of each weight squared\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(f'{datetime.datetime.now()} Epoch {epoch}, Training loss {loss_train / len(train_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "Now implement dropout, where we zero out a random fraction of outputs of neurons across the network, making them more robust to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDropout(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1= n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1//2, kernel_size=3, padding=1)\n",
    "        self.conv2_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.fc1 = nn.Linear(n_chans1//2 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2) \n",
    "        out = self.conv1_dropout(out)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = self.conv2_dropout(out)\n",
    "        out = out.view(-1, n_chans1//2 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout will be zero-d out when not training automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetDropout(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1_dropout): Dropout2d(p=0.4, inplace=False)\n",
       "  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_dropout): Dropout2d(p=0.4, inplace=False)\n",
       "  (fc1): Linear(in_features=1024, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dropout = NetDropout()\n",
    "model_dropout.train() # sets module in training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetDropout(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1_dropout): Dropout2d(p=0.4, inplace=False)\n",
       "  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_dropout): Dropout2d(p=0.4, inplace=False)\n",
       "  (fc1): Linear(in_features=1024, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dropout.eval() # sets module in evaluation mode -- Dropout module responds accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "Next, we'll implement batch normalization, where we rescale the inputs to the activations of the network so that minibatches have a certain desirable distribution. This will help us avoid the inputs being too far in the saturated parts of the activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBatchNorm(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1= n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_batchnorm = nn.BatchNorm2d(num_features=n_chans1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1//2, kernel_size=3, padding=1)\n",
    "        self.conv2_batchnorm = nn.BatchNorm2d(num_features=n_chans1//2)\n",
    "        self.fc1 = nn.Linear(n_chans1//2 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1_batchnorm(self.conv1(x)) # apply batch normalization before activation function\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)  \n",
    "        out = self.conv2_batchnorm(self.conv2(out))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)        \n",
    "        out = out.view(-1, n_chans1//2 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to dropout, we want PyTorch to normalize only based on training data. Calling ```model.eval()``` will stop PyTorch from keeping estimates of the overall mean and variance from each mini-batch, using what it has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing depth\n",
    "Amplifies vanishing (exploding) gradient problem--very little (too much) update to each weight.\n",
    "\n",
    "Residual networks (ResNets) can help with this. They use a skip connection to short-circuit blocks of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDepth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1= n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)        \n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1//2, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1//2, n_chans1//2, kernel_size=3, padding=1) \n",
    "        self.fc1 = nn.Linear(n_chans1//2 * 4 * 4, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2) # B x 32 x 16 x 16     \n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2) # B x 16 x 8 x 8       \n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)), 2) # B x 16 x 4 x 4\n",
    "        out = out.view(-1, n_chans1//2 * 4 * 4)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetRes(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1= n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)        \n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1//2, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1//2, n_chans1//2, kernel_size=3, padding=1) \n",
    "        self.fc1 = nn.Linear(n_chans1//2 * 4 * 4, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out1 = out # saving output of first layer (identity mapping)\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2) # adding to output of third layer\n",
    "        out = out.view(-1, n_chans1//2 * 4 * 4)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNets make deeper parameters have a more direct connection with the loss. This allows for depths of 100 or more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating very deep networks\n",
    "Define a building block and build dynamically in a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias=False) # bias false because batch norm layer will cancel out effect of bias\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans) # helps prevent gradients from vanishing\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu') # custom initializations -- normal random elements with std as computed in ResNet paper\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetResDeep(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        # create sequential nn block, ensuring output of one resblock is input to the next\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out) # will traverse through all resblocks\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnetdp = NetResDeep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* Weight initialization is important! Make sure it's in a good place for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc\n",
    "Note: iterating over train_loader iterates through all the data in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for imgs, label in train_loader:\n",
    "    i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156.25"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cifar2)/64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutions and pooling is used as it helps us account for spatial relationships across the whole image--convolutions applied to increasingly smaller images will let us better see the bigger picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewNet(nn.Module): # will not work as is if kernel size is changed; need to change dims of linear layer\n",
    "    def __init__(self, n_chans1=32, kernel_size_1=3, kernel_size_2=3):\n",
    "        super().__init__()\n",
    "        self.n_chans1= n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=kernel_size_1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1//2, kernel_size=kernel_size_2, padding=1)\n",
    "        self.fc1 = nn.Linear(n_chans1//2 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2) # B x 3 x 32 x 32 -> B x 32 x 32 x 32 -> B x 32 x 16 x 16\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2) # B x 32 x 16 x 16 -> B x 16 x 16 x 16 -> B x 16 x 8 x 8\n",
    "        out = out.view(-1, self.n_chans1//2 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            out = model(imgs)\n",
    "            loss = loss_fn(out, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item() # use item() to ignore gradient\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(f'{datetime.datetime.now()} Epoch {epoch}: Training Loss {loss_train/len(train_loader)}') # avg loss per batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From PyTorch docs, note that\n",
    "$$H_{out} = \\left\\lfloor\\frac{H_{in} + 2*\\text{padding}[0] - \\text{dilation}[0]*(\\text{kernel_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor$$\n",
    "So, if $H_{in} = 5, \\text{kernel_size}=(5, 5), \\text{padding}=1$ and stride and dilation are their default values (both 1), then $H_{out} = \\left\\lfloor\\frac{32 + 2 - (5 - 1) - 1}{1} + 1\\right\\rfloor = 30$. $W_{out}$ follows the same formula, so is also 30.\n",
    "\n",
    "Now test 3 x 3 kernel vs. 5 x 5 kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[864, 32, 4608, 16, 32768, 32, 64, 2]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_model = NewNet(32, 3, 3)\n",
    "ex_numel_list = [p.numel() for p in ex_model.parameters()]\n",
    "ex_numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "NewNet                                   [64, 2]                   --\n",
       "├─Conv2d: 1-1                            [64, 32, 32, 32]          896\n",
       "├─Conv2d: 1-2                            [64, 16, 16, 16]          4,624\n",
       "├─Linear: 1-3                            [64, 32]                  32,800\n",
       "├─Linear: 1-4                            [64, 2]                   66\n",
       "==========================================================================================\n",
       "Total params: 38,386\n",
       "Trainable params: 38,386\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 136.58\n",
       "==========================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 18.89\n",
       "Params size (MB): 0.15\n",
       "Estimated Total Size (MB): 19.83\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(ex_model, input_size=(64, 3, 32, 32)) # batch size is 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2400, 32, 12800, 16, 32768, 32, 64, 2]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ex_model = NewNet(32, 5, 5)\n",
    "new_ex_numel_list = [p.numel() for p in new_ex_model.parameters()]\n",
    "new_ex_numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "NewNet                                   [36, 2]                   --\n",
       "├─Conv2d: 1-1                            [64, 32, 30, 30]          2,432\n",
       "├─Conv2d: 1-2                            [64, 16, 13, 13]          12,816\n",
       "├─Linear: 1-3                            [36, 32]                  32,800\n",
       "├─Linear: 1-4                            [36, 2]                   66\n",
       "==========================================================================================\n",
       "Total params: 48,114\n",
       "Trainable params: 48,114\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 279.88\n",
       "==========================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 16.14\n",
       "Params size (MB): 0.19\n",
       "Estimated Total Size (MB): 17.12\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(new_ex_model, input_size=(64, 3, 32, 32)) # batch size is 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why am I getting the batch size to be 36 at the end (the linear steps)? This is because I hardcode ```self.fc1 = nn.Linear(n_chans1//2 * 8 * 8, 32)```, which should only be true with the given kernel size. Otherwise, the dimensions of the output of the first two convolutional layers with pooling will be different. For example, with a 5 x 5 kernel, it should be ```self.fc1 = nn.Linear(n_chans1//2 * 6 * 6, 32)``` I think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiveNet(nn.Module):\n",
    "    def __init__(self, n_chans1=32, kernel_size_1=5, kernel_size_2=5):\n",
    "        super().__init__()\n",
    "        self.n_chans1= n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=kernel_size_1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1//2, kernel_size=kernel_size_2, padding=1)\n",
    "        self.fc1 = nn.Linear(n_chans1//2 * 6 * 6, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2) # B x 3 x 32 x 32 -> B x 32 x 32 x 32 -> B x 32 x 16 x 16\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2) # B x 32 x 16 x 16 -> B x 16 x 16 x 16 -> B x 16 x 8 x 8\n",
    "        out = out.view(-1, self.n_chans1//2 * 6 * 6)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FiveNet                                  [64, 2]                   --\n",
       "├─Conv2d: 1-1                            [64, 32, 30, 30]          2,432\n",
       "├─Conv2d: 1-2                            [64, 16, 13, 13]          12,816\n",
       "├─Linear: 1-3                            [64, 32]                  18,464\n",
       "├─Linear: 1-4                            [64, 2]                   66\n",
       "==========================================================================================\n",
       "Total params: 33,778\n",
       "Trainable params: 33,778\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 279.89\n",
       "==========================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 16.15\n",
       "Params size (MB): 0.14\n",
       "Estimated Total Size (MB): 17.07\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five_model = FiveNet()\n",
    "summary(five_model, input_size=(64, 3, 32, 32)) # batch size is 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-12 22:52:32.789849 Epoch 1: Training Loss 0.1333121230742734\n",
      "2022-06-12 22:53:41.470584 Epoch 10: Training Loss 0.11762520341072114\n",
      "2022-06-12 22:55:03.319294 Epoch 20: Training Loss 0.09843060482222184\n",
      "2022-06-12 22:56:15.226333 Epoch 30: Training Loss 0.0805586260879875\n",
      "2022-06-12 22:57:26.913775 Epoch 40: Training Loss 0.06581721312490998\n",
      "2022-06-12 22:58:39.839661 Epoch 50: Training Loss 0.05408415285526377\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "n_epochs = 50\n",
    "optimizer = optim.SGD(ex_model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "new_training_loop(n_epochs, optimizer, ex_model, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-13 00:52:21.485437 Epoch 1: Training Loss 0.5671150743202039\n",
      "2022-06-13 00:53:33.689738 Epoch 10: Training Loss 0.3164553333809421\n",
      "2022-06-13 00:54:55.064251 Epoch 20: Training Loss 0.27160359895343233\n",
      "2022-06-13 00:56:17.768859 Epoch 30: Training Loss 0.2309839193513439\n",
      "2022-06-13 00:57:36.224165 Epoch 40: Training Loss 0.19687850517072494\n",
      "2022-06-13 00:58:58.008023 Epoch 50: Training Loss 0.1620599454280677\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "n_epochs = 50\n",
    "optimizer = optim.SGD(five_model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "new_training_loop(n_epochs, optimizer, five_model, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the training error seems to increase with the 5 x 5 kernel vs. the 3 x 3 kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the formula\n",
    "$$H_{out} = \\left\\lfloor\\frac{H_{in} + 2*\\text{padding}[0] - \\text{dilation}[0]*(\\text{kernel_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor$$\n",
    "Letting ```kernel_size=(1, 3)``` we have\n",
    "$$H_{out} = \\left\\lfloor\\frac{32 + 2 - (1 - 1) - 1}{1} + 1\\right\\rfloor = 34$$\n",
    "$$W_{out} = \\left\\lfloor\\frac{32 + 2 - (3 - 1) - 1}{1} + 1\\right\\rfloor = 32$$\n",
    "So, after 2 conv layers with pooling, the dimensions of the height and width should be (34/2 + 2)/2 = 9 and (32/2)/2 = 8. So, the linear model will take ```n_chans1//2 * 9 * 8``` as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 34, 32])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testConv = nn.Conv2d(3, 32, kernel_size=(1, 3), padding=1)\n",
    "testConv(torch.zeros(64, 3, 32, 32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16, 19, 16])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testConv2 = nn.Conv2d(32, 16, kernel_size=(1, 3), padding=1)\n",
    "testConv2(torch.zeros(64, 32, 17, 16)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneThreeNet(nn.Module):\n",
    "    def __init__(self, n_chans1=32, kernel_size_1=(1, 3), kernel_size_2=(1, 3)):\n",
    "        super().__init__()\n",
    "        self.n_chans1= n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=kernel_size_1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1//2, kernel_size=kernel_size_2, padding=1)\n",
    "        self.fc1 = nn.Linear(n_chans1//2 * 9 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2) # B x 3 x 32 x 32 -> B x 32 x 32 x 32 -> B x 32 x 16 x 16\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2) # B x 32 x 16 x 16 -> B x 16 x 16 x 16 -> B x 16 x 8 x 8\n",
    "        out = out.view(-1, self.n_chans1//2 * 9 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "OneThreeNet                              [64, 2]                   --\n",
       "├─Conv2d: 1-1                            [64, 32, 34, 32]          320\n",
       "├─Conv2d: 1-2                            [64, 16, 19, 16]          1,552\n",
       "├─Linear: 1-3                            [64, 32]                  36,896\n",
       "├─Linear: 1-4                            [64, 2]                   66\n",
       "==========================================================================================\n",
       "Total params: 38,834\n",
       "Trainable params: 38,834\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 54.84\n",
       "==========================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 20.33\n",
       "Params size (MB): 0.16\n",
       "Estimated Total Size (MB): 21.28\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneThreeModel = OneThreeNet()\n",
    "summary(oneThreeModel, input_size=(64, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, with this asymmetric kernel, the height and width of the output images after each convolution will be different."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learn",
   "language": "python",
   "name": "deep_learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
