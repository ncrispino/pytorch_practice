{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple model fitting\n",
    "using temperature data--try to convert unknown units to Celsius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0] # Celsius\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4] # unknown\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying a linear model, i.e., estimate w and b s.t. the error is minimized and $t_c = w t_u + b$.\n",
    "\n",
    "Need to choose a loss function, which the learning algorithm attempts to minimize. Consider squared error and absolute error. Note that squared error greater penalizes wildly wrong results--prefers to have more slightly off errors than a few very off errors.\n",
    "\n",
    "Define model and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b\n",
    "\n",
    "def loss_fn(t_p, t_c):\n",
    "    '''\n",
    "    Mean squared loss\n",
    "    '''\n",
    "    sq_diffs = (t_p - t_c)**2\n",
    "    return sq_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
       "        48.4000, 60.4000, 68.4000])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.ones(())\n",
    "b = torch.zeros(())\n",
    "\n",
    "t_p = model(t_u, w, b)\n",
    "t_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8848)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(t_p, t_c)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside\n",
    "pytorch uses broadcasting, meaning we can add a scalar to a multidim tensor and it will broadcast that scalar up in dimensions so that the result makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: x: torch.Size([]), y: torch.Size([3, 1])\n",
      "\tz: torch.Size([1, 3]), a: torch.Size([2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(())\n",
    "y = torch.ones(3, 1)\n",
    "z = torch.ones(1, 3)\n",
    "a = torch.ones(2, 1, 1)\n",
    "print(f'shapes: x: {x.shape}, y: {y.shape}\\n\\tz: {z.shape}, a: {a.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x * y).shape # scalar * elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y * z).shape # matrix mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y * a).shape # broadcast the last 2 dimensions of a to 3, 1 to match y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a * z).shape # broadcast the last 2 dimensions of a to 1, 3 to match z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4517.2974)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta = 0.1\n",
    "loss_rate_of_change_w = (loss_fn(model(t_u, w + delta, b), t_c) - loss_fn(model(t_u, w - delta, b), t_c))/(2.0 * delta)\n",
    "loss_rate_of_change_w # in some neighborhood of w and b, a unit increase in w changes the loss by this amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change w proportionally to loss and change slowly because the rate of change could differ dramatically even in small neighborhoods around w. So, learning is scaled by a small factor called $\\eta$, the learning rate.\n",
    "\n",
    "If change is negative, then need to increase the weight to minimize loss, as a unit increase in w will decrease loss.\\\n",
    "If change is positive, then need to decrease the weight to minimize loss, as a unit increase in w will increase loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "w = w - learning_rate*loss_rate_of_change_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2603314.)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_rate_of_change_b = (loss_fn(model(t_u, w, b + delta), t_c) - loss_fn(model(t_u, w, b - delta), t_c))/(2.0 * delta)\n",
    "b = b - learning_rate*loss_rate_of_change_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now solve analytically (i.e., without arbitrary delta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dloss_fn(t_p, t_c):\n",
    "    dsq_diffs = 2 * (t_p - t_c) / t_p.shape[0]\n",
    "    return dsq_diffs\n",
    "\n",
    "def dmodel_dw(t_u, w, b):\n",
    "    return t_u\n",
    "\n",
    "def dmodel_db(t_u, w, b):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By chain rule,\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial t_p}\\frac{\\partial t_p}{\\partial w}$$ similarly for b and\n",
    "$$\\nabla_{w, b}L = \\left(\\frac{\\partial L}{\\partial w}, \\frac{\\partial L}{\\partial b}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fn(t_u, t_c, t_p, w, b):\n",
    "    dloss_dtp = dloss_fn(t_p, t_c)\n",
    "    dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)\n",
    "    dloss_db = dloss_dtp * dmodel_db(t_u, w, b)\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()]) # need to sum because we want one scalar for each term in gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(n_epochs):\n",
    "        w, b = params\n",
    "        t_p = model(t_u, w, b) # forward pass\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b) # backward pass         \n",
    "        params -= learning_rate*grad\n",
    "        print(f'Epoch {epoch}, Loss {float(loss)}')\n",
    "        print(f'\\t Params: {params}')\n",
    "        print(f'\\t Grad: {grad}')\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 1763.884765625\n",
      "\t Params: tensor([-44.1730,  -0.8260])\n",
      "\t Grad: tensor([4517.2964,   82.6000])\n",
      "Epoch 1, Loss 5802484.5\n",
      "\t Params: tensor([2568.4011,   45.1637])\n",
      "\t Grad: tensor([-261257.4062,   -4598.9702])\n",
      "Epoch 2, Loss 19408029696.0\n",
      "\t Params: tensor([-148527.7344,   -2616.3931])\n",
      "\t Grad: tensor([15109614.0000,   266155.6875])\n",
      "Epoch 3, Loss 64915905708032.0\n",
      "\t Params: tensor([8589999.0000,  151310.8906])\n",
      "\t Grad: tensor([-8.7385e+08, -1.5393e+07])\n",
      "Epoch 4, Loss 2.1713052546105344e+17\n",
      "\t Params: tensor([-4.9680e+08, -8.7510e+06])\n",
      "\t Grad: tensor([5.0539e+10, 8.9023e+08])\n",
      "Epoch 5, Loss 7.262575831529281e+20\n",
      "\t Params: tensor([2.8732e+10, 5.0610e+08])\n",
      "\t Grad: tensor([-2.9229e+12, -5.1486e+10])\n",
      "Epoch 6, Loss 2.429183416467663e+24\n",
      "\t Params: tensor([-1.6617e+12, -2.9270e+10])\n",
      "\t Grad: tensor([1.6904e+14, 2.9776e+12])\n",
      "Epoch 7, Loss 8.125122549611731e+27\n",
      "\t Params: tensor([9.6102e+13, 1.6928e+12])\n",
      "\t Grad: tensor([-9.7764e+15, -1.7221e+14])\n",
      "Epoch 8, Loss 2.717688212084259e+31\n",
      "\t Params: tensor([-5.5580e+15, -9.7903e+13])\n",
      "\t Grad: tensor([5.6541e+17, 9.9596e+15])\n",
      "Epoch 9, Loss 9.090110518901907e+34\n",
      "\t Params: tensor([3.2144e+17, 5.6621e+15])\n",
      "\t Grad: tensor([-3.2700e+19, -5.7600e+17])\n",
      "Epoch 10, Loss inf\n",
      "\t Params: tensor([-1.8590e+19, -3.2746e+17])\n",
      "\t Grad: tensor([1.8912e+21, 3.3313e+19])\n",
      "Epoch 11, Loss inf\n",
      "\t Params: tensor([1.0752e+21, 1.8939e+19])\n",
      "\t Grad: tensor([-1.0937e+23, -1.9266e+21])\n",
      "Epoch 12, Loss inf\n",
      "\t Params: tensor([-6.2181e+22, -1.0953e+21])\n",
      "\t Grad: tensor([6.3256e+24, 1.1142e+23])\n",
      "Epoch 13, Loss inf\n",
      "\t Params: tensor([3.5962e+24, 6.3346e+22])\n",
      "\t Grad: tensor([-3.6584e+26, -6.4441e+24])\n",
      "Epoch 14, Loss inf\n",
      "\t Params: tensor([-2.0798e+26, -3.6636e+24])\n",
      "\t Grad: tensor([2.1158e+28, 3.7269e+26])\n",
      "Epoch 15, Loss inf\n",
      "\t Params: tensor([1.2028e+28, 2.1188e+26])\n",
      "\t Grad: tensor([-1.2236e+30, -2.1554e+28])\n",
      "Epoch 16, Loss inf\n",
      "\t Params: tensor([-6.9566e+29, -1.2254e+28])\n",
      "\t Grad: tensor([7.0769e+31, 1.2466e+30])\n",
      "Epoch 17, Loss inf\n",
      "\t Params: tensor([4.0233e+31, 7.0869e+29])\n",
      "\t Grad: tensor([-4.0929e+33, -7.2095e+31])\n",
      "Epoch 18, Loss inf\n",
      "\t Params: tensor([-2.3268e+33, -4.0987e+31])\n",
      "\t Grad: tensor([2.3671e+35, 4.1695e+33])\n",
      "Epoch 19, Loss inf\n",
      "\t Params: tensor([1.3457e+35, 2.3704e+33])\n",
      "\t Grad: tensor([-1.3690e+37, -2.4114e+35])\n",
      "Epoch 20, Loss inf\n",
      "\t Params: tensor([       -inf, -1.3709e+35])\n",
      "\t Grad: tensor([       inf, 1.3946e+37])\n",
      "Epoch 21, Loss inf\n",
      "\t Params: tensor([nan, inf])\n",
      "\t Grad: tensor([-inf, -inf])\n",
      "Epoch 22, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 23, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 24, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 25, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 26, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 27, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 28, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 29, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 30, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 31, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 32, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 33, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 34, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 35, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 36, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 37, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 38, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 39, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 40, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 41, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 42, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 43, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 44, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 45, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 46, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 47, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 48, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 49, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 50, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 51, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 52, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 53, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 54, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 55, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 56, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 57, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 58, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 59, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 60, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 61, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 62, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 63, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 64, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 65, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 66, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 67, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 68, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 69, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 70, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 71, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 72, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 73, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 74, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 75, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 76, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 77, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 78, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 79, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 80, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 81, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 82, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 83, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 84, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 85, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 86, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 87, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 88, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 89, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 90, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 91, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 92, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 93, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 94, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 95, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 96, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 97, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 98, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n",
      "Epoch 99, Loss nan\n",
      "\t Params: tensor([nan, nan])\n",
      "\t Grad: tensor([nan, nan])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(100, 1e-2, torch.tensor([1.0, 0.0]), t_u, t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter values great increased -- losses approached inf. Want convergence, not divergence. Choose smaller learning rate to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 1763.884765625\n",
      "\t Params: tensor([ 0.5483, -0.0083])\n",
      "\t Grad: tensor([4517.2964,   82.6000])\n",
      "Epoch 1, Loss 323.09051513671875\n",
      "\t Params: tensor([ 0.3623, -0.0118])\n",
      "\t Grad: tensor([1859.5493,   35.7843])\n",
      "Epoch 2, Loss 78.92963409423828\n",
      "\t Params: tensor([ 0.2858, -0.0135])\n",
      "\t Grad: tensor([765.4666,  16.5122])\n",
      "Epoch 3, Loss 37.5528450012207\n",
      "\t Params: tensor([ 0.2543, -0.0143])\n",
      "\t Grad: tensor([315.0790,   8.5787])\n",
      "Epoch 4, Loss 30.540283203125\n",
      "\t Params: tensor([ 0.2413, -0.0149])\n",
      "\t Grad: tensor([129.6733,   5.3127])\n",
      "Epoch 5, Loss 29.351154327392578\n",
      "\t Params: tensor([ 0.2360, -0.0153])\n",
      "\t Grad: tensor([53.3495,  3.9682])\n",
      "Epoch 6, Loss 29.148883819580078\n",
      "\t Params: tensor([ 0.2338, -0.0156])\n",
      "\t Grad: tensor([21.9304,  3.4148])\n",
      "Epoch 7, Loss 29.113847732543945\n",
      "\t Params: tensor([ 0.2329, -0.0159])\n",
      "\t Grad: tensor([8.9964, 3.1869])\n",
      "Epoch 8, Loss 29.107145309448242\n",
      "\t Params: tensor([ 0.2325, -0.0162])\n",
      "\t Grad: tensor([3.6721, 3.0930])\n",
      "Epoch 9, Loss 29.105247497558594\n",
      "\t Params: tensor([ 0.2324, -0.0166])\n",
      "\t Grad: tensor([1.4803, 3.0544])\n",
      "Epoch 10, Loss 29.104167938232422\n",
      "\t Params: tensor([ 0.2323, -0.0169])\n",
      "\t Grad: tensor([0.5781, 3.0384])\n",
      "Epoch 11, Loss 29.103221893310547\n",
      "\t Params: tensor([ 0.2323, -0.0172])\n",
      "\t Grad: tensor([0.2066, 3.0318])\n",
      "Epoch 12, Loss 29.102294921875\n",
      "\t Params: tensor([ 0.2323, -0.0175])\n",
      "\t Grad: tensor([0.0537, 3.0291])\n",
      "Epoch 13, Loss 29.10137939453125\n",
      "\t Params: tensor([ 0.2323, -0.0178])\n",
      "\t Grad: tensor([-0.0093,  3.0279])\n",
      "Epoch 14, Loss 29.100465774536133\n",
      "\t Params: tensor([ 0.2323, -0.0181])\n",
      "\t Grad: tensor([-0.0353,  3.0274])\n",
      "Epoch 15, Loss 29.09954833984375\n",
      "\t Params: tensor([ 0.2323, -0.0184])\n",
      "\t Grad: tensor([-0.0459,  3.0272])\n",
      "Epoch 16, Loss 29.098630905151367\n",
      "\t Params: tensor([ 0.2323, -0.0187])\n",
      "\t Grad: tensor([-0.0502,  3.0270])\n",
      "Epoch 17, Loss 29.09771728515625\n",
      "\t Params: tensor([ 0.2323, -0.0190])\n",
      "\t Grad: tensor([-0.0520,  3.0270])\n",
      "Epoch 18, Loss 29.0967960357666\n",
      "\t Params: tensor([ 0.2323, -0.0193])\n",
      "\t Grad: tensor([-0.0528,  3.0269])\n",
      "Epoch 19, Loss 29.09588050842285\n",
      "\t Params: tensor([ 0.2323, -0.0196])\n",
      "\t Grad: tensor([-0.0531,  3.0268])\n",
      "Epoch 20, Loss 29.094959259033203\n",
      "\t Params: tensor([ 0.2323, -0.0199])\n",
      "\t Grad: tensor([-0.0533,  3.0268])\n",
      "Epoch 21, Loss 29.09404945373535\n",
      "\t Params: tensor([ 0.2323, -0.0202])\n",
      "\t Grad: tensor([-0.0533,  3.0267])\n",
      "Epoch 22, Loss 29.0931339263916\n",
      "\t Params: tensor([ 0.2323, -0.0205])\n",
      "\t Grad: tensor([-0.0533,  3.0267])\n",
      "Epoch 23, Loss 29.09221649169922\n",
      "\t Params: tensor([ 0.2323, -0.0208])\n",
      "\t Grad: tensor([-0.0533,  3.0266])\n",
      "Epoch 24, Loss 29.09130096435547\n",
      "\t Params: tensor([ 0.2323, -0.0211])\n",
      "\t Grad: tensor([-0.0533,  3.0266])\n",
      "Epoch 25, Loss 29.09038543701172\n",
      "\t Params: tensor([ 0.2323, -0.0214])\n",
      "\t Grad: tensor([-0.0533,  3.0265])\n",
      "Epoch 26, Loss 29.08946418762207\n",
      "\t Params: tensor([ 0.2323, -0.0217])\n",
      "\t Grad: tensor([-0.0533,  3.0265])\n",
      "Epoch 27, Loss 29.088550567626953\n",
      "\t Params: tensor([ 0.2323, -0.0220])\n",
      "\t Grad: tensor([-0.0532,  3.0264])\n",
      "Epoch 28, Loss 29.087635040283203\n",
      "\t Params: tensor([ 0.2323, -0.0223])\n",
      "\t Grad: tensor([-0.0533,  3.0264])\n",
      "Epoch 29, Loss 29.086713790893555\n",
      "\t Params: tensor([ 0.2323, -0.0226])\n",
      "\t Grad: tensor([-0.0533,  3.0263])\n",
      "Epoch 30, Loss 29.085803985595703\n",
      "\t Params: tensor([ 0.2324, -0.0229])\n",
      "\t Grad: tensor([-0.0532,  3.0262])\n",
      "Epoch 31, Loss 29.084888458251953\n",
      "\t Params: tensor([ 0.2324, -0.0232])\n",
      "\t Grad: tensor([-0.0533,  3.0262])\n",
      "Epoch 32, Loss 29.083967208862305\n",
      "\t Params: tensor([ 0.2324, -0.0235])\n",
      "\t Grad: tensor([-0.0533,  3.0261])\n",
      "Epoch 33, Loss 29.083057403564453\n",
      "\t Params: tensor([ 0.2324, -0.0238])\n",
      "\t Grad: tensor([-0.0533,  3.0261])\n",
      "Epoch 34, Loss 29.082141876220703\n",
      "\t Params: tensor([ 0.2324, -0.0241])\n",
      "\t Grad: tensor([-0.0532,  3.0260])\n",
      "Epoch 35, Loss 29.081220626831055\n",
      "\t Params: tensor([ 0.2324, -0.0244])\n",
      "\t Grad: tensor([-0.0533,  3.0260])\n",
      "Epoch 36, Loss 29.08030891418457\n",
      "\t Params: tensor([ 0.2324, -0.0247])\n",
      "\t Grad: tensor([-0.0533,  3.0259])\n",
      "Epoch 37, Loss 29.079389572143555\n",
      "\t Params: tensor([ 0.2324, -0.0250])\n",
      "\t Grad: tensor([-0.0532,  3.0259])\n",
      "Epoch 38, Loss 29.078474044799805\n",
      "\t Params: tensor([ 0.2324, -0.0253])\n",
      "\t Grad: tensor([-0.0533,  3.0258])\n",
      "Epoch 39, Loss 29.07756233215332\n",
      "\t Params: tensor([ 0.2324, -0.0256])\n",
      "\t Grad: tensor([-0.0533,  3.0258])\n",
      "Epoch 40, Loss 29.076648712158203\n",
      "\t Params: tensor([ 0.2324, -0.0259])\n",
      "\t Grad: tensor([-0.0533,  3.0257])\n",
      "Epoch 41, Loss 29.07573127746582\n",
      "\t Params: tensor([ 0.2324, -0.0262])\n",
      "\t Grad: tensor([-0.0532,  3.0257])\n",
      "Epoch 42, Loss 29.074811935424805\n",
      "\t Params: tensor([ 0.2324, -0.0265])\n",
      "\t Grad: tensor([-0.0533,  3.0256])\n",
      "Epoch 43, Loss 29.073894500732422\n",
      "\t Params: tensor([ 0.2324, -0.0268])\n",
      "\t Grad: tensor([-0.0533,  3.0256])\n",
      "Epoch 44, Loss 29.072980880737305\n",
      "\t Params: tensor([ 0.2324, -0.0271])\n",
      "\t Grad: tensor([-0.0533,  3.0255])\n",
      "Epoch 45, Loss 29.07206916809082\n",
      "\t Params: tensor([ 0.2324, -0.0274])\n",
      "\t Grad: tensor([-0.0533,  3.0254])\n",
      "Epoch 46, Loss 29.071147918701172\n",
      "\t Params: tensor([ 0.2324, -0.0277])\n",
      "\t Grad: tensor([-0.0533,  3.0254])\n",
      "Epoch 47, Loss 29.070234298706055\n",
      "\t Params: tensor([ 0.2324, -0.0281])\n",
      "\t Grad: tensor([-0.0533,  3.0253])\n",
      "Epoch 48, Loss 29.06932258605957\n",
      "\t Params: tensor([ 0.2325, -0.0284])\n",
      "\t Grad: tensor([-0.0533,  3.0253])\n",
      "Epoch 49, Loss 29.068401336669922\n",
      "\t Params: tensor([ 0.2325, -0.0287])\n",
      "\t Grad: tensor([-0.0532,  3.0252])\n",
      "Epoch 50, Loss 29.067485809326172\n",
      "\t Params: tensor([ 0.2325, -0.0290])\n",
      "\t Grad: tensor([-0.0533,  3.0252])\n",
      "Epoch 51, Loss 29.066566467285156\n",
      "\t Params: tensor([ 0.2325, -0.0293])\n",
      "\t Grad: tensor([-0.0533,  3.0251])\n",
      "Epoch 52, Loss 29.065656661987305\n",
      "\t Params: tensor([ 0.2325, -0.0296])\n",
      "\t Grad: tensor([-0.0533,  3.0251])\n",
      "Epoch 53, Loss 29.064741134643555\n",
      "\t Params: tensor([ 0.2325, -0.0299])\n",
      "\t Grad: tensor([-0.0533,  3.0250])\n",
      "Epoch 54, Loss 29.063825607299805\n",
      "\t Params: tensor([ 0.2325, -0.0302])\n",
      "\t Grad: tensor([-0.0532,  3.0250])\n",
      "Epoch 55, Loss 29.062910079956055\n",
      "\t Params: tensor([ 0.2325, -0.0305])\n",
      "\t Grad: tensor([-0.0533,  3.0249])\n",
      "Epoch 56, Loss 29.061994552612305\n",
      "\t Params: tensor([ 0.2325, -0.0308])\n",
      "\t Grad: tensor([-0.0532,  3.0249])\n",
      "Epoch 57, Loss 29.061079025268555\n",
      "\t Params: tensor([ 0.2325, -0.0311])\n",
      "\t Grad: tensor([-0.0533,  3.0248])\n",
      "Epoch 58, Loss 29.060169219970703\n",
      "\t Params: tensor([ 0.2325, -0.0314])\n",
      "\t Grad: tensor([-0.0533,  3.0248])\n",
      "Epoch 59, Loss 29.059247970581055\n",
      "\t Params: tensor([ 0.2325, -0.0317])\n",
      "\t Grad: tensor([-0.0533,  3.0247])\n",
      "Epoch 60, Loss 29.05833625793457\n",
      "\t Params: tensor([ 0.2325, -0.0320])\n",
      "\t Grad: tensor([-0.0533,  3.0247])\n",
      "Epoch 61, Loss 29.057415008544922\n",
      "\t Params: tensor([ 0.2325, -0.0323])\n",
      "\t Grad: tensor([-0.0534,  3.0246])\n",
      "Epoch 62, Loss 29.056507110595703\n",
      "\t Params: tensor([ 0.2325, -0.0326])\n",
      "\t Grad: tensor([-0.0533,  3.0245])\n",
      "Epoch 63, Loss 29.055585861206055\n",
      "\t Params: tensor([ 0.2325, -0.0329])\n",
      "\t Grad: tensor([-0.0532,  3.0245])\n",
      "Epoch 64, Loss 29.05467414855957\n",
      "\t Params: tensor([ 0.2325, -0.0332])\n",
      "\t Grad: tensor([-0.0533,  3.0244])\n",
      "Epoch 65, Loss 29.053760528564453\n",
      "\t Params: tensor([ 0.2325, -0.0335])\n",
      "\t Grad: tensor([-0.0533,  3.0244])\n",
      "Epoch 66, Loss 29.05284309387207\n",
      "\t Params: tensor([ 0.2325, -0.0338])\n",
      "\t Grad: tensor([-0.0533,  3.0243])\n",
      "Epoch 67, Loss 29.051929473876953\n",
      "\t Params: tensor([ 0.2326, -0.0341])\n",
      "\t Grad: tensor([-0.0532,  3.0243])\n",
      "Epoch 68, Loss 29.05101203918457\n",
      "\t Params: tensor([ 0.2326, -0.0344])\n",
      "\t Grad: tensor([-0.0533,  3.0242])\n",
      "Epoch 69, Loss 29.050098419189453\n",
      "\t Params: tensor([ 0.2326, -0.0347])\n",
      "\t Grad: tensor([-0.0532,  3.0242])\n",
      "Epoch 70, Loss 29.049182891845703\n",
      "\t Params: tensor([ 0.2326, -0.0350])\n",
      "\t Grad: tensor([-0.0533,  3.0241])\n",
      "Epoch 71, Loss 29.04827308654785\n",
      "\t Params: tensor([ 0.2326, -0.0353])\n",
      "\t Grad: tensor([-0.0533,  3.0241])\n",
      "Epoch 72, Loss 29.04734992980957\n",
      "\t Params: tensor([ 0.2326, -0.0356])\n",
      "\t Grad: tensor([-0.0532,  3.0240])\n",
      "Epoch 73, Loss 29.04644203186035\n",
      "\t Params: tensor([ 0.2326, -0.0359])\n",
      "\t Grad: tensor([-0.0533,  3.0240])\n",
      "Epoch 74, Loss 29.045530319213867\n",
      "\t Params: tensor([ 0.2326, -0.0362])\n",
      "\t Grad: tensor([-0.0532,  3.0239])\n",
      "Epoch 75, Loss 29.04461097717285\n",
      "\t Params: tensor([ 0.2326, -0.0365])\n",
      "\t Grad: tensor([-0.0533,  3.0239])\n",
      "Epoch 76, Loss 29.043699264526367\n",
      "\t Params: tensor([ 0.2326, -0.0368])\n",
      "\t Grad: tensor([-0.0533,  3.0238])\n",
      "Epoch 77, Loss 29.042783737182617\n",
      "\t Params: tensor([ 0.2326, -0.0371])\n",
      "\t Grad: tensor([-0.0533,  3.0238])\n",
      "Epoch 78, Loss 29.0418701171875\n",
      "\t Params: tensor([ 0.2326, -0.0374])\n",
      "\t Grad: tensor([-0.0533,  3.0237])\n",
      "Epoch 79, Loss 29.04095458984375\n",
      "\t Params: tensor([ 0.2326, -0.0377])\n",
      "\t Grad: tensor([-0.0532,  3.0236])\n",
      "Epoch 80, Loss 29.0400390625\n",
      "\t Params: tensor([ 0.2326, -0.0380])\n",
      "\t Grad: tensor([-0.0534,  3.0236])\n",
      "Epoch 81, Loss 29.039121627807617\n",
      "\t Params: tensor([ 0.2326, -0.0383])\n",
      "\t Grad: tensor([-0.0533,  3.0235])\n",
      "Epoch 82, Loss 29.038209915161133\n",
      "\t Params: tensor([ 0.2326, -0.0386])\n",
      "\t Grad: tensor([-0.0532,  3.0235])\n",
      "Epoch 83, Loss 29.037294387817383\n",
      "\t Params: tensor([ 0.2326, -0.0389])\n",
      "\t Grad: tensor([-0.0533,  3.0234])\n",
      "Epoch 84, Loss 29.036378860473633\n",
      "\t Params: tensor([ 0.2326, -0.0392])\n",
      "\t Grad: tensor([-0.0533,  3.0234])\n",
      "Epoch 85, Loss 29.035463333129883\n",
      "\t Params: tensor([ 0.2326, -0.0395])\n",
      "\t Grad: tensor([-0.0532,  3.0233])\n",
      "Epoch 86, Loss 29.03455352783203\n",
      "\t Params: tensor([ 0.2327, -0.0398])\n",
      "\t Grad: tensor([-0.0533,  3.0233])\n",
      "Epoch 87, Loss 29.03363609313965\n",
      "\t Params: tensor([ 0.2327, -0.0401])\n",
      "\t Grad: tensor([-0.0532,  3.0232])\n",
      "Epoch 88, Loss 29.03272247314453\n",
      "\t Params: tensor([ 0.2327, -0.0405])\n",
      "\t Grad: tensor([-0.0533,  3.0232])\n",
      "Epoch 89, Loss 29.031810760498047\n",
      "\t Params: tensor([ 0.2327, -0.0408])\n",
      "\t Grad: tensor([-0.0533,  3.0231])\n",
      "Epoch 90, Loss 29.030895233154297\n",
      "\t Params: tensor([ 0.2327, -0.0411])\n",
      "\t Grad: tensor([-0.0532,  3.0231])\n",
      "Epoch 91, Loss 29.02997589111328\n",
      "\t Params: tensor([ 0.2327, -0.0414])\n",
      "\t Grad: tensor([-0.0532,  3.0230])\n",
      "Epoch 92, Loss 29.02906608581543\n",
      "\t Params: tensor([ 0.2327, -0.0417])\n",
      "\t Grad: tensor([-0.0533,  3.0230])\n",
      "Epoch 93, Loss 29.02815055847168\n",
      "\t Params: tensor([ 0.2327, -0.0420])\n",
      "\t Grad: tensor([-0.0532,  3.0229])\n",
      "Epoch 94, Loss 29.02723503112793\n",
      "\t Params: tensor([ 0.2327, -0.0423])\n",
      "\t Grad: tensor([-0.0533,  3.0229])\n",
      "Epoch 95, Loss 29.026323318481445\n",
      "\t Params: tensor([ 0.2327, -0.0426])\n",
      "\t Grad: tensor([-0.0533,  3.0228])\n",
      "Epoch 96, Loss 29.025409698486328\n",
      "\t Params: tensor([ 0.2327, -0.0429])\n",
      "\t Grad: tensor([-0.0532,  3.0227])\n",
      "Epoch 97, Loss 29.024492263793945\n",
      "\t Params: tensor([ 0.2327, -0.0432])\n",
      "\t Grad: tensor([-0.0532,  3.0227])\n",
      "Epoch 98, Loss 29.023582458496094\n",
      "\t Params: tensor([ 0.2327, -0.0435])\n",
      "\t Grad: tensor([-0.0533,  3.0226])\n",
      "Epoch 99, Loss 29.022666931152344\n",
      "\t Params: tensor([ 0.2327, -0.0438])\n",
      "\t Grad: tensor([-0.0532,  3.0226])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2327, -0.0438])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(100, 1e-4, torch.tensor([1.0, 0.0]), t_u, t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter updates are small -- could change this by making learning_rate adaptive.\n",
    "\n",
    "Also, the first-epoch gradient for the weight is much larger (~50x) than the gradient for the bias. Try to normalize weights so that we can use the same learning rate for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_un = 0.1 * t_u # shrinks inputs so gradient isn't too different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 80.36434173583984\n",
      "\t Params: tensor([1.7761, 0.1064])\n",
      "\t Grad: tensor([-77.6140, -10.6400])\n",
      "Epoch 1, Loss 37.574913024902344\n",
      "\t Params: tensor([2.0848, 0.1303])\n",
      "\t Grad: tensor([-30.8623,  -2.3864])\n",
      "Epoch 2, Loss 30.871076583862305\n",
      "\t Params: tensor([2.2094, 0.1217])\n",
      "\t Grad: tensor([-12.4631,   0.8587])\n",
      "Epoch 3, Loss 29.756193161010742\n",
      "\t Params: tensor([2.2616, 0.1004])\n",
      "\t Grad: tensor([-5.2218,  2.1327])\n",
      "Epoch 4, Loss 29.507152557373047\n",
      "\t Params: tensor([2.2853, 0.0740])\n",
      "\t Grad: tensor([-2.3715,  2.6310])\n",
      "Epoch 5, Loss 29.3924560546875\n",
      "\t Params: tensor([2.2978, 0.0458])\n",
      "\t Grad: tensor([-1.2492,  2.8241])\n",
      "Epoch 6, Loss 29.298828125\n",
      "\t Params: tensor([2.3059, 0.0168])\n",
      "\t Grad: tensor([-0.8071,  2.8970])\n",
      "Epoch 7, Loss 29.208717346191406\n",
      "\t Params: tensor([ 2.3122, -0.0124])\n",
      "\t Grad: tensor([-0.6325,  2.9227])\n",
      "Epoch 8, Loss 29.119415283203125\n",
      "\t Params: tensor([ 2.3178, -0.0417])\n",
      "\t Grad: tensor([-0.5633,  2.9298])\n",
      "Epoch 9, Loss 29.030488967895508\n",
      "\t Params: tensor([ 2.3232, -0.0710])\n",
      "\t Grad: tensor([-0.5355,  2.9295])\n",
      "Epoch 10, Loss 28.941877365112305\n",
      "\t Params: tensor([ 2.3284, -0.1003])\n",
      "\t Grad: tensor([-0.5240,  2.9264])\n",
      "Epoch 11, Loss 28.853565216064453\n",
      "\t Params: tensor([ 2.3336, -0.1295])\n",
      "\t Grad: tensor([-0.5190,  2.9222])\n",
      "Epoch 12, Loss 28.765552520751953\n",
      "\t Params: tensor([ 2.3388, -0.1587])\n",
      "\t Grad: tensor([-0.5165,  2.9175])\n",
      "Epoch 13, Loss 28.6778507232666\n",
      "\t Params: tensor([ 2.3439, -0.1878])\n",
      "\t Grad: tensor([-0.5150,  2.9126])\n",
      "Epoch 14, Loss 28.590431213378906\n",
      "\t Params: tensor([ 2.3491, -0.2169])\n",
      "\t Grad: tensor([-0.5138,  2.9077])\n",
      "Epoch 15, Loss 28.503318786621094\n",
      "\t Params: tensor([ 2.3542, -0.2459])\n",
      "\t Grad: tensor([-0.5129,  2.9028])\n",
      "Epoch 16, Loss 28.4164981842041\n",
      "\t Params: tensor([ 2.3593, -0.2749])\n",
      "\t Grad: tensor([-0.5120,  2.8979])\n",
      "Epoch 17, Loss 28.329973220825195\n",
      "\t Params: tensor([ 2.3644, -0.3038])\n",
      "\t Grad: tensor([-0.5111,  2.8930])\n",
      "Epoch 18, Loss 28.243741989135742\n",
      "\t Params: tensor([ 2.3695, -0.3327])\n",
      "\t Grad: tensor([-0.5102,  2.8881])\n",
      "Epoch 19, Loss 28.157804489135742\n",
      "\t Params: tensor([ 2.3746, -0.3615])\n",
      "\t Grad: tensor([-0.5093,  2.8832])\n",
      "Epoch 20, Loss 28.07215118408203\n",
      "\t Params: tensor([ 2.3797, -0.3903])\n",
      "\t Grad: tensor([-0.5084,  2.8783])\n",
      "Epoch 21, Loss 27.986797332763672\n",
      "\t Params: tensor([ 2.3848, -0.4190])\n",
      "\t Grad: tensor([-0.5076,  2.8734])\n",
      "Epoch 22, Loss 27.9017276763916\n",
      "\t Params: tensor([ 2.3899, -0.4477])\n",
      "\t Grad: tensor([-0.5067,  2.8685])\n",
      "Epoch 23, Loss 27.81694984436035\n",
      "\t Params: tensor([ 2.3949, -0.4763])\n",
      "\t Grad: tensor([-0.5059,  2.8636])\n",
      "Epoch 24, Loss 27.732463836669922\n",
      "\t Params: tensor([ 2.4000, -0.5049])\n",
      "\t Grad: tensor([-0.5050,  2.8588])\n",
      "Epoch 25, Loss 27.648256301879883\n",
      "\t Params: tensor([ 2.4050, -0.5335])\n",
      "\t Grad: tensor([-0.5042,  2.8539])\n",
      "Epoch 26, Loss 27.56434440612793\n",
      "\t Params: tensor([ 2.4101, -0.5620])\n",
      "\t Grad: tensor([-0.5033,  2.8490])\n",
      "Epoch 27, Loss 27.4807071685791\n",
      "\t Params: tensor([ 2.4151, -0.5904])\n",
      "\t Grad: tensor([-0.5024,  2.8442])\n",
      "Epoch 28, Loss 27.397361755371094\n",
      "\t Params: tensor([ 2.4201, -0.6188])\n",
      "\t Grad: tensor([-0.5016,  2.8394])\n",
      "Epoch 29, Loss 27.314294815063477\n",
      "\t Params: tensor([ 2.4251, -0.6471])\n",
      "\t Grad: tensor([-0.5007,  2.8346])\n",
      "Epoch 30, Loss 27.23151206970215\n",
      "\t Params: tensor([ 2.4301, -0.6754])\n",
      "\t Grad: tensor([-0.4999,  2.8297])\n",
      "Epoch 31, Loss 27.149009704589844\n",
      "\t Params: tensor([ 2.4351, -0.7037])\n",
      "\t Grad: tensor([-0.4990,  2.8249])\n",
      "Epoch 32, Loss 27.066789627075195\n",
      "\t Params: tensor([ 2.4401, -0.7319])\n",
      "\t Grad: tensor([-0.4982,  2.8201])\n",
      "Epoch 33, Loss 26.984844207763672\n",
      "\t Params: tensor([ 2.4450, -0.7600])\n",
      "\t Grad: tensor([-0.4973,  2.8153])\n",
      "Epoch 34, Loss 26.903175354003906\n",
      "\t Params: tensor([ 2.4500, -0.7881])\n",
      "\t Grad: tensor([-0.4965,  2.8106])\n",
      "Epoch 35, Loss 26.82179069519043\n",
      "\t Params: tensor([ 2.4550, -0.8162])\n",
      "\t Grad: tensor([-0.4957,  2.8058])\n",
      "Epoch 36, Loss 26.740678787231445\n",
      "\t Params: tensor([ 2.4599, -0.8442])\n",
      "\t Grad: tensor([-0.4948,  2.8010])\n",
      "Epoch 37, Loss 26.65983772277832\n",
      "\t Params: tensor([ 2.4649, -0.8722])\n",
      "\t Grad: tensor([-0.4940,  2.7963])\n",
      "Epoch 38, Loss 26.57927894592285\n",
      "\t Params: tensor([ 2.4698, -0.9001])\n",
      "\t Grad: tensor([-0.4931,  2.7915])\n",
      "Epoch 39, Loss 26.498987197875977\n",
      "\t Params: tensor([ 2.4747, -0.9280])\n",
      "\t Grad: tensor([-0.4923,  2.7868])\n",
      "Epoch 40, Loss 26.418973922729492\n",
      "\t Params: tensor([ 2.4796, -0.9558])\n",
      "\t Grad: tensor([-0.4915,  2.7820])\n",
      "Epoch 41, Loss 26.3392276763916\n",
      "\t Params: tensor([ 2.4845, -0.9836])\n",
      "\t Grad: tensor([-0.4906,  2.7773])\n",
      "Epoch 42, Loss 26.259754180908203\n",
      "\t Params: tensor([ 2.4894, -1.0113])\n",
      "\t Grad: tensor([-0.4898,  2.7726])\n",
      "Epoch 43, Loss 26.1805477142334\n",
      "\t Params: tensor([ 2.4943, -1.0390])\n",
      "\t Grad: tensor([-0.4890,  2.7679])\n",
      "Epoch 44, Loss 26.10161590576172\n",
      "\t Params: tensor([ 2.4992, -1.0666])\n",
      "\t Grad: tensor([-0.4881,  2.7632])\n",
      "Epoch 45, Loss 26.022947311401367\n",
      "\t Params: tensor([ 2.5041, -1.0942])\n",
      "\t Grad: tensor([-0.4873,  2.7585])\n",
      "Epoch 46, Loss 25.944543838500977\n",
      "\t Params: tensor([ 2.5089, -1.1217])\n",
      "\t Grad: tensor([-0.4865,  2.7538])\n",
      "Epoch 47, Loss 25.866416931152344\n",
      "\t Params: tensor([ 2.5138, -1.1492])\n",
      "\t Grad: tensor([-0.4856,  2.7491])\n",
      "Epoch 48, Loss 25.788549423217773\n",
      "\t Params: tensor([ 2.5186, -1.1766])\n",
      "\t Grad: tensor([-0.4848,  2.7444])\n",
      "Epoch 49, Loss 25.7109375\n",
      "\t Params: tensor([ 2.5235, -1.2040])\n",
      "\t Grad: tensor([-0.4840,  2.7398])\n",
      "Epoch 50, Loss 25.63360023498535\n",
      "\t Params: tensor([ 2.5283, -1.2314])\n",
      "\t Grad: tensor([-0.4832,  2.7351])\n",
      "Epoch 51, Loss 25.5565242767334\n",
      "\t Params: tensor([ 2.5331, -1.2587])\n",
      "\t Grad: tensor([-0.4823,  2.7305])\n",
      "Epoch 52, Loss 25.479700088500977\n",
      "\t Params: tensor([ 2.5379, -1.2860])\n",
      "\t Grad: tensor([-0.4815,  2.7258])\n",
      "Epoch 53, Loss 25.403148651123047\n",
      "\t Params: tensor([ 2.5428, -1.3132])\n",
      "\t Grad: tensor([-0.4807,  2.7212])\n",
      "Epoch 54, Loss 25.32685089111328\n",
      "\t Params: tensor([ 2.5476, -1.3403])\n",
      "\t Grad: tensor([-0.4799,  2.7166])\n",
      "Epoch 55, Loss 25.250810623168945\n",
      "\t Params: tensor([ 2.5523, -1.3675])\n",
      "\t Grad: tensor([-0.4791,  2.7120])\n",
      "Epoch 56, Loss 25.17503547668457\n",
      "\t Params: tensor([ 2.5571, -1.3945])\n",
      "\t Grad: tensor([-0.4783,  2.7074])\n",
      "Epoch 57, Loss 25.099512100219727\n",
      "\t Params: tensor([ 2.5619, -1.4216])\n",
      "\t Grad: tensor([-0.4775,  2.7028])\n",
      "Epoch 58, Loss 25.024248123168945\n",
      "\t Params: tensor([ 2.5667, -1.4485])\n",
      "\t Grad: tensor([-0.4766,  2.6982])\n",
      "Epoch 59, Loss 24.949235916137695\n",
      "\t Params: tensor([ 2.5714, -1.4755])\n",
      "\t Grad: tensor([-0.4758,  2.6936])\n",
      "Epoch 60, Loss 24.874483108520508\n",
      "\t Params: tensor([ 2.5762, -1.5024])\n",
      "\t Grad: tensor([-0.4750,  2.6890])\n",
      "Epoch 61, Loss 24.799976348876953\n",
      "\t Params: tensor([ 2.5809, -1.5292])\n",
      "\t Grad: tensor([-0.4742,  2.6845])\n",
      "Epoch 62, Loss 24.725736618041992\n",
      "\t Params: tensor([ 2.5857, -1.5560])\n",
      "\t Grad: tensor([-0.4734,  2.6799])\n",
      "Epoch 63, Loss 24.6517391204834\n",
      "\t Params: tensor([ 2.5904, -1.5828])\n",
      "\t Grad: tensor([-0.4726,  2.6753])\n",
      "Epoch 64, Loss 24.577985763549805\n",
      "\t Params: tensor([ 2.5951, -1.6095])\n",
      "\t Grad: tensor([-0.4718,  2.6708])\n",
      "Epoch 65, Loss 24.504493713378906\n",
      "\t Params: tensor([ 2.5998, -1.6361])\n",
      "\t Grad: tensor([-0.4710,  2.6663])\n",
      "Epoch 66, Loss 24.431251525878906\n",
      "\t Params: tensor([ 2.6045, -1.6628])\n",
      "\t Grad: tensor([-0.4702,  2.6617])\n",
      "Epoch 67, Loss 24.358257293701172\n",
      "\t Params: tensor([ 2.6092, -1.6893])\n",
      "\t Grad: tensor([-0.4694,  2.6572])\n",
      "Epoch 68, Loss 24.285505294799805\n",
      "\t Params: tensor([ 2.6139, -1.7159])\n",
      "\t Grad: tensor([-0.4686,  2.6527])\n",
      "Epoch 69, Loss 24.21299934387207\n",
      "\t Params: tensor([ 2.6186, -1.7423])\n",
      "\t Grad: tensor([-0.4678,  2.6482])\n",
      "Epoch 70, Loss 24.1407413482666\n",
      "\t Params: tensor([ 2.6232, -1.7688])\n",
      "\t Grad: tensor([-0.4670,  2.6437])\n",
      "Epoch 71, Loss 24.06873321533203\n",
      "\t Params: tensor([ 2.6279, -1.7952])\n",
      "\t Grad: tensor([-0.4662,  2.6392])\n",
      "Epoch 72, Loss 23.996971130371094\n",
      "\t Params: tensor([ 2.6326, -1.8215])\n",
      "\t Grad: tensor([-0.4654,  2.6347])\n",
      "Epoch 73, Loss 23.925445556640625\n",
      "\t Params: tensor([ 2.6372, -1.8478])\n",
      "\t Grad: tensor([-0.4646,  2.6302])\n",
      "Epoch 74, Loss 23.854167938232422\n",
      "\t Params: tensor([ 2.6418, -1.8741])\n",
      "\t Grad: tensor([-0.4638,  2.6258])\n",
      "Epoch 75, Loss 23.783124923706055\n",
      "\t Params: tensor([ 2.6465, -1.9003])\n",
      "\t Grad: tensor([-0.4631,  2.6213])\n",
      "Epoch 76, Loss 23.71232795715332\n",
      "\t Params: tensor([ 2.6511, -1.9265])\n",
      "\t Grad: tensor([-0.4623,  2.6169])\n",
      "Epoch 77, Loss 23.641773223876953\n",
      "\t Params: tensor([ 2.6557, -1.9526])\n",
      "\t Grad: tensor([-0.4615,  2.6124])\n",
      "Epoch 78, Loss 23.571455001831055\n",
      "\t Params: tensor([ 2.6603, -1.9787])\n",
      "\t Grad: tensor([-0.4607,  2.6080])\n",
      "Epoch 79, Loss 23.501379013061523\n",
      "\t Params: tensor([ 2.6649, -2.0047])\n",
      "\t Grad: tensor([-0.4599,  2.6035])\n",
      "Epoch 80, Loss 23.431537628173828\n",
      "\t Params: tensor([ 2.6695, -2.0307])\n",
      "\t Grad: tensor([-0.4591,  2.5991])\n",
      "Epoch 81, Loss 23.361936569213867\n",
      "\t Params: tensor([ 2.6741, -2.0566])\n",
      "\t Grad: tensor([-0.4584,  2.5947])\n",
      "Epoch 82, Loss 23.292570114135742\n",
      "\t Params: tensor([ 2.6787, -2.0825])\n",
      "\t Grad: tensor([-0.4576,  2.5903])\n",
      "Epoch 83, Loss 23.22343635559082\n",
      "\t Params: tensor([ 2.6832, -2.1084])\n",
      "\t Grad: tensor([-0.4568,  2.5859])\n",
      "Epoch 84, Loss 23.154541015625\n",
      "\t Params: tensor([ 2.6878, -2.1342])\n",
      "\t Grad: tensor([-0.4560,  2.5815])\n",
      "Epoch 85, Loss 23.08588218688965\n",
      "\t Params: tensor([ 2.6923, -2.1600])\n",
      "\t Grad: tensor([-0.4553,  2.5771])\n",
      "Epoch 86, Loss 23.017446517944336\n",
      "\t Params: tensor([ 2.6969, -2.1857])\n",
      "\t Grad: tensor([-0.4545,  2.5727])\n",
      "Epoch 87, Loss 22.949251174926758\n",
      "\t Params: tensor([ 2.7014, -2.2114])\n",
      "\t Grad: tensor([-0.4537,  2.5684])\n",
      "Epoch 88, Loss 22.881282806396484\n",
      "\t Params: tensor([ 2.7060, -2.2370])\n",
      "\t Grad: tensor([-0.4529,  2.5640])\n",
      "Epoch 89, Loss 22.813549041748047\n",
      "\t Params: tensor([ 2.7105, -2.2626])\n",
      "\t Grad: tensor([-0.4522,  2.5597])\n",
      "Epoch 90, Loss 22.746044158935547\n",
      "\t Params: tensor([ 2.7150, -2.2882])\n",
      "\t Grad: tensor([-0.4514,  2.5553])\n",
      "Epoch 91, Loss 22.67876625061035\n",
      "\t Params: tensor([ 2.7195, -2.3137])\n",
      "\t Grad: tensor([-0.4506,  2.5510])\n",
      "Epoch 92, Loss 22.611717224121094\n",
      "\t Params: tensor([ 2.7240, -2.3392])\n",
      "\t Grad: tensor([-0.4499,  2.5466])\n",
      "Epoch 93, Loss 22.544898986816406\n",
      "\t Params: tensor([ 2.7285, -2.3646])\n",
      "\t Grad: tensor([-0.4491,  2.5423])\n",
      "Epoch 94, Loss 22.47830581665039\n",
      "\t Params: tensor([ 2.7330, -2.3900])\n",
      "\t Grad: tensor([-0.4483,  2.5380])\n",
      "Epoch 95, Loss 22.41193389892578\n",
      "\t Params: tensor([ 2.7374, -2.4153])\n",
      "\t Grad: tensor([-0.4476,  2.5337])\n",
      "Epoch 96, Loss 22.345792770385742\n",
      "\t Params: tensor([ 2.7419, -2.4406])\n",
      "\t Grad: tensor([-0.4468,  2.5294])\n",
      "Epoch 97, Loss 22.279874801635742\n",
      "\t Params: tensor([ 2.7464, -2.4658])\n",
      "\t Grad: tensor([-0.4461,  2.5251])\n",
      "Epoch 98, Loss 22.21418571472168\n",
      "\t Params: tensor([ 2.7508, -2.4910])\n",
      "\t Grad: tensor([-0.4453,  2.5208])\n",
      "Epoch 99, Loss 22.148710250854492\n",
      "\t Params: tensor([ 2.7553, -2.5162])\n",
      "\t Grad: tensor([-0.4446,  2.5165])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 2.7553, -2.5162])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(100, 1e-2, torch.tensor([1.0, 0.0]), t_un, t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 80.36434173583984\n",
      "\t Params: tensor([1.7761, 0.1064])\n",
      "\t Grad: tensor([-77.6140, -10.6400])\n",
      "Epoch 1, Loss 37.574913024902344\n",
      "\t Params: tensor([2.0848, 0.1303])\n",
      "\t Grad: tensor([-30.8623,  -2.3864])\n",
      "Epoch 2, Loss 30.871076583862305\n",
      "\t Params: tensor([2.2094, 0.1217])\n",
      "\t Grad: tensor([-12.4631,   0.8587])\n",
      "Epoch 3, Loss 29.756193161010742\n",
      "\t Params: tensor([2.2616, 0.1004])\n",
      "\t Grad: tensor([-5.2218,  2.1327])\n",
      "Epoch 4, Loss 29.507152557373047\n",
      "\t Params: tensor([2.2853, 0.0740])\n",
      "\t Grad: tensor([-2.3715,  2.6310])\n",
      "Epoch 5, Loss 29.3924560546875\n",
      "\t Params: tensor([2.2978, 0.0458])\n",
      "\t Grad: tensor([-1.2492,  2.8241])\n",
      "Epoch 6, Loss 29.298828125\n",
      "\t Params: tensor([2.3059, 0.0168])\n",
      "\t Grad: tensor([-0.8071,  2.8970])\n",
      "Epoch 7, Loss 29.208717346191406\n",
      "\t Params: tensor([ 2.3122, -0.0124])\n",
      "\t Grad: tensor([-0.6325,  2.9227])\n",
      "Epoch 8, Loss 29.119415283203125\n",
      "\t Params: tensor([ 2.3178, -0.0417])\n",
      "\t Grad: tensor([-0.5633,  2.9298])\n",
      "Epoch 9, Loss 29.030488967895508\n",
      "\t Params: tensor([ 2.3232, -0.0710])\n",
      "\t Grad: tensor([-0.5355,  2.9295])\n",
      "Epoch 10, Loss 28.941877365112305\n",
      "\t Params: tensor([ 2.3284, -0.1003])\n",
      "\t Grad: tensor([-0.5240,  2.9264])\n",
      "Epoch 11, Loss 28.853565216064453\n",
      "\t Params: tensor([ 2.3336, -0.1295])\n",
      "\t Grad: tensor([-0.5190,  2.9222])\n",
      "Epoch 12, Loss 28.765552520751953\n",
      "\t Params: tensor([ 2.3388, -0.1587])\n",
      "\t Grad: tensor([-0.5165,  2.9175])\n",
      "Epoch 13, Loss 28.6778507232666\n",
      "\t Params: tensor([ 2.3439, -0.1878])\n",
      "\t Grad: tensor([-0.5150,  2.9126])\n",
      "Epoch 14, Loss 28.590431213378906\n",
      "\t Params: tensor([ 2.3491, -0.2169])\n",
      "\t Grad: tensor([-0.5138,  2.9077])\n",
      "Epoch 15, Loss 28.503318786621094\n",
      "\t Params: tensor([ 2.3542, -0.2459])\n",
      "\t Grad: tensor([-0.5129,  2.9028])\n",
      "Epoch 16, Loss 28.4164981842041\n",
      "\t Params: tensor([ 2.3593, -0.2749])\n",
      "\t Grad: tensor([-0.5120,  2.8979])\n",
      "Epoch 17, Loss 28.329973220825195\n",
      "\t Params: tensor([ 2.3644, -0.3038])\n",
      "\t Grad: tensor([-0.5111,  2.8930])\n",
      "Epoch 18, Loss 28.243741989135742\n",
      "\t Params: tensor([ 2.3695, -0.3327])\n",
      "\t Grad: tensor([-0.5102,  2.8881])\n",
      "Epoch 19, Loss 28.157804489135742\n",
      "\t Params: tensor([ 2.3746, -0.3615])\n",
      "\t Grad: tensor([-0.5093,  2.8832])\n",
      "Epoch 20, Loss 28.07215118408203\n",
      "\t Params: tensor([ 2.3797, -0.3903])\n",
      "\t Grad: tensor([-0.5084,  2.8783])\n",
      "Epoch 21, Loss 27.986797332763672\n",
      "\t Params: tensor([ 2.3848, -0.4190])\n",
      "\t Grad: tensor([-0.5076,  2.8734])\n",
      "Epoch 22, Loss 27.9017276763916\n",
      "\t Params: tensor([ 2.3899, -0.4477])\n",
      "\t Grad: tensor([-0.5067,  2.8685])\n",
      "Epoch 23, Loss 27.81694984436035\n",
      "\t Params: tensor([ 2.3949, -0.4763])\n",
      "\t Grad: tensor([-0.5059,  2.8636])\n",
      "Epoch 24, Loss 27.732463836669922\n",
      "\t Params: tensor([ 2.4000, -0.5049])\n",
      "\t Grad: tensor([-0.5050,  2.8588])\n",
      "Epoch 25, Loss 27.648256301879883\n",
      "\t Params: tensor([ 2.4050, -0.5335])\n",
      "\t Grad: tensor([-0.5042,  2.8539])\n",
      "Epoch 26, Loss 27.56434440612793\n",
      "\t Params: tensor([ 2.4101, -0.5620])\n",
      "\t Grad: tensor([-0.5033,  2.8490])\n",
      "Epoch 27, Loss 27.4807071685791\n",
      "\t Params: tensor([ 2.4151, -0.5904])\n",
      "\t Grad: tensor([-0.5024,  2.8442])\n",
      "Epoch 28, Loss 27.397361755371094\n",
      "\t Params: tensor([ 2.4201, -0.6188])\n",
      "\t Grad: tensor([-0.5016,  2.8394])\n",
      "Epoch 29, Loss 27.314294815063477\n",
      "\t Params: tensor([ 2.4251, -0.6471])\n",
      "\t Grad: tensor([-0.5007,  2.8346])\n",
      "Epoch 30, Loss 27.23151206970215\n",
      "\t Params: tensor([ 2.4301, -0.6754])\n",
      "\t Grad: tensor([-0.4999,  2.8297])\n",
      "Epoch 31, Loss 27.149009704589844\n",
      "\t Params: tensor([ 2.4351, -0.7037])\n",
      "\t Grad: tensor([-0.4990,  2.8249])\n",
      "Epoch 32, Loss 27.066789627075195\n",
      "\t Params: tensor([ 2.4401, -0.7319])\n",
      "\t Grad: tensor([-0.4982,  2.8201])\n",
      "Epoch 33, Loss 26.984844207763672\n",
      "\t Params: tensor([ 2.4450, -0.7600])\n",
      "\t Grad: tensor([-0.4973,  2.8153])\n",
      "Epoch 34, Loss 26.903175354003906\n",
      "\t Params: tensor([ 2.4500, -0.7881])\n",
      "\t Grad: tensor([-0.4965,  2.8106])\n",
      "Epoch 35, Loss 26.82179069519043\n",
      "\t Params: tensor([ 2.4550, -0.8162])\n",
      "\t Grad: tensor([-0.4957,  2.8058])\n",
      "Epoch 36, Loss 26.740678787231445\n",
      "\t Params: tensor([ 2.4599, -0.8442])\n",
      "\t Grad: tensor([-0.4948,  2.8010])\n",
      "Epoch 37, Loss 26.65983772277832\n",
      "\t Params: tensor([ 2.4649, -0.8722])\n",
      "\t Grad: tensor([-0.4940,  2.7963])\n",
      "Epoch 38, Loss 26.57927894592285\n",
      "\t Params: tensor([ 2.4698, -0.9001])\n",
      "\t Grad: tensor([-0.4931,  2.7915])\n",
      "Epoch 39, Loss 26.498987197875977\n",
      "\t Params: tensor([ 2.4747, -0.9280])\n",
      "\t Grad: tensor([-0.4923,  2.7868])\n",
      "Epoch 40, Loss 26.418973922729492\n",
      "\t Params: tensor([ 2.4796, -0.9558])\n",
      "\t Grad: tensor([-0.4915,  2.7820])\n",
      "Epoch 41, Loss 26.3392276763916\n",
      "\t Params: tensor([ 2.4845, -0.9836])\n",
      "\t Grad: tensor([-0.4906,  2.7773])\n",
      "Epoch 42, Loss 26.259754180908203\n",
      "\t Params: tensor([ 2.4894, -1.0113])\n",
      "\t Grad: tensor([-0.4898,  2.7726])\n",
      "Epoch 43, Loss 26.1805477142334\n",
      "\t Params: tensor([ 2.4943, -1.0390])\n",
      "\t Grad: tensor([-0.4890,  2.7679])\n",
      "Epoch 44, Loss 26.10161590576172\n",
      "\t Params: tensor([ 2.4992, -1.0666])\n",
      "\t Grad: tensor([-0.4881,  2.7632])\n",
      "Epoch 45, Loss 26.022947311401367\n",
      "\t Params: tensor([ 2.5041, -1.0942])\n",
      "\t Grad: tensor([-0.4873,  2.7585])\n",
      "Epoch 46, Loss 25.944543838500977\n",
      "\t Params: tensor([ 2.5089, -1.1217])\n",
      "\t Grad: tensor([-0.4865,  2.7538])\n",
      "Epoch 47, Loss 25.866416931152344\n",
      "\t Params: tensor([ 2.5138, -1.1492])\n",
      "\t Grad: tensor([-0.4856,  2.7491])\n",
      "Epoch 48, Loss 25.788549423217773\n",
      "\t Params: tensor([ 2.5186, -1.1766])\n",
      "\t Grad: tensor([-0.4848,  2.7444])\n",
      "Epoch 49, Loss 25.7109375\n",
      "\t Params: tensor([ 2.5235, -1.2040])\n",
      "\t Grad: tensor([-0.4840,  2.7398])\n",
      "Epoch 50, Loss 25.63360023498535\n",
      "\t Params: tensor([ 2.5283, -1.2314])\n",
      "\t Grad: tensor([-0.4832,  2.7351])\n",
      "Epoch 51, Loss 25.5565242767334\n",
      "\t Params: tensor([ 2.5331, -1.2587])\n",
      "\t Grad: tensor([-0.4823,  2.7305])\n",
      "Epoch 52, Loss 25.479700088500977\n",
      "\t Params: tensor([ 2.5379, -1.2860])\n",
      "\t Grad: tensor([-0.4815,  2.7258])\n",
      "Epoch 53, Loss 25.403148651123047\n",
      "\t Params: tensor([ 2.5428, -1.3132])\n",
      "\t Grad: tensor([-0.4807,  2.7212])\n",
      "Epoch 54, Loss 25.32685089111328\n",
      "\t Params: tensor([ 2.5476, -1.3403])\n",
      "\t Grad: tensor([-0.4799,  2.7166])\n",
      "Epoch 55, Loss 25.250810623168945\n",
      "\t Params: tensor([ 2.5523, -1.3675])\n",
      "\t Grad: tensor([-0.4791,  2.7120])\n",
      "Epoch 56, Loss 25.17503547668457\n",
      "\t Params: tensor([ 2.5571, -1.3945])\n",
      "\t Grad: tensor([-0.4783,  2.7074])\n",
      "Epoch 57, Loss 25.099512100219727\n",
      "\t Params: tensor([ 2.5619, -1.4216])\n",
      "\t Grad: tensor([-0.4775,  2.7028])\n",
      "Epoch 58, Loss 25.024248123168945\n",
      "\t Params: tensor([ 2.5667, -1.4485])\n",
      "\t Grad: tensor([-0.4766,  2.6982])\n",
      "Epoch 59, Loss 24.949235916137695\n",
      "\t Params: tensor([ 2.5714, -1.4755])\n",
      "\t Grad: tensor([-0.4758,  2.6936])\n",
      "Epoch 60, Loss 24.874483108520508\n",
      "\t Params: tensor([ 2.5762, -1.5024])\n",
      "\t Grad: tensor([-0.4750,  2.6890])\n",
      "Epoch 61, Loss 24.799976348876953\n",
      "\t Params: tensor([ 2.5809, -1.5292])\n",
      "\t Grad: tensor([-0.4742,  2.6845])\n",
      "Epoch 62, Loss 24.725736618041992\n",
      "\t Params: tensor([ 2.5857, -1.5560])\n",
      "\t Grad: tensor([-0.4734,  2.6799])\n",
      "Epoch 63, Loss 24.6517391204834\n",
      "\t Params: tensor([ 2.5904, -1.5828])\n",
      "\t Grad: tensor([-0.4726,  2.6753])\n",
      "Epoch 64, Loss 24.577985763549805\n",
      "\t Params: tensor([ 2.5951, -1.6095])\n",
      "\t Grad: tensor([-0.4718,  2.6708])\n",
      "Epoch 65, Loss 24.504493713378906\n",
      "\t Params: tensor([ 2.5998, -1.6361])\n",
      "\t Grad: tensor([-0.4710,  2.6663])\n",
      "Epoch 66, Loss 24.431251525878906\n",
      "\t Params: tensor([ 2.6045, -1.6628])\n",
      "\t Grad: tensor([-0.4702,  2.6617])\n",
      "Epoch 67, Loss 24.358257293701172\n",
      "\t Params: tensor([ 2.6092, -1.6893])\n",
      "\t Grad: tensor([-0.4694,  2.6572])\n",
      "Epoch 68, Loss 24.285505294799805\n",
      "\t Params: tensor([ 2.6139, -1.7159])\n",
      "\t Grad: tensor([-0.4686,  2.6527])\n",
      "Epoch 69, Loss 24.21299934387207\n",
      "\t Params: tensor([ 2.6186, -1.7423])\n",
      "\t Grad: tensor([-0.4678,  2.6482])\n",
      "Epoch 70, Loss 24.1407413482666\n",
      "\t Params: tensor([ 2.6232, -1.7688])\n",
      "\t Grad: tensor([-0.4670,  2.6437])\n",
      "Epoch 71, Loss 24.06873321533203\n",
      "\t Params: tensor([ 2.6279, -1.7952])\n",
      "\t Grad: tensor([-0.4662,  2.6392])\n",
      "Epoch 72, Loss 23.996971130371094\n",
      "\t Params: tensor([ 2.6326, -1.8215])\n",
      "\t Grad: tensor([-0.4654,  2.6347])\n",
      "Epoch 73, Loss 23.925445556640625\n",
      "\t Params: tensor([ 2.6372, -1.8478])\n",
      "\t Grad: tensor([-0.4646,  2.6302])\n",
      "Epoch 74, Loss 23.854167938232422\n",
      "\t Params: tensor([ 2.6418, -1.8741])\n",
      "\t Grad: tensor([-0.4638,  2.6258])\n",
      "Epoch 75, Loss 23.783124923706055\n",
      "\t Params: tensor([ 2.6465, -1.9003])\n",
      "\t Grad: tensor([-0.4631,  2.6213])\n",
      "Epoch 76, Loss 23.71232795715332\n",
      "\t Params: tensor([ 2.6511, -1.9265])\n",
      "\t Grad: tensor([-0.4623,  2.6169])\n",
      "Epoch 77, Loss 23.641773223876953\n",
      "\t Params: tensor([ 2.6557, -1.9526])\n",
      "\t Grad: tensor([-0.4615,  2.6124])\n",
      "Epoch 78, Loss 23.571455001831055\n",
      "\t Params: tensor([ 2.6603, -1.9787])\n",
      "\t Grad: tensor([-0.4607,  2.6080])\n",
      "Epoch 79, Loss 23.501379013061523\n",
      "\t Params: tensor([ 2.6649, -2.0047])\n",
      "\t Grad: tensor([-0.4599,  2.6035])\n",
      "Epoch 80, Loss 23.431537628173828\n",
      "\t Params: tensor([ 2.6695, -2.0307])\n",
      "\t Grad: tensor([-0.4591,  2.5991])\n",
      "Epoch 81, Loss 23.361936569213867\n",
      "\t Params: tensor([ 2.6741, -2.0566])\n",
      "\t Grad: tensor([-0.4584,  2.5947])\n",
      "Epoch 82, Loss 23.292570114135742\n",
      "\t Params: tensor([ 2.6787, -2.0825])\n",
      "\t Grad: tensor([-0.4576,  2.5903])\n",
      "Epoch 83, Loss 23.22343635559082\n",
      "\t Params: tensor([ 2.6832, -2.1084])\n",
      "\t Grad: tensor([-0.4568,  2.5859])\n",
      "Epoch 84, Loss 23.154541015625\n",
      "\t Params: tensor([ 2.6878, -2.1342])\n",
      "\t Grad: tensor([-0.4560,  2.5815])\n",
      "Epoch 85, Loss 23.08588218688965\n",
      "\t Params: tensor([ 2.6923, -2.1600])\n",
      "\t Grad: tensor([-0.4553,  2.5771])\n",
      "Epoch 86, Loss 23.017446517944336\n",
      "\t Params: tensor([ 2.6969, -2.1857])\n",
      "\t Grad: tensor([-0.4545,  2.5727])\n",
      "Epoch 87, Loss 22.949251174926758\n",
      "\t Params: tensor([ 2.7014, -2.2114])\n",
      "\t Grad: tensor([-0.4537,  2.5684])\n",
      "Epoch 88, Loss 22.881282806396484\n",
      "\t Params: tensor([ 2.7060, -2.2370])\n",
      "\t Grad: tensor([-0.4529,  2.5640])\n",
      "Epoch 89, Loss 22.813549041748047\n",
      "\t Params: tensor([ 2.7105, -2.2626])\n",
      "\t Grad: tensor([-0.4522,  2.5597])\n",
      "Epoch 90, Loss 22.746044158935547\n",
      "\t Params: tensor([ 2.7150, -2.2882])\n",
      "\t Grad: tensor([-0.4514,  2.5553])\n",
      "Epoch 91, Loss 22.67876625061035\n",
      "\t Params: tensor([ 2.7195, -2.3137])\n",
      "\t Grad: tensor([-0.4506,  2.5510])\n",
      "Epoch 92, Loss 22.611717224121094\n",
      "\t Params: tensor([ 2.7240, -2.3392])\n",
      "\t Grad: tensor([-0.4499,  2.5466])\n",
      "Epoch 93, Loss 22.544898986816406\n",
      "\t Params: tensor([ 2.7285, -2.3646])\n",
      "\t Grad: tensor([-0.4491,  2.5423])\n",
      "Epoch 94, Loss 22.47830581665039\n",
      "\t Params: tensor([ 2.7330, -2.3900])\n",
      "\t Grad: tensor([-0.4483,  2.5380])\n",
      "Epoch 95, Loss 22.41193389892578\n",
      "\t Params: tensor([ 2.7374, -2.4153])\n",
      "\t Grad: tensor([-0.4476,  2.5337])\n",
      "Epoch 96, Loss 22.345792770385742\n",
      "\t Params: tensor([ 2.7419, -2.4406])\n",
      "\t Grad: tensor([-0.4468,  2.5294])\n",
      "Epoch 97, Loss 22.279874801635742\n",
      "\t Params: tensor([ 2.7464, -2.4658])\n",
      "\t Grad: tensor([-0.4461,  2.5251])\n",
      "Epoch 98, Loss 22.21418571472168\n",
      "\t Params: tensor([ 2.7508, -2.4910])\n",
      "\t Grad: tensor([-0.4453,  2.5208])\n",
      "Epoch 99, Loss 22.148710250854492\n",
      "\t Params: tensor([ 2.7553, -2.5162])\n",
      "\t Grad: tensor([-0.4446,  2.5165])\n",
      "Epoch 100, Loss 22.083463668823242\n",
      "\t Params: tensor([ 2.7597, -2.5413])\n",
      "\t Grad: tensor([-0.4438,  2.5122])\n",
      "Epoch 101, Loss 22.018436431884766\n",
      "\t Params: tensor([ 2.7641, -2.5664])\n",
      "\t Grad: tensor([-0.4430,  2.5080])\n",
      "Epoch 102, Loss 21.953632354736328\n",
      "\t Params: tensor([ 2.7686, -2.5914])\n",
      "\t Grad: tensor([-0.4423,  2.5037])\n",
      "Epoch 103, Loss 21.88904571533203\n",
      "\t Params: tensor([ 2.7730, -2.6164])\n",
      "\t Grad: tensor([-0.4415,  2.4994])\n",
      "Epoch 104, Loss 21.824676513671875\n",
      "\t Params: tensor([ 2.7774, -2.6414])\n",
      "\t Grad: tensor([-0.4408,  2.4952])\n",
      "Epoch 105, Loss 21.760528564453125\n",
      "\t Params: tensor([ 2.7818, -2.6663])\n",
      "\t Grad: tensor([-0.4400,  2.4910])\n",
      "Epoch 106, Loss 21.69659996032715\n",
      "\t Params: tensor([ 2.7862, -2.6912])\n",
      "\t Grad: tensor([-0.4393,  2.4867])\n",
      "Epoch 107, Loss 21.632883071899414\n",
      "\t Params: tensor([ 2.7906, -2.7160])\n",
      "\t Grad: tensor([-0.4385,  2.4825])\n",
      "Epoch 108, Loss 21.56938934326172\n",
      "\t Params: tensor([ 2.7949, -2.7408])\n",
      "\t Grad: tensor([-0.4378,  2.4783])\n",
      "Epoch 109, Loss 21.506101608276367\n",
      "\t Params: tensor([ 2.7993, -2.7655])\n",
      "\t Grad: tensor([-0.4370,  2.4741])\n",
      "Epoch 110, Loss 21.443037033081055\n",
      "\t Params: tensor([ 2.8037, -2.7902])\n",
      "\t Grad: tensor([-0.4363,  2.4699])\n",
      "Epoch 111, Loss 21.380186080932617\n",
      "\t Params: tensor([ 2.8080, -2.8149])\n",
      "\t Grad: tensor([-0.4356,  2.4657])\n",
      "Epoch 112, Loss 21.317548751831055\n",
      "\t Params: tensor([ 2.8124, -2.8395])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad: tensor([-0.4348,  2.4615])\n",
      "Epoch 113, Loss 21.255117416381836\n",
      "\t Params: tensor([ 2.8167, -2.8641])\n",
      "\t Grad: tensor([-0.4341,  2.4573])\n",
      "Epoch 114, Loss 21.192907333374023\n",
      "\t Params: tensor([ 2.8211, -2.8886])\n",
      "\t Grad: tensor([-0.4334,  2.4531])\n",
      "Epoch 115, Loss 21.130897521972656\n",
      "\t Params: tensor([ 2.8254, -2.9131])\n",
      "\t Grad: tensor([-0.4326,  2.4490])\n",
      "Epoch 116, Loss 21.06910514831543\n",
      "\t Params: tensor([ 2.8297, -2.9375])\n",
      "\t Grad: tensor([-0.4319,  2.4448])\n",
      "Epoch 117, Loss 21.007526397705078\n",
      "\t Params: tensor([ 2.8340, -2.9619])\n",
      "\t Grad: tensor([-0.4311,  2.4407])\n",
      "Epoch 118, Loss 20.946149826049805\n",
      "\t Params: tensor([ 2.8383, -2.9863])\n",
      "\t Grad: tensor([-0.4304,  2.4365])\n",
      "Epoch 119, Loss 20.884981155395508\n",
      "\t Params: tensor([ 2.8426, -3.0106])\n",
      "\t Grad: tensor([-0.4297,  2.4324])\n",
      "Epoch 120, Loss 20.824024200439453\n",
      "\t Params: tensor([ 2.8469, -3.0349])\n",
      "\t Grad: tensor([-0.4290,  2.4282])\n",
      "Epoch 121, Loss 20.763273239135742\n",
      "\t Params: tensor([ 2.8512, -3.0592])\n",
      "\t Grad: tensor([-0.4282,  2.4241])\n",
      "Epoch 122, Loss 20.702728271484375\n",
      "\t Params: tensor([ 2.8555, -3.0834])\n",
      "\t Grad: tensor([-0.4275,  2.4200])\n",
      "Epoch 123, Loss 20.642383575439453\n",
      "\t Params: tensor([ 2.8597, -3.1075])\n",
      "\t Grad: tensor([-0.4268,  2.4159])\n",
      "Epoch 124, Loss 20.58224868774414\n",
      "\t Params: tensor([ 2.8640, -3.1316])\n",
      "\t Grad: tensor([-0.4260,  2.4118])\n",
      "Epoch 125, Loss 20.522321701049805\n",
      "\t Params: tensor([ 2.8682, -3.1557])\n",
      "\t Grad: tensor([-0.4253,  2.4077])\n",
      "Epoch 126, Loss 20.46259307861328\n",
      "\t Params: tensor([ 2.8725, -3.1797])\n",
      "\t Grad: tensor([-0.4246,  2.4036])\n",
      "Epoch 127, Loss 20.40306854248047\n",
      "\t Params: tensor([ 2.8767, -3.2037])\n",
      "\t Grad: tensor([-0.4239,  2.3995])\n",
      "Epoch 128, Loss 20.34374237060547\n",
      "\t Params: tensor([ 2.8810, -3.2277])\n",
      "\t Grad: tensor([-0.4232,  2.3954])\n",
      "Epoch 129, Loss 20.284624099731445\n",
      "\t Params: tensor([ 2.8852, -3.2516])\n",
      "\t Grad: tensor([-0.4224,  2.3914])\n",
      "Epoch 130, Loss 20.2257022857666\n",
      "\t Params: tensor([ 2.8894, -3.2755])\n",
      "\t Grad: tensor([-0.4217,  2.3873])\n",
      "Epoch 131, Loss 20.166980743408203\n",
      "\t Params: tensor([ 2.8936, -3.2993])\n",
      "\t Grad: tensor([-0.4210,  2.3832])\n",
      "Epoch 132, Loss 20.108461380004883\n",
      "\t Params: tensor([ 2.8978, -3.3231])\n",
      "\t Grad: tensor([-0.4203,  2.3792])\n",
      "Epoch 133, Loss 20.05013656616211\n",
      "\t Params: tensor([ 2.9020, -3.3469])\n",
      "\t Grad: tensor([-0.4196,  2.3752])\n",
      "Epoch 134, Loss 19.992015838623047\n",
      "\t Params: tensor([ 2.9062, -3.3706])\n",
      "\t Grad: tensor([-0.4189,  2.3711])\n",
      "Epoch 135, Loss 19.934085845947266\n",
      "\t Params: tensor([ 2.9104, -3.3942])\n",
      "\t Grad: tensor([-0.4182,  2.3671])\n",
      "Epoch 136, Loss 19.876352310180664\n",
      "\t Params: tensor([ 2.9146, -3.4179])\n",
      "\t Grad: tensor([-0.4174,  2.3631])\n",
      "Epoch 137, Loss 19.818822860717773\n",
      "\t Params: tensor([ 2.9187, -3.4415])\n",
      "\t Grad: tensor([-0.4167,  2.3591])\n",
      "Epoch 138, Loss 19.7614803314209\n",
      "\t Params: tensor([ 2.9229, -3.4650])\n",
      "\t Grad: tensor([-0.4160,  2.3550])\n",
      "Epoch 139, Loss 19.704336166381836\n",
      "\t Params: tensor([ 2.9270, -3.4885])\n",
      "\t Grad: tensor([-0.4153,  2.3510])\n",
      "Epoch 140, Loss 19.647384643554688\n",
      "\t Params: tensor([ 2.9312, -3.5120])\n",
      "\t Grad: tensor([-0.4146,  2.3471])\n",
      "Epoch 141, Loss 19.590625762939453\n",
      "\t Params: tensor([ 2.9353, -3.5354])\n",
      "\t Grad: tensor([-0.4139,  2.3431])\n",
      "Epoch 142, Loss 19.534061431884766\n",
      "\t Params: tensor([ 2.9395, -3.5588])\n",
      "\t Grad: tensor([-0.4132,  2.3391])\n",
      "Epoch 143, Loss 19.477689743041992\n",
      "\t Params: tensor([ 2.9436, -3.5822])\n",
      "\t Grad: tensor([-0.4125,  2.3351])\n",
      "Epoch 144, Loss 19.421506881713867\n",
      "\t Params: tensor([ 2.9477, -3.6055])\n",
      "\t Grad: tensor([-0.4118,  2.3311])\n",
      "Epoch 145, Loss 19.365514755249023\n",
      "\t Params: tensor([ 2.9518, -3.6287])\n",
      "\t Grad: tensor([-0.4111,  2.3272])\n",
      "Epoch 146, Loss 19.309715270996094\n",
      "\t Params: tensor([ 2.9559, -3.6520])\n",
      "\t Grad: tensor([-0.4104,  2.3232])\n",
      "Epoch 147, Loss 19.254106521606445\n",
      "\t Params: tensor([ 2.9600, -3.6752])\n",
      "\t Grad: tensor([-0.4097,  2.3193])\n",
      "Epoch 148, Loss 19.198684692382812\n",
      "\t Params: tensor([ 2.9641, -3.6983])\n",
      "\t Grad: tensor([-0.4090,  2.3153])\n",
      "Epoch 149, Loss 19.14344596862793\n",
      "\t Params: tensor([ 2.9682, -3.7214])\n",
      "\t Grad: tensor([-0.4083,  2.3114])\n",
      "Epoch 150, Loss 19.088401794433594\n",
      "\t Params: tensor([ 2.9723, -3.7445])\n",
      "\t Grad: tensor([-0.4076,  2.3075])\n",
      "Epoch 151, Loss 19.03354263305664\n",
      "\t Params: tensor([ 2.9763, -3.7675])\n",
      "\t Grad: tensor([-0.4069,  2.3036])\n",
      "Epoch 152, Loss 18.97886848449707\n",
      "\t Params: tensor([ 2.9804, -3.7905])\n",
      "\t Grad: tensor([-0.4062,  2.2997])\n",
      "Epoch 153, Loss 18.92437744140625\n",
      "\t Params: tensor([ 2.9844, -3.8135])\n",
      "\t Grad: tensor([-0.4056,  2.2957])\n",
      "Epoch 154, Loss 18.870080947875977\n",
      "\t Params: tensor([ 2.9885, -3.8364])\n",
      "\t Grad: tensor([-0.4049,  2.2918])\n",
      "Epoch 155, Loss 18.815959930419922\n",
      "\t Params: tensor([ 2.9925, -3.8593])\n",
      "\t Grad: tensor([-0.4042,  2.2880])\n",
      "Epoch 156, Loss 18.762022018432617\n",
      "\t Params: tensor([ 2.9966, -3.8821])\n",
      "\t Grad: tensor([-0.4035,  2.2841])\n",
      "Epoch 157, Loss 18.708271026611328\n",
      "\t Params: tensor([ 3.0006, -3.9049])\n",
      "\t Grad: tensor([-0.4028,  2.2802])\n",
      "Epoch 158, Loss 18.654699325561523\n",
      "\t Params: tensor([ 3.0046, -3.9277])\n",
      "\t Grad: tensor([-0.4021,  2.2763])\n",
      "Epoch 159, Loss 18.6013126373291\n",
      "\t Params: tensor([ 3.0086, -3.9504])\n",
      "\t Grad: tensor([-0.4014,  2.2724])\n",
      "Epoch 160, Loss 18.54810905456543\n",
      "\t Params: tensor([ 3.0126, -3.9731])\n",
      "\t Grad: tensor([-0.4007,  2.2686])\n",
      "Epoch 161, Loss 18.495084762573242\n",
      "\t Params: tensor([ 3.0166, -3.9958])\n",
      "\t Grad: tensor([-0.4001,  2.2647])\n",
      "Epoch 162, Loss 18.442235946655273\n",
      "\t Params: tensor([ 3.0206, -4.0184])\n",
      "\t Grad: tensor([-0.3994,  2.2609])\n",
      "Epoch 163, Loss 18.389570236206055\n",
      "\t Params: tensor([ 3.0246, -4.0409])\n",
      "\t Grad: tensor([-0.3987,  2.2570])\n",
      "Epoch 164, Loss 18.337080001831055\n",
      "\t Params: tensor([ 3.0286, -4.0635])\n",
      "\t Grad: tensor([-0.3980,  2.2532])\n",
      "Epoch 165, Loss 18.28477668762207\n",
      "\t Params: tensor([ 3.0326, -4.0860])\n",
      "\t Grad: tensor([-0.3974,  2.2494])\n",
      "Epoch 166, Loss 18.232641220092773\n",
      "\t Params: tensor([ 3.0365, -4.1084])\n",
      "\t Grad: tensor([-0.3967,  2.2456])\n",
      "Epoch 167, Loss 18.18068504333496\n",
      "\t Params: tensor([ 3.0405, -4.1308])\n",
      "\t Grad: tensor([-0.3960,  2.2417])\n",
      "Epoch 168, Loss 18.12890625\n",
      "\t Params: tensor([ 3.0445, -4.1532])\n",
      "\t Grad: tensor([-0.3953,  2.2379])\n",
      "Epoch 169, Loss 18.077301025390625\n",
      "\t Params: tensor([ 3.0484, -4.1756])\n",
      "\t Grad: tensor([-0.3947,  2.2341])\n",
      "Epoch 170, Loss 18.025876998901367\n",
      "\t Params: tensor([ 3.0523, -4.1979])\n",
      "\t Grad: tensor([-0.3940,  2.2303])\n",
      "Epoch 171, Loss 17.97462272644043\n",
      "\t Params: tensor([ 3.0563, -4.2201])\n",
      "\t Grad: tensor([-0.3933,  2.2266])\n",
      "Epoch 172, Loss 17.923545837402344\n",
      "\t Params: tensor([ 3.0602, -4.2424])\n",
      "\t Grad: tensor([-0.3927,  2.2228])\n",
      "Epoch 173, Loss 17.872642517089844\n",
      "\t Params: tensor([ 3.0641, -4.2646])\n",
      "\t Grad: tensor([-0.3920,  2.2190])\n",
      "Epoch 174, Loss 17.821908950805664\n",
      "\t Params: tensor([ 3.0680, -4.2867])\n",
      "\t Grad: tensor([-0.3913,  2.2152])\n",
      "Epoch 175, Loss 17.771345138549805\n",
      "\t Params: tensor([ 3.0719, -4.3088])\n",
      "\t Grad: tensor([-0.3907,  2.2115])\n",
      "Epoch 176, Loss 17.72095489501953\n",
      "\t Params: tensor([ 3.0758, -4.3309])\n",
      "\t Grad: tensor([-0.3900,  2.2077])\n",
      "Epoch 177, Loss 17.670738220214844\n",
      "\t Params: tensor([ 3.0797, -4.3529])\n",
      "\t Grad: tensor([-0.3893,  2.2040])\n",
      "Epoch 178, Loss 17.620689392089844\n",
      "\t Params: tensor([ 3.0836, -4.3749])\n",
      "\t Grad: tensor([-0.3887,  2.2002])\n",
      "Epoch 179, Loss 17.57081413269043\n",
      "\t Params: tensor([ 3.0875, -4.3969])\n",
      "\t Grad: tensor([-0.3880,  2.1965])\n",
      "Epoch 180, Loss 17.521102905273438\n",
      "\t Params: tensor([ 3.0914, -4.4188])\n",
      "\t Grad: tensor([-0.3873,  2.1927])\n",
      "Epoch 181, Loss 17.47156524658203\n",
      "\t Params: tensor([ 3.0952, -4.4407])\n",
      "\t Grad: tensor([-0.3867,  2.1890])\n",
      "Epoch 182, Loss 17.422191619873047\n",
      "\t Params: tensor([ 3.0991, -4.4626])\n",
      "\t Grad: tensor([-0.3860,  2.1853])\n",
      "Epoch 183, Loss 17.37299346923828\n",
      "\t Params: tensor([ 3.1030, -4.4844])\n",
      "\t Grad: tensor([-0.3854,  2.1816])\n",
      "Epoch 184, Loss 17.32395362854004\n",
      "\t Params: tensor([ 3.1068, -4.5062])\n",
      "\t Grad: tensor([-0.3847,  2.1779])\n",
      "Epoch 185, Loss 17.275083541870117\n",
      "\t Params: tensor([ 3.1106, -4.5279])\n",
      "\t Grad: tensor([-0.3841,  2.1742])\n",
      "Epoch 186, Loss 17.22637939453125\n",
      "\t Params: tensor([ 3.1145, -4.5496])\n",
      "\t Grad: tensor([-0.3834,  2.1705])\n",
      "Epoch 187, Loss 17.177839279174805\n",
      "\t Params: tensor([ 3.1183, -4.5713])\n",
      "\t Grad: tensor([-0.3828,  2.1668])\n",
      "Epoch 188, Loss 17.12946319580078\n",
      "\t Params: tensor([ 3.1221, -4.5929])\n",
      "\t Grad: tensor([-0.3821,  2.1631])\n",
      "Epoch 189, Loss 17.081254959106445\n",
      "\t Params: tensor([ 3.1259, -4.6145])\n",
      "\t Grad: tensor([-0.3815,  2.1594])\n",
      "Epoch 190, Loss 17.0332088470459\n",
      "\t Params: tensor([ 3.1298, -4.6361])\n",
      "\t Grad: tensor([-0.3808,  2.1558])\n",
      "Epoch 191, Loss 16.985326766967773\n",
      "\t Params: tensor([ 3.1336, -4.6576])\n",
      "\t Grad: tensor([-0.3802,  2.1521])\n",
      "Epoch 192, Loss 16.937604904174805\n",
      "\t Params: tensor([ 3.1374, -4.6791])\n",
      "\t Grad: tensor([-0.3795,  2.1485])\n",
      "Epoch 193, Loss 16.890047073364258\n",
      "\t Params: tensor([ 3.1411, -4.7005])\n",
      "\t Grad: tensor([-0.3789,  2.1448])\n",
      "Epoch 194, Loss 16.842649459838867\n",
      "\t Params: tensor([ 3.1449, -4.7219])\n",
      "\t Grad: tensor([-0.3782,  2.1412])\n",
      "Epoch 195, Loss 16.795412063598633\n",
      "\t Params: tensor([ 3.1487, -4.7433])\n",
      "\t Grad: tensor([-0.3776,  2.1375])\n",
      "Epoch 196, Loss 16.74833869934082\n",
      "\t Params: tensor([ 3.1525, -4.7646])\n",
      "\t Grad: tensor([-0.3770,  2.1339])\n",
      "Epoch 197, Loss 16.7014217376709\n",
      "\t Params: tensor([ 3.1562, -4.7859])\n",
      "\t Grad: tensor([-0.3763,  2.1303])\n",
      "Epoch 198, Loss 16.654661178588867\n",
      "\t Params: tensor([ 3.1600, -4.8072])\n",
      "\t Grad: tensor([-0.3757,  2.1267])\n",
      "Epoch 199, Loss 16.60806655883789\n",
      "\t Params: tensor([ 3.1637, -4.8284])\n",
      "\t Grad: tensor([-0.3750,  2.1230])\n",
      "Epoch 200, Loss 16.561622619628906\n",
      "\t Params: tensor([ 3.1675, -4.8496])\n",
      "\t Grad: tensor([-0.3744,  2.1194])\n",
      "Epoch 201, Loss 16.515342712402344\n",
      "\t Params: tensor([ 3.1712, -4.8708])\n",
      "\t Grad: tensor([-0.3738,  2.1158])\n",
      "Epoch 202, Loss 16.469219207763672\n",
      "\t Params: tensor([ 3.1750, -4.8919])\n",
      "\t Grad: tensor([-0.3731,  2.1122])\n",
      "Epoch 203, Loss 16.423248291015625\n",
      "\t Params: tensor([ 3.1787, -4.9130])\n",
      "\t Grad: tensor([-0.3725,  2.1087])\n",
      "Epoch 204, Loss 16.37743377685547\n",
      "\t Params: tensor([ 3.1824, -4.9341])\n",
      "\t Grad: tensor([-0.3719,  2.1051])\n",
      "Epoch 205, Loss 16.331775665283203\n",
      "\t Params: tensor([ 3.1861, -4.9551])\n",
      "\t Grad: tensor([-0.3712,  2.1015])\n",
      "Epoch 206, Loss 16.28627586364746\n",
      "\t Params: tensor([ 3.1898, -4.9760])\n",
      "\t Grad: tensor([-0.3706,  2.0979])\n",
      "Epoch 207, Loss 16.240928649902344\n",
      "\t Params: tensor([ 3.1935, -4.9970])\n",
      "\t Grad: tensor([-0.3700,  2.0944])\n",
      "Epoch 208, Loss 16.19573211669922\n",
      "\t Params: tensor([ 3.1972, -5.0179])\n",
      "\t Grad: tensor([-0.3694,  2.0908])\n",
      "Epoch 209, Loss 16.150693893432617\n",
      "\t Params: tensor([ 3.2009, -5.0388])\n",
      "\t Grad: tensor([-0.3687,  2.0873])\n",
      "Epoch 210, Loss 16.105806350708008\n",
      "\t Params: tensor([ 3.2046, -5.0596])\n",
      "\t Grad: tensor([-0.3681,  2.0837])\n",
      "Epoch 211, Loss 16.061073303222656\n",
      "\t Params: tensor([ 3.2082, -5.0804])\n",
      "\t Grad: tensor([-0.3675,  2.0802])\n",
      "Epoch 212, Loss 16.01648712158203\n",
      "\t Params: tensor([ 3.2119, -5.1012])\n",
      "\t Grad: tensor([-0.3668,  2.0766])\n",
      "Epoch 213, Loss 15.972058296203613\n",
      "\t Params: tensor([ 3.2156, -5.1219])\n",
      "\t Grad: tensor([-0.3662,  2.0731])\n",
      "Epoch 214, Loss 15.927776336669922\n",
      "\t Params: tensor([ 3.2192, -5.1426])\n",
      "\t Grad: tensor([-0.3656,  2.0696])\n",
      "Epoch 215, Loss 15.883645057678223\n",
      "\t Params: tensor([ 3.2229, -5.1633])\n",
      "\t Grad: tensor([-0.3650,  2.0661])\n",
      "Epoch 216, Loss 15.8396635055542\n",
      "\t Params: tensor([ 3.2265, -5.1839])\n",
      "\t Grad: tensor([-0.3644,  2.0626])\n",
      "Epoch 217, Loss 15.795831680297852\n",
      "\t Params: tensor([ 3.2302, -5.2045])\n",
      "\t Grad: tensor([-0.3637,  2.0591])\n",
      "Epoch 218, Loss 15.752152442932129\n",
      "\t Params: tensor([ 3.2338, -5.2250])\n",
      "\t Grad: tensor([-0.3631,  2.0556])\n",
      "Epoch 219, Loss 15.708612442016602\n",
      "\t Params: tensor([ 3.2374, -5.2456])\n",
      "\t Grad: tensor([-0.3625,  2.0521])\n",
      "Epoch 220, Loss 15.665225982666016\n",
      "\t Params: tensor([ 3.2410, -5.2660])\n",
      "\t Grad: tensor([-0.3619,  2.0486])\n",
      "Epoch 221, Loss 15.621990203857422\n",
      "\t Params: tensor([ 3.2447, -5.2865])\n",
      "\t Grad: tensor([-0.3613,  2.0451])\n",
      "Epoch 222, Loss 15.578896522521973\n",
      "\t Params: tensor([ 3.2483, -5.3069])\n",
      "\t Grad: tensor([-0.3607,  2.0416])\n",
      "Epoch 223, Loss 15.53594970703125\n",
      "\t Params: tensor([ 3.2519, -5.3273])\n",
      "\t Grad: tensor([-0.3601,  2.0382])\n",
      "Epoch 224, Loss 15.493149757385254\n",
      "\t Params: tensor([ 3.2555, -5.3476])\n",
      "\t Grad: tensor([-0.3594,  2.0347])\n",
      "Epoch 225, Loss 15.450494766235352\n",
      "\t Params: tensor([ 3.2590, -5.3680])\n",
      "\t Grad: tensor([-0.3588,  2.0312])\n",
      "Epoch 226, Loss 15.407980918884277\n",
      "\t Params: tensor([ 3.2626, -5.3882])\n",
      "\t Grad: tensor([-0.3582,  2.0278])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227, Loss 15.365615844726562\n",
      "\t Params: tensor([ 3.2662, -5.4085])\n",
      "\t Grad: tensor([-0.3576,  2.0243])\n",
      "Epoch 228, Loss 15.323395729064941\n",
      "\t Params: tensor([ 3.2698, -5.4287])\n",
      "\t Grad: tensor([-0.3570,  2.0209])\n",
      "Epoch 229, Loss 15.281316757202148\n",
      "\t Params: tensor([ 3.2733, -5.4489])\n",
      "\t Grad: tensor([-0.3564,  2.0175])\n",
      "Epoch 230, Loss 15.2393798828125\n",
      "\t Params: tensor([ 3.2769, -5.4690])\n",
      "\t Grad: tensor([-0.3558,  2.0140])\n",
      "Epoch 231, Loss 15.197585105895996\n",
      "\t Params: tensor([ 3.2804, -5.4891])\n",
      "\t Grad: tensor([-0.3552,  2.0106])\n",
      "Epoch 232, Loss 15.155932426452637\n",
      "\t Params: tensor([ 3.2840, -5.5092])\n",
      "\t Grad: tensor([-0.3546,  2.0072])\n",
      "Epoch 233, Loss 15.114424705505371\n",
      "\t Params: tensor([ 3.2875, -5.5292])\n",
      "\t Grad: tensor([-0.3540,  2.0038])\n",
      "Epoch 234, Loss 15.073055267333984\n",
      "\t Params: tensor([ 3.2911, -5.5492])\n",
      "\t Grad: tensor([-0.3534,  2.0004])\n",
      "Epoch 235, Loss 15.03182315826416\n",
      "\t Params: tensor([ 3.2946, -5.5692])\n",
      "\t Grad: tensor([-0.3528,  1.9970])\n",
      "Epoch 236, Loss 14.990734100341797\n",
      "\t Params: tensor([ 3.2981, -5.5891])\n",
      "\t Grad: tensor([-0.3522,  1.9936])\n",
      "Epoch 237, Loss 14.949784278869629\n",
      "\t Params: tensor([ 3.3016, -5.6090])\n",
      "\t Grad: tensor([-0.3516,  1.9902])\n",
      "Epoch 238, Loss 14.90897274017334\n",
      "\t Params: tensor([ 3.3051, -5.6289])\n",
      "\t Grad: tensor([-0.3510,  1.9868])\n",
      "Epoch 239, Loss 14.868304252624512\n",
      "\t Params: tensor([ 3.3086, -5.6487])\n",
      "\t Grad: tensor([-0.3504,  1.9835])\n",
      "Epoch 240, Loss 14.827767372131348\n",
      "\t Params: tensor([ 3.3121, -5.6685])\n",
      "\t Grad: tensor([-0.3498,  1.9801])\n",
      "Epoch 241, Loss 14.787369728088379\n",
      "\t Params: tensor([ 3.3156, -5.6883])\n",
      "\t Grad: tensor([-0.3492,  1.9767])\n",
      "Epoch 242, Loss 14.747109413146973\n",
      "\t Params: tensor([ 3.3191, -5.7080])\n",
      "\t Grad: tensor([-0.3486,  1.9734])\n",
      "Epoch 243, Loss 14.706989288330078\n",
      "\t Params: tensor([ 3.3226, -5.7277])\n",
      "\t Grad: tensor([-0.3480,  1.9700])\n",
      "Epoch 244, Loss 14.667001724243164\n",
      "\t Params: tensor([ 3.3261, -5.7474])\n",
      "\t Grad: tensor([-0.3474,  1.9667])\n",
      "Epoch 245, Loss 14.627150535583496\n",
      "\t Params: tensor([ 3.3295, -5.7670])\n",
      "\t Grad: tensor([-0.3468,  1.9633])\n",
      "Epoch 246, Loss 14.587435722351074\n",
      "\t Params: tensor([ 3.3330, -5.7866])\n",
      "\t Grad: tensor([-0.3462,  1.9600])\n",
      "Epoch 247, Loss 14.547855377197266\n",
      "\t Params: tensor([ 3.3365, -5.8062])\n",
      "\t Grad: tensor([-0.3456,  1.9567])\n",
      "Epoch 248, Loss 14.508408546447754\n",
      "\t Params: tensor([ 3.3399, -5.8257])\n",
      "\t Grad: tensor([-0.3451,  1.9533])\n",
      "Epoch 249, Loss 14.469097137451172\n",
      "\t Params: tensor([ 3.3434, -5.8452])\n",
      "\t Grad: tensor([-0.3445,  1.9500])\n",
      "Epoch 250, Loss 14.429920196533203\n",
      "\t Params: tensor([ 3.3468, -5.8647])\n",
      "\t Grad: tensor([-0.3439,  1.9467])\n",
      "Epoch 251, Loss 14.390870094299316\n",
      "\t Params: tensor([ 3.3502, -5.8841])\n",
      "\t Grad: tensor([-0.3433,  1.9434])\n",
      "Epoch 252, Loss 14.351956367492676\n",
      "\t Params: tensor([ 3.3537, -5.9035])\n",
      "\t Grad: tensor([-0.3427,  1.9401])\n",
      "Epoch 253, Loss 14.313177108764648\n",
      "\t Params: tensor([ 3.3571, -5.9229])\n",
      "\t Grad: tensor([-0.3421,  1.9368])\n",
      "Epoch 254, Loss 14.274529457092285\n",
      "\t Params: tensor([ 3.3605, -5.9422])\n",
      "\t Grad: tensor([-0.3416,  1.9335])\n",
      "Epoch 255, Loss 14.236008644104004\n",
      "\t Params: tensor([ 3.3639, -5.9615])\n",
      "\t Grad: tensor([-0.3410,  1.9302])\n",
      "Epoch 256, Loss 14.197620391845703\n",
      "\t Params: tensor([ 3.3673, -5.9808])\n",
      "\t Grad: tensor([-0.3404,  1.9269])\n",
      "Epoch 257, Loss 14.15936279296875\n",
      "\t Params: tensor([ 3.3707, -6.0000])\n",
      "\t Grad: tensor([-0.3398,  1.9237])\n",
      "Epoch 258, Loss 14.121233940124512\n",
      "\t Params: tensor([ 3.3741, -6.0192])\n",
      "\t Grad: tensor([-0.3392,  1.9204])\n",
      "Epoch 259, Loss 14.083235740661621\n",
      "\t Params: tensor([ 3.3775, -6.0384])\n",
      "\t Grad: tensor([-0.3387,  1.9171])\n",
      "Epoch 260, Loss 14.045367240905762\n",
      "\t Params: tensor([ 3.3809, -6.0576])\n",
      "\t Grad: tensor([-0.3381,  1.9139])\n",
      "Epoch 261, Loss 14.0076265335083\n",
      "\t Params: tensor([ 3.3842, -6.0767])\n",
      "\t Grad: tensor([-0.3375,  1.9106])\n",
      "Epoch 262, Loss 13.970015525817871\n",
      "\t Params: tensor([ 3.3876, -6.0957])\n",
      "\t Grad: tensor([-0.3369,  1.9074])\n",
      "Epoch 263, Loss 13.932531356811523\n",
      "\t Params: tensor([ 3.3910, -6.1148])\n",
      "\t Grad: tensor([-0.3364,  1.9041])\n",
      "Epoch 264, Loss 13.895172119140625\n",
      "\t Params: tensor([ 3.3943, -6.1338])\n",
      "\t Grad: tensor([-0.3358,  1.9009])\n",
      "Epoch 265, Loss 13.857943534851074\n",
      "\t Params: tensor([ 3.3977, -6.1528])\n",
      "\t Grad: tensor([-0.3352,  1.8977])\n",
      "Epoch 266, Loss 13.820837020874023\n",
      "\t Params: tensor([ 3.4010, -6.1717])\n",
      "\t Grad: tensor([-0.3347,  1.8945])\n",
      "Epoch 267, Loss 13.783858299255371\n",
      "\t Params: tensor([ 3.4044, -6.1906])\n",
      "\t Grad: tensor([-0.3341,  1.8912])\n",
      "Epoch 268, Loss 13.7470064163208\n",
      "\t Params: tensor([ 3.4077, -6.2095])\n",
      "\t Grad: tensor([-0.3335,  1.8880])\n",
      "Epoch 269, Loss 13.710277557373047\n",
      "\t Params: tensor([ 3.4110, -6.2284])\n",
      "\t Grad: tensor([-0.3330,  1.8848])\n",
      "Epoch 270, Loss 13.673675537109375\n",
      "\t Params: tensor([ 3.4144, -6.2472])\n",
      "\t Grad: tensor([-0.3324,  1.8816])\n",
      "Epoch 271, Loss 13.637195587158203\n",
      "\t Params: tensor([ 3.4177, -6.2660])\n",
      "\t Grad: tensor([-0.3318,  1.8784])\n",
      "Epoch 272, Loss 13.600841522216797\n",
      "\t Params: tensor([ 3.4210, -6.2847])\n",
      "\t Grad: tensor([-0.3313,  1.8752])\n",
      "Epoch 273, Loss 13.564608573913574\n",
      "\t Params: tensor([ 3.4243, -6.3034])\n",
      "\t Grad: tensor([-0.3307,  1.8720])\n",
      "Epoch 274, Loss 13.5285005569458\n",
      "\t Params: tensor([ 3.4276, -6.3221])\n",
      "\t Grad: tensor([-0.3301,  1.8689])\n",
      "Epoch 275, Loss 13.492513656616211\n",
      "\t Params: tensor([ 3.4309, -6.3408])\n",
      "\t Grad: tensor([-0.3296,  1.8657])\n",
      "Epoch 276, Loss 13.456650733947754\n",
      "\t Params: tensor([ 3.4342, -6.3594])\n",
      "\t Grad: tensor([-0.3290,  1.8625])\n",
      "Epoch 277, Loss 13.420909881591797\n",
      "\t Params: tensor([ 3.4375, -6.3780])\n",
      "\t Grad: tensor([-0.3285,  1.8594])\n",
      "Epoch 278, Loss 13.385287284851074\n",
      "\t Params: tensor([ 3.4407, -6.3966])\n",
      "\t Grad: tensor([-0.3279,  1.8562])\n",
      "Epoch 279, Loss 13.349788665771484\n",
      "\t Params: tensor([ 3.4440, -6.4151])\n",
      "\t Grad: tensor([-0.3274,  1.8530])\n",
      "Epoch 280, Loss 13.314407348632812\n",
      "\t Params: tensor([ 3.4473, -6.4336])\n",
      "\t Grad: tensor([-0.3268,  1.8499])\n",
      "Epoch 281, Loss 13.279150009155273\n",
      "\t Params: tensor([ 3.4506, -6.4520])\n",
      "\t Grad: tensor([-0.3262,  1.8468])\n",
      "Epoch 282, Loss 13.244009017944336\n",
      "\t Params: tensor([ 3.4538, -6.4705])\n",
      "\t Grad: tensor([-0.3257,  1.8436])\n",
      "Epoch 283, Loss 13.208991050720215\n",
      "\t Params: tensor([ 3.4571, -6.4889])\n",
      "\t Grad: tensor([-0.3251,  1.8405])\n",
      "Epoch 284, Loss 13.174088478088379\n",
      "\t Params: tensor([ 3.4603, -6.5073])\n",
      "\t Grad: tensor([-0.3246,  1.8374])\n",
      "Epoch 285, Loss 13.139307022094727\n",
      "\t Params: tensor([ 3.4635, -6.5256])\n",
      "\t Grad: tensor([-0.3240,  1.8342])\n",
      "Epoch 286, Loss 13.104639053344727\n",
      "\t Params: tensor([ 3.4668, -6.5439])\n",
      "\t Grad: tensor([-0.3235,  1.8311])\n",
      "Epoch 287, Loss 13.07009220123291\n",
      "\t Params: tensor([ 3.4700, -6.5622])\n",
      "\t Grad: tensor([-0.3229,  1.8280])\n",
      "Epoch 288, Loss 13.035663604736328\n",
      "\t Params: tensor([ 3.4732, -6.5804])\n",
      "\t Grad: tensor([-0.3224,  1.8249])\n",
      "Epoch 289, Loss 13.001349449157715\n",
      "\t Params: tensor([ 3.4765, -6.5987])\n",
      "\t Grad: tensor([-0.3218,  1.8218])\n",
      "Epoch 290, Loss 12.967151641845703\n",
      "\t Params: tensor([ 3.4797, -6.6169])\n",
      "\t Grad: tensor([-0.3213,  1.8187])\n",
      "Epoch 291, Loss 12.933074951171875\n",
      "\t Params: tensor([ 3.4829, -6.6350])\n",
      "\t Grad: tensor([-0.3207,  1.8156])\n",
      "Epoch 292, Loss 12.89910888671875\n",
      "\t Params: tensor([ 3.4861, -6.6531])\n",
      "\t Grad: tensor([-0.3202,  1.8125])\n",
      "Epoch 293, Loss 12.865259170532227\n",
      "\t Params: tensor([ 3.4893, -6.6712])\n",
      "\t Grad: tensor([-0.3196,  1.8095])\n",
      "Epoch 294, Loss 12.831524848937988\n",
      "\t Params: tensor([ 3.4925, -6.6893])\n",
      "\t Grad: tensor([-0.3191,  1.8064])\n",
      "Epoch 295, Loss 12.797904014587402\n",
      "\t Params: tensor([ 3.4956, -6.7073])\n",
      "\t Grad: tensor([-0.3186,  1.8033])\n",
      "Epoch 296, Loss 12.764398574829102\n",
      "\t Params: tensor([ 3.4988, -6.7253])\n",
      "\t Grad: tensor([-0.3180,  1.8003])\n",
      "Epoch 297, Loss 12.731006622314453\n",
      "\t Params: tensor([ 3.5020, -6.7433])\n",
      "\t Grad: tensor([-0.3175,  1.7972])\n",
      "Epoch 298, Loss 12.69772720336914\n",
      "\t Params: tensor([ 3.5052, -6.7612])\n",
      "\t Grad: tensor([-0.3169,  1.7941])\n",
      "Epoch 299, Loss 12.664559364318848\n",
      "\t Params: tensor([ 3.5083, -6.7792])\n",
      "\t Grad: tensor([-0.3164,  1.7911])\n",
      "Epoch 300, Loss 12.63150691986084\n",
      "\t Params: tensor([ 3.5115, -6.7970])\n",
      "\t Grad: tensor([-0.3159,  1.7881])\n",
      "Epoch 301, Loss 12.598567962646484\n",
      "\t Params: tensor([ 3.5146, -6.8149])\n",
      "\t Grad: tensor([-0.3153,  1.7850])\n",
      "Epoch 302, Loss 12.5657377243042\n",
      "\t Params: tensor([ 3.5178, -6.8327])\n",
      "\t Grad: tensor([-0.3148,  1.7820])\n",
      "Epoch 303, Loss 12.533020973205566\n",
      "\t Params: tensor([ 3.5209, -6.8505])\n",
      "\t Grad: tensor([-0.3143,  1.7790])\n",
      "Epoch 304, Loss 12.500412940979004\n",
      "\t Params: tensor([ 3.5241, -6.8683])\n",
      "\t Grad: tensor([-0.3137,  1.7759])\n",
      "Epoch 305, Loss 12.46791934967041\n",
      "\t Params: tensor([ 3.5272, -6.8860])\n",
      "\t Grad: tensor([-0.3132,  1.7729])\n",
      "Epoch 306, Loss 12.435531616210938\n",
      "\t Params: tensor([ 3.5303, -6.9037])\n",
      "\t Grad: tensor([-0.3127,  1.7699])\n",
      "Epoch 307, Loss 12.4032564163208\n",
      "\t Params: tensor([ 3.5335, -6.9213])\n",
      "\t Grad: tensor([-0.3121,  1.7669])\n",
      "Epoch 308, Loss 12.371089935302734\n",
      "\t Params: tensor([ 3.5366, -6.9390])\n",
      "\t Grad: tensor([-0.3116,  1.7639])\n",
      "Epoch 309, Loss 12.339031219482422\n",
      "\t Params: tensor([ 3.5397, -6.9566])\n",
      "\t Grad: tensor([-0.3111,  1.7609])\n",
      "Epoch 310, Loss 12.307082176208496\n",
      "\t Params: tensor([ 3.5428, -6.9742])\n",
      "\t Grad: tensor([-0.3105,  1.7579])\n",
      "Epoch 311, Loss 12.275246620178223\n",
      "\t Params: tensor([ 3.5459, -6.9917])\n",
      "\t Grad: tensor([-0.3100,  1.7549])\n",
      "Epoch 312, Loss 12.243509292602539\n",
      "\t Params: tensor([ 3.5490, -7.0092])\n",
      "\t Grad: tensor([-0.3095,  1.7519])\n",
      "Epoch 313, Loss 12.21188735961914\n",
      "\t Params: tensor([ 3.5521, -7.0267])\n",
      "\t Grad: tensor([-0.3090,  1.7490])\n",
      "Epoch 314, Loss 12.180370330810547\n",
      "\t Params: tensor([ 3.5552, -7.0442])\n",
      "\t Grad: tensor([-0.3084,  1.7460])\n",
      "Epoch 315, Loss 12.148962020874023\n",
      "\t Params: tensor([ 3.5582, -7.0616])\n",
      "\t Grad: tensor([-0.3079,  1.7430])\n",
      "Epoch 316, Loss 12.117656707763672\n",
      "\t Params: tensor([ 3.5613, -7.0790])\n",
      "\t Grad: tensor([-0.3074,  1.7401])\n",
      "Epoch 317, Loss 12.086462020874023\n",
      "\t Params: tensor([ 3.5644, -7.0964])\n",
      "\t Grad: tensor([-0.3069,  1.7371])\n",
      "Epoch 318, Loss 12.055373191833496\n",
      "\t Params: tensor([ 3.5674, -7.1137])\n",
      "\t Grad: tensor([-0.3063,  1.7342])\n",
      "Epoch 319, Loss 12.024384498596191\n",
      "\t Params: tensor([ 3.5705, -7.1310])\n",
      "\t Grad: tensor([-0.3058,  1.7312])\n",
      "Epoch 320, Loss 11.993508338928223\n",
      "\t Params: tensor([ 3.5736, -7.1483])\n",
      "\t Grad: tensor([-0.3053,  1.7283])\n",
      "Epoch 321, Loss 11.96273136138916\n",
      "\t Params: tensor([ 3.5766, -7.1656])\n",
      "\t Grad: tensor([-0.3048,  1.7253])\n",
      "Epoch 322, Loss 11.932056427001953\n",
      "\t Params: tensor([ 3.5796, -7.1828])\n",
      "\t Grad: tensor([-0.3043,  1.7224])\n",
      "Epoch 323, Loss 11.90149211883545\n",
      "\t Params: tensor([ 3.5827, -7.2000])\n",
      "\t Grad: tensor([-0.3037,  1.7195])\n",
      "Epoch 324, Loss 11.871028900146484\n",
      "\t Params: tensor([ 3.5857, -7.2172])\n",
      "\t Grad: tensor([-0.3032,  1.7166])\n",
      "Epoch 325, Loss 11.840670585632324\n",
      "\t Params: tensor([ 3.5887, -7.2343])\n",
      "\t Grad: tensor([-0.3027,  1.7136])\n",
      "Epoch 326, Loss 11.810413360595703\n",
      "\t Params: tensor([ 3.5918, -7.2514])\n",
      "\t Grad: tensor([-0.3022,  1.7107])\n",
      "Epoch 327, Loss 11.780257225036621\n",
      "\t Params: tensor([ 3.5948, -7.2685])\n",
      "\t Grad: tensor([-0.3017,  1.7078])\n",
      "Epoch 328, Loss 11.750207901000977\n",
      "\t Params: tensor([ 3.5978, -7.2855])\n",
      "\t Grad: tensor([-0.3012,  1.7049])\n",
      "Epoch 329, Loss 11.720257759094238\n",
      "\t Params: tensor([ 3.6008, -7.3026])\n",
      "\t Grad: tensor([-0.3007,  1.7020])\n",
      "Epoch 330, Loss 11.690411567687988\n",
      "\t Params: tensor([ 3.6038, -7.3196])\n",
      "\t Grad: tensor([-0.3002,  1.6991])\n",
      "Epoch 331, Loss 11.660663604736328\n",
      "\t Params: tensor([ 3.6068, -7.3365])\n",
      "\t Grad: tensor([-0.2996,  1.6963])\n",
      "Epoch 332, Loss 11.631014823913574\n",
      "\t Params: tensor([ 3.6098, -7.3535])\n",
      "\t Grad: tensor([-0.2991,  1.6934])\n",
      "Epoch 333, Loss 11.601472854614258\n",
      "\t Params: tensor([ 3.6128, -7.3704])\n",
      "\t Grad: tensor([-0.2986,  1.6905])\n",
      "Epoch 334, Loss 11.572030067443848\n",
      "\t Params: tensor([ 3.6158, -7.3872])\n",
      "\t Grad: tensor([-0.2981,  1.6876])\n",
      "Epoch 335, Loss 11.542685508728027\n",
      "\t Params: tensor([ 3.6187, -7.4041])\n",
      "\t Grad: tensor([-0.2976,  1.6848])\n",
      "Epoch 336, Loss 11.513440132141113\n",
      "\t Params: tensor([ 3.6217, -7.4209])\n",
      "\t Grad: tensor([-0.2971,  1.6819])\n",
      "Epoch 337, Loss 11.484292984008789\n",
      "\t Params: tensor([ 3.6247, -7.4377])\n",
      "\t Grad: tensor([-0.2966,  1.6790])\n",
      "Epoch 338, Loss 11.455245971679688\n",
      "\t Params: tensor([ 3.6276, -7.4545])\n",
      "\t Grad: tensor([-0.2961,  1.6762])\n",
      "Epoch 339, Loss 11.426300048828125\n",
      "\t Params: tensor([ 3.6306, -7.4712])\n",
      "\t Grad: tensor([-0.2956,  1.6733])\n",
      "Epoch 340, Loss 11.39744758605957\n",
      "\t Params: tensor([ 3.6335, -7.4879])\n",
      "\t Grad: tensor([-0.2951,  1.6705])\n",
      "Epoch 341, Loss 11.368696212768555\n",
      "\t Params: tensor([ 3.6365, -7.5046])\n",
      "\t Grad: tensor([-0.2946,  1.6677])\n",
      "Epoch 342, Loss 11.340043067932129\n",
      "\t Params: tensor([ 3.6394, -7.5212])\n",
      "\t Grad: tensor([-0.2941,  1.6648])\n",
      "Epoch 343, Loss 11.311487197875977\n",
      "\t Params: tensor([ 3.6424, -7.5378])\n",
      "\t Grad: tensor([-0.2936,  1.6620])\n",
      "Epoch 344, Loss 11.283027648925781\n",
      "\t Params: tensor([ 3.6453, -7.5544])\n",
      "\t Grad: tensor([-0.2931,  1.6592])\n",
      "Epoch 345, Loss 11.254661560058594\n",
      "\t Params: tensor([ 3.6482, -7.5710])\n",
      "\t Grad: tensor([-0.2926,  1.6564])\n",
      "Epoch 346, Loss 11.226395606994629\n",
      "\t Params: tensor([ 3.6511, -7.5875])\n",
      "\t Grad: tensor([-0.2921,  1.6535])\n",
      "Epoch 347, Loss 11.198220252990723\n",
      "\t Params: tensor([ 3.6541, -7.6040])\n",
      "\t Grad: tensor([-0.2916,  1.6507])\n",
      "Epoch 348, Loss 11.170149803161621\n",
      "\t Params: tensor([ 3.6570, -7.6205])\n",
      "\t Grad: tensor([-0.2911,  1.6479])\n",
      "Epoch 349, Loss 11.142169952392578\n",
      "\t Params: tensor([ 3.6599, -7.6370])\n",
      "\t Grad: tensor([-0.2906,  1.6451])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 350, Loss 11.11428165435791\n",
      "\t Params: tensor([ 3.6628, -7.6534])\n",
      "\t Grad: tensor([-0.2901,  1.6423])\n",
      "Epoch 351, Loss 11.086490631103516\n",
      "\t Params: tensor([ 3.6657, -7.6698])\n",
      "\t Grad: tensor([-0.2896,  1.6395])\n",
      "Epoch 352, Loss 11.058796882629395\n",
      "\t Params: tensor([ 3.6686, -7.6861])\n",
      "\t Grad: tensor([-0.2892,  1.6368])\n",
      "Epoch 353, Loss 11.031192779541016\n",
      "\t Params: tensor([ 3.6714, -7.7025])\n",
      "\t Grad: tensor([-0.2886,  1.6340])\n",
      "Epoch 354, Loss 11.00368595123291\n",
      "\t Params: tensor([ 3.6743, -7.7188])\n",
      "\t Grad: tensor([-0.2882,  1.6312])\n",
      "Epoch 355, Loss 10.976269721984863\n",
      "\t Params: tensor([ 3.6772, -7.7351])\n",
      "\t Grad: tensor([-0.2877,  1.6284])\n",
      "Epoch 356, Loss 10.94894790649414\n",
      "\t Params: tensor([ 3.6801, -7.7513])\n",
      "\t Grad: tensor([-0.2872,  1.6257])\n",
      "Epoch 357, Loss 10.92171859741211\n",
      "\t Params: tensor([ 3.6829, -7.7676])\n",
      "\t Grad: tensor([-0.2867,  1.6229])\n",
      "Epoch 358, Loss 10.894580841064453\n",
      "\t Params: tensor([ 3.6858, -7.7838])\n",
      "\t Grad: tensor([-0.2862,  1.6201])\n",
      "Epoch 359, Loss 10.867536544799805\n",
      "\t Params: tensor([ 3.6887, -7.7999])\n",
      "\t Grad: tensor([-0.2857,  1.6174])\n",
      "Epoch 360, Loss 10.840582847595215\n",
      "\t Params: tensor([ 3.6915, -7.8161])\n",
      "\t Grad: tensor([-0.2852,  1.6146])\n",
      "Epoch 361, Loss 10.813720703125\n",
      "\t Params: tensor([ 3.6944, -7.8322])\n",
      "\t Grad: tensor([-0.2847,  1.6119])\n",
      "Epoch 362, Loss 10.78695011138916\n",
      "\t Params: tensor([ 3.6972, -7.8483])\n",
      "\t Grad: tensor([-0.2843,  1.6092])\n",
      "Epoch 363, Loss 10.760270118713379\n",
      "\t Params: tensor([ 3.7000, -7.8644])\n",
      "\t Grad: tensor([-0.2838,  1.6064])\n",
      "Epoch 364, Loss 10.733680725097656\n",
      "\t Params: tensor([ 3.7029, -7.8804])\n",
      "\t Grad: tensor([-0.2833,  1.6037])\n",
      "Epoch 365, Loss 10.707183837890625\n",
      "\t Params: tensor([ 3.7057, -7.8964])\n",
      "\t Grad: tensor([-0.2828,  1.6010])\n",
      "Epoch 366, Loss 10.680774688720703\n",
      "\t Params: tensor([ 3.7085, -7.9124])\n",
      "\t Grad: tensor([-0.2823,  1.5983])\n",
      "Epoch 367, Loss 10.654454231262207\n",
      "\t Params: tensor([ 3.7113, -7.9284])\n",
      "\t Grad: tensor([-0.2819,  1.5955])\n",
      "Epoch 368, Loss 10.628225326538086\n",
      "\t Params: tensor([ 3.7142, -7.9443])\n",
      "\t Grad: tensor([-0.2814,  1.5928])\n",
      "Epoch 369, Loss 10.602086067199707\n",
      "\t Params: tensor([ 3.7170, -7.9602])\n",
      "\t Grad: tensor([-0.2809,  1.5901])\n",
      "Epoch 370, Loss 10.576033592224121\n",
      "\t Params: tensor([ 3.7198, -7.9761])\n",
      "\t Grad: tensor([-0.2804,  1.5874])\n",
      "Epoch 371, Loss 10.550070762634277\n",
      "\t Params: tensor([ 3.7226, -7.9919])\n",
      "\t Grad: tensor([-0.2799,  1.5847])\n",
      "Epoch 372, Loss 10.52419376373291\n",
      "\t Params: tensor([ 3.7254, -8.0077])\n",
      "\t Grad: tensor([-0.2795,  1.5820])\n",
      "Epoch 373, Loss 10.498409271240234\n",
      "\t Params: tensor([ 3.7282, -8.0235])\n",
      "\t Grad: tensor([-0.2790,  1.5794])\n",
      "Epoch 374, Loss 10.47270679473877\n",
      "\t Params: tensor([ 3.7309, -8.0393])\n",
      "\t Grad: tensor([-0.2785,  1.5767])\n",
      "Epoch 375, Loss 10.44709300994873\n",
      "\t Params: tensor([ 3.7337, -8.0550])\n",
      "\t Grad: tensor([-0.2780,  1.5740])\n",
      "Epoch 376, Loss 10.421568870544434\n",
      "\t Params: tensor([ 3.7365, -8.0707])\n",
      "\t Grad: tensor([-0.2776,  1.5713])\n",
      "Epoch 377, Loss 10.396132469177246\n",
      "\t Params: tensor([ 3.7393, -8.0864])\n",
      "\t Grad: tensor([-0.2771,  1.5686])\n",
      "Epoch 378, Loss 10.370779037475586\n",
      "\t Params: tensor([ 3.7420, -8.1021])\n",
      "\t Grad: tensor([-0.2766,  1.5660])\n",
      "Epoch 379, Loss 10.345510482788086\n",
      "\t Params: tensor([ 3.7448, -8.1177])\n",
      "\t Grad: tensor([-0.2762,  1.5633])\n",
      "Epoch 380, Loss 10.320327758789062\n",
      "\t Params: tensor([ 3.7476, -8.1333])\n",
      "\t Grad: tensor([-0.2757,  1.5607])\n",
      "Epoch 381, Loss 10.295233726501465\n",
      "\t Params: tensor([ 3.7503, -8.1489])\n",
      "\t Grad: tensor([-0.2752,  1.5580])\n",
      "Epoch 382, Loss 10.270223617553711\n",
      "\t Params: tensor([ 3.7531, -8.1645])\n",
      "\t Grad: tensor([-0.2748,  1.5554])\n",
      "Epoch 383, Loss 10.245296478271484\n",
      "\t Params: tensor([ 3.7558, -8.1800])\n",
      "\t Grad: tensor([-0.2743,  1.5527])\n",
      "Epoch 384, Loss 10.220457077026367\n",
      "\t Params: tensor([ 3.7585, -8.1955])\n",
      "\t Grad: tensor([-0.2738,  1.5501])\n",
      "Epoch 385, Loss 10.195700645446777\n",
      "\t Params: tensor([ 3.7613, -8.2110])\n",
      "\t Grad: tensor([-0.2734,  1.5475])\n",
      "Epoch 386, Loss 10.171029090881348\n",
      "\t Params: tensor([ 3.7640, -8.2264])\n",
      "\t Grad: tensor([-0.2729,  1.5448])\n",
      "Epoch 387, Loss 10.146437644958496\n",
      "\t Params: tensor([ 3.7667, -8.2418])\n",
      "\t Grad: tensor([-0.2724,  1.5422])\n",
      "Epoch 388, Loss 10.12193489074707\n",
      "\t Params: tensor([ 3.7694, -8.2572])\n",
      "\t Grad: tensor([-0.2720,  1.5396])\n",
      "Epoch 389, Loss 10.097512245178223\n",
      "\t Params: tensor([ 3.7722, -8.2726])\n",
      "\t Grad: tensor([-0.2715,  1.5370])\n",
      "Epoch 390, Loss 10.073172569274902\n",
      "\t Params: tensor([ 3.7749, -8.2879])\n",
      "\t Grad: tensor([-0.2711,  1.5344])\n",
      "Epoch 391, Loss 10.048918724060059\n",
      "\t Params: tensor([ 3.7776, -8.3033])\n",
      "\t Grad: tensor([-0.2706,  1.5317])\n",
      "Epoch 392, Loss 10.02474308013916\n",
      "\t Params: tensor([ 3.7803, -8.3185])\n",
      "\t Grad: tensor([-0.2701,  1.5291])\n",
      "Epoch 393, Loss 10.000652313232422\n",
      "\t Params: tensor([ 3.7830, -8.3338])\n",
      "\t Grad: tensor([-0.2697,  1.5265])\n",
      "Epoch 394, Loss 9.976639747619629\n",
      "\t Params: tensor([ 3.7857, -8.3491])\n",
      "\t Grad: tensor([-0.2692,  1.5240])\n",
      "Epoch 395, Loss 9.952712059020996\n",
      "\t Params: tensor([ 3.7884, -8.3643])\n",
      "\t Grad: tensor([-0.2688,  1.5214])\n",
      "Epoch 396, Loss 9.928861618041992\n",
      "\t Params: tensor([ 3.7910, -8.3795])\n",
      "\t Grad: tensor([-0.2683,  1.5188])\n",
      "Epoch 397, Loss 9.9050931930542\n",
      "\t Params: tensor([ 3.7937, -8.3946])\n",
      "\t Grad: tensor([-0.2678,  1.5162])\n",
      "Epoch 398, Loss 9.88140869140625\n",
      "\t Params: tensor([ 3.7964, -8.4098])\n",
      "\t Grad: tensor([-0.2674,  1.5136])\n",
      "Epoch 399, Loss 9.857804298400879\n",
      "\t Params: tensor([ 3.7991, -8.4249])\n",
      "\t Grad: tensor([-0.2669,  1.5111])\n",
      "Epoch 400, Loss 9.834277153015137\n",
      "\t Params: tensor([ 3.8017, -8.4399])\n",
      "\t Grad: tensor([-0.2665,  1.5085])\n",
      "Epoch 401, Loss 9.810831069946289\n",
      "\t Params: tensor([ 3.8044, -8.4550])\n",
      "\t Grad: tensor([-0.2660,  1.5059])\n",
      "Epoch 402, Loss 9.787466049194336\n",
      "\t Params: tensor([ 3.8070, -8.4700])\n",
      "\t Grad: tensor([-0.2656,  1.5034])\n",
      "Epoch 403, Loss 9.764176368713379\n",
      "\t Params: tensor([ 3.8097, -8.4851])\n",
      "\t Grad: tensor([-0.2651,  1.5008])\n",
      "Epoch 404, Loss 9.740972518920898\n",
      "\t Params: tensor([ 3.8123, -8.5000])\n",
      "\t Grad: tensor([-0.2647,  1.4983])\n",
      "Epoch 405, Loss 9.717843055725098\n",
      "\t Params: tensor([ 3.8150, -8.5150])\n",
      "\t Grad: tensor([-0.2642,  1.4957])\n",
      "Epoch 406, Loss 9.694792747497559\n",
      "\t Params: tensor([ 3.8176, -8.5299])\n",
      "\t Grad: tensor([-0.2638,  1.4932])\n",
      "Epoch 407, Loss 9.67182445526123\n",
      "\t Params: tensor([ 3.8202, -8.5448])\n",
      "\t Grad: tensor([-0.2633,  1.4906])\n",
      "Epoch 408, Loss 9.64892578125\n",
      "\t Params: tensor([ 3.8229, -8.5597])\n",
      "\t Grad: tensor([-0.2629,  1.4881])\n",
      "Epoch 409, Loss 9.626110076904297\n",
      "\t Params: tensor([ 3.8255, -8.5746])\n",
      "\t Grad: tensor([-0.2624,  1.4856])\n",
      "Epoch 410, Loss 9.603372573852539\n",
      "\t Params: tensor([ 3.8281, -8.5894])\n",
      "\t Grad: tensor([-0.2620,  1.4831])\n",
      "Epoch 411, Loss 9.580709457397461\n",
      "\t Params: tensor([ 3.8307, -8.6042])\n",
      "\t Grad: tensor([-0.2615,  1.4805])\n",
      "Epoch 412, Loss 9.558124542236328\n",
      "\t Params: tensor([ 3.8333, -8.6190])\n",
      "\t Grad: tensor([-0.2611,  1.4780])\n",
      "Epoch 413, Loss 9.535616874694824\n",
      "\t Params: tensor([ 3.8360, -8.6337])\n",
      "\t Grad: tensor([-0.2606,  1.4755])\n",
      "Epoch 414, Loss 9.51318359375\n",
      "\t Params: tensor([ 3.8386, -8.6485])\n",
      "\t Grad: tensor([-0.2602,  1.4730])\n",
      "Epoch 415, Loss 9.490828514099121\n",
      "\t Params: tensor([ 3.8412, -8.6632])\n",
      "\t Grad: tensor([-0.2598,  1.4705])\n",
      "Epoch 416, Loss 9.468550682067871\n",
      "\t Params: tensor([ 3.8437, -8.6779])\n",
      "\t Grad: tensor([-0.2593,  1.4680])\n",
      "Epoch 417, Loss 9.4463472366333\n",
      "\t Params: tensor([ 3.8463, -8.6925])\n",
      "\t Grad: tensor([-0.2589,  1.4655])\n",
      "Epoch 418, Loss 9.424216270446777\n",
      "\t Params: tensor([ 3.8489, -8.7071])\n",
      "\t Grad: tensor([-0.2584,  1.4630])\n",
      "Epoch 419, Loss 9.4021635055542\n",
      "\t Params: tensor([ 3.8515, -8.7217])\n",
      "\t Grad: tensor([-0.2580,  1.4605])\n",
      "Epoch 420, Loss 9.380184173583984\n",
      "\t Params: tensor([ 3.8541, -8.7363])\n",
      "\t Grad: tensor([-0.2576,  1.4581])\n",
      "Epoch 421, Loss 9.358282089233398\n",
      "\t Params: tensor([ 3.8566, -8.7509])\n",
      "\t Grad: tensor([-0.2571,  1.4556])\n",
      "Epoch 422, Loss 9.336447715759277\n",
      "\t Params: tensor([ 3.8592, -8.7654])\n",
      "\t Grad: tensor([-0.2567,  1.4531])\n",
      "Epoch 423, Loss 9.314695358276367\n",
      "\t Params: tensor([ 3.8618, -8.7799])\n",
      "\t Grad: tensor([-0.2563,  1.4506])\n",
      "Epoch 424, Loss 9.293011665344238\n",
      "\t Params: tensor([ 3.8643, -8.7944])\n",
      "\t Grad: tensor([-0.2558,  1.4482])\n",
      "Epoch 425, Loss 9.271403312683105\n",
      "\t Params: tensor([ 3.8669, -8.8089])\n",
      "\t Grad: tensor([-0.2554,  1.4457])\n",
      "Epoch 426, Loss 9.249871253967285\n",
      "\t Params: tensor([ 3.8694, -8.8233])\n",
      "\t Grad: tensor([-0.2550,  1.4433])\n",
      "Epoch 427, Loss 9.228409767150879\n",
      "\t Params: tensor([ 3.8720, -8.8377])\n",
      "\t Grad: tensor([-0.2545,  1.4408])\n",
      "Epoch 428, Loss 9.207021713256836\n",
      "\t Params: tensor([ 3.8745, -8.8521])\n",
      "\t Grad: tensor([-0.2541,  1.4384])\n",
      "Epoch 429, Loss 9.185704231262207\n",
      "\t Params: tensor([ 3.8771, -8.8664])\n",
      "\t Grad: tensor([-0.2537,  1.4359])\n",
      "Epoch 430, Loss 9.164462089538574\n",
      "\t Params: tensor([ 3.8796, -8.8808])\n",
      "\t Grad: tensor([-0.2532,  1.4335])\n",
      "Epoch 431, Loss 9.143288612365723\n",
      "\t Params: tensor([ 3.8821, -8.8951])\n",
      "\t Grad: tensor([-0.2528,  1.4310])\n",
      "Epoch 432, Loss 9.122188568115234\n",
      "\t Params: tensor([ 3.8846, -8.9094])\n",
      "\t Grad: tensor([-0.2524,  1.4286])\n",
      "Epoch 433, Loss 9.101160049438477\n",
      "\t Params: tensor([ 3.8872, -8.9236])\n",
      "\t Grad: tensor([-0.2519,  1.4262])\n",
      "Epoch 434, Loss 9.080204010009766\n",
      "\t Params: tensor([ 3.8897, -8.9379])\n",
      "\t Grad: tensor([-0.2515,  1.4238])\n",
      "Epoch 435, Loss 9.059317588806152\n",
      "\t Params: tensor([ 3.8922, -8.9521])\n",
      "\t Grad: tensor([-0.2511,  1.4213])\n",
      "Epoch 436, Loss 9.038501739501953\n",
      "\t Params: tensor([ 3.8947, -8.9663])\n",
      "\t Grad: tensor([-0.2507,  1.4189])\n",
      "Epoch 437, Loss 9.017757415771484\n",
      "\t Params: tensor([ 3.8972, -8.9804])\n",
      "\t Grad: tensor([-0.2502,  1.4165])\n",
      "Epoch 438, Loss 8.99708366394043\n",
      "\t Params: tensor([ 3.8997, -8.9946])\n",
      "\t Grad: tensor([-0.2498,  1.4141])\n",
      "Epoch 439, Loss 8.976478576660156\n",
      "\t Params: tensor([ 3.9022, -9.0087])\n",
      "\t Grad: tensor([-0.2494,  1.4117])\n",
      "Epoch 440, Loss 8.955944061279297\n",
      "\t Params: tensor([ 3.9047, -9.0228])\n",
      "\t Grad: tensor([-0.2489,  1.4093])\n",
      "Epoch 441, Loss 8.935480117797852\n",
      "\t Params: tensor([ 3.9072, -9.0369])\n",
      "\t Grad: tensor([-0.2485,  1.4069])\n",
      "Epoch 442, Loss 8.915088653564453\n",
      "\t Params: tensor([ 3.9096, -9.0509])\n",
      "\t Grad: tensor([-0.2481,  1.4045])\n",
      "Epoch 443, Loss 8.89476203918457\n",
      "\t Params: tensor([ 3.9121, -9.0649])\n",
      "\t Grad: tensor([-0.2477,  1.4021])\n",
      "Epoch 444, Loss 8.874507904052734\n",
      "\t Params: tensor([ 3.9146, -9.0789])\n",
      "\t Grad: tensor([-0.2473,  1.3998])\n",
      "Epoch 445, Loss 8.854317665100098\n",
      "\t Params: tensor([ 3.9171, -9.0929])\n",
      "\t Grad: tensor([-0.2468,  1.3974])\n",
      "Epoch 446, Loss 8.834197044372559\n",
      "\t Params: tensor([ 3.9195, -9.1068])\n",
      "\t Grad: tensor([-0.2464,  1.3950])\n",
      "Epoch 447, Loss 8.814148902893066\n",
      "\t Params: tensor([ 3.9220, -9.1208])\n",
      "\t Grad: tensor([-0.2460,  1.3926])\n",
      "Epoch 448, Loss 8.794161796569824\n",
      "\t Params: tensor([ 3.9244, -9.1347])\n",
      "\t Grad: tensor([-0.2456,  1.3903])\n",
      "Epoch 449, Loss 8.774252891540527\n",
      "\t Params: tensor([ 3.9269, -9.1486])\n",
      "\t Grad: tensor([-0.2452,  1.3879])\n",
      "Epoch 450, Loss 8.75440502166748\n",
      "\t Params: tensor([ 3.9293, -9.1624])\n",
      "\t Grad: tensor([-0.2448,  1.3856])\n",
      "Epoch 451, Loss 8.734622955322266\n",
      "\t Params: tensor([ 3.9318, -9.1762])\n",
      "\t Grad: tensor([-0.2443,  1.3832])\n",
      "Epoch 452, Loss 8.714910507202148\n",
      "\t Params: tensor([ 3.9342, -9.1901])\n",
      "\t Grad: tensor([-0.2439,  1.3808])\n",
      "Epoch 453, Loss 8.695265769958496\n",
      "\t Params: tensor([ 3.9367, -9.2038])\n",
      "\t Grad: tensor([-0.2435,  1.3785])\n",
      "Epoch 454, Loss 8.675687789916992\n",
      "\t Params: tensor([ 3.9391, -9.2176])\n",
      "\t Grad: tensor([-0.2431,  1.3762])\n",
      "Epoch 455, Loss 8.656172752380371\n",
      "\t Params: tensor([ 3.9415, -9.2313])\n",
      "\t Grad: tensor([-0.2427,  1.3738])\n",
      "Epoch 456, Loss 8.63672924041748\n",
      "\t Params: tensor([ 3.9439, -9.2451])\n",
      "\t Grad: tensor([-0.2423,  1.3715])\n",
      "Epoch 457, Loss 8.61734676361084\n",
      "\t Params: tensor([ 3.9464, -9.2587])\n",
      "\t Grad: tensor([-0.2419,  1.3692])\n",
      "Epoch 458, Loss 8.598029136657715\n",
      "\t Params: tensor([ 3.9488, -9.2724])\n",
      "\t Grad: tensor([-0.2414,  1.3668])\n",
      "Epoch 459, Loss 8.578781127929688\n",
      "\t Params: tensor([ 3.9512, -9.2861])\n",
      "\t Grad: tensor([-0.2410,  1.3645])\n",
      "Epoch 460, Loss 8.55959701538086\n",
      "\t Params: tensor([ 3.9536, -9.2997])\n",
      "\t Grad: tensor([-0.2406,  1.3622])\n",
      "Epoch 461, Loss 8.540478706359863\n",
      "\t Params: tensor([ 3.9560, -9.3133])\n",
      "\t Grad: tensor([-0.2402,  1.3599])\n",
      "Epoch 462, Loss 8.5214262008667\n",
      "\t Params: tensor([ 3.9584, -9.3269])\n",
      "\t Grad: tensor([-0.2398,  1.3576])\n",
      "Epoch 463, Loss 8.502436637878418\n",
      "\t Params: tensor([ 3.9608, -9.3404])\n",
      "\t Grad: tensor([-0.2394,  1.3553])\n",
      "Epoch 464, Loss 8.483516693115234\n",
      "\t Params: tensor([ 3.9632, -9.3539])\n",
      "\t Grad: tensor([-0.2390,  1.3530])\n",
      "Epoch 465, Loss 8.464652061462402\n",
      "\t Params: tensor([ 3.9656, -9.3674])\n",
      "\t Grad: tensor([-0.2386,  1.3507])\n",
      "Epoch 466, Loss 8.445858001708984\n",
      "\t Params: tensor([ 3.9679, -9.3809])\n",
      "\t Grad: tensor([-0.2382,  1.3484])\n",
      "Epoch 467, Loss 8.427127838134766\n",
      "\t Params: tensor([ 3.9703, -9.3944])\n",
      "\t Grad: tensor([-0.2378,  1.3461])\n",
      "Epoch 468, Loss 8.408453941345215\n",
      "\t Params: tensor([ 3.9727, -9.4078])\n",
      "\t Grad: tensor([-0.2374,  1.3438])\n",
      "Epoch 469, Loss 8.389847755432129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params: tensor([ 3.9751, -9.4212])\n",
      "\t Grad: tensor([-0.2370,  1.3415])\n",
      "Epoch 470, Loss 8.371305465698242\n",
      "\t Params: tensor([ 3.9774, -9.4346])\n",
      "\t Grad: tensor([-0.2366,  1.3392])\n",
      "Epoch 471, Loss 8.352828025817871\n",
      "\t Params: tensor([ 3.9798, -9.4480])\n",
      "\t Grad: tensor([-0.2362,  1.3370])\n",
      "Epoch 472, Loss 8.3344087600708\n",
      "\t Params: tensor([ 3.9822, -9.4614])\n",
      "\t Grad: tensor([-0.2358,  1.3347])\n",
      "Epoch 473, Loss 8.316054344177246\n",
      "\t Params: tensor([ 3.9845, -9.4747])\n",
      "\t Grad: tensor([-0.2354,  1.3324])\n",
      "Epoch 474, Loss 8.29776382446289\n",
      "\t Params: tensor([ 3.9869, -9.4880])\n",
      "\t Grad: tensor([-0.2350,  1.3301])\n",
      "Epoch 475, Loss 8.279534339904785\n",
      "\t Params: tensor([ 3.9892, -9.5013])\n",
      "\t Grad: tensor([-0.2346,  1.3279])\n",
      "Epoch 476, Loss 8.261368751525879\n",
      "\t Params: tensor([ 3.9915, -9.5145])\n",
      "\t Grad: tensor([-0.2342,  1.3256])\n",
      "Epoch 477, Loss 8.24325942993164\n",
      "\t Params: tensor([ 3.9939, -9.5277])\n",
      "\t Grad: tensor([-0.2338,  1.3234])\n",
      "Epoch 478, Loss 8.225213050842285\n",
      "\t Params: tensor([ 3.9962, -9.5410])\n",
      "\t Grad: tensor([-0.2334,  1.3211])\n",
      "Epoch 479, Loss 8.207230567932129\n",
      "\t Params: tensor([ 3.9985, -9.5541])\n",
      "\t Grad: tensor([-0.2330,  1.3189])\n",
      "Epoch 480, Loss 8.189310073852539\n",
      "\t Params: tensor([ 4.0009, -9.5673])\n",
      "\t Grad: tensor([-0.2326,  1.3166])\n",
      "Epoch 481, Loss 8.171451568603516\n",
      "\t Params: tensor([ 4.0032, -9.5805])\n",
      "\t Grad: tensor([-0.2322,  1.3144])\n",
      "Epoch 482, Loss 8.153647422790527\n",
      "\t Params: tensor([ 4.0055, -9.5936])\n",
      "\t Grad: tensor([-0.2318,  1.3122])\n",
      "Epoch 483, Loss 8.135906219482422\n",
      "\t Params: tensor([ 4.0078, -9.6067])\n",
      "\t Grad: tensor([-0.2314,  1.3100])\n",
      "Epoch 484, Loss 8.118226051330566\n",
      "\t Params: tensor([ 4.0101, -9.6198])\n",
      "\t Grad: tensor([-0.2310,  1.3077])\n",
      "Epoch 485, Loss 8.100606918334961\n",
      "\t Params: tensor([ 4.0124, -9.6328])\n",
      "\t Grad: tensor([-0.2306,  1.3055])\n",
      "Epoch 486, Loss 8.08304500579834\n",
      "\t Params: tensor([ 4.0147, -9.6458])\n",
      "\t Grad: tensor([-0.2302,  1.3033])\n",
      "Epoch 487, Loss 8.065547943115234\n",
      "\t Params: tensor([ 4.0170, -9.6589])\n",
      "\t Grad: tensor([-0.2298,  1.3011])\n",
      "Epoch 488, Loss 8.048104286193848\n",
      "\t Params: tensor([ 4.0193, -9.6718])\n",
      "\t Grad: tensor([-0.2295,  1.2989])\n",
      "Epoch 489, Loss 8.030723571777344\n",
      "\t Params: tensor([ 4.0216, -9.6848])\n",
      "\t Grad: tensor([-0.2291,  1.2967])\n",
      "Epoch 490, Loss 8.01340103149414\n",
      "\t Params: tensor([ 4.0239, -9.6978])\n",
      "\t Grad: tensor([-0.2287,  1.2945])\n",
      "Epoch 491, Loss 7.996136665344238\n",
      "\t Params: tensor([ 4.0262, -9.7107])\n",
      "\t Grad: tensor([-0.2283,  1.2923])\n",
      "Epoch 492, Loss 7.97892951965332\n",
      "\t Params: tensor([ 4.0285, -9.7236])\n",
      "\t Grad: tensor([-0.2279,  1.2901])\n",
      "Epoch 493, Loss 7.961782932281494\n",
      "\t Params: tensor([ 4.0308, -9.7365])\n",
      "\t Grad: tensor([-0.2275,  1.2879])\n",
      "Epoch 494, Loss 7.944689750671387\n",
      "\t Params: tensor([ 4.0330, -9.7493])\n",
      "\t Grad: tensor([-0.2271,  1.2857])\n",
      "Epoch 495, Loss 7.9276628494262695\n",
      "\t Params: tensor([ 4.0353, -9.7621])\n",
      "\t Grad: tensor([-0.2267,  1.2835])\n",
      "Epoch 496, Loss 7.9106903076171875\n",
      "\t Params: tensor([ 4.0376, -9.7750])\n",
      "\t Grad: tensor([-0.2263,  1.2813])\n",
      "Epoch 497, Loss 7.893775463104248\n",
      "\t Params: tensor([ 4.0398, -9.7878])\n",
      "\t Grad: tensor([-0.2260,  1.2791])\n",
      "Epoch 498, Loss 7.876915454864502\n",
      "\t Params: tensor([ 4.0421, -9.8005])\n",
      "\t Grad: tensor([-0.2256,  1.2770])\n",
      "Epoch 499, Loss 7.860115051269531\n",
      "\t Params: tensor([ 4.0443, -9.8133])\n",
      "\t Grad: tensor([-0.2252,  1.2748])\n",
      "Epoch 500, Loss 7.843369007110596\n",
      "\t Params: tensor([ 4.0466, -9.8260])\n",
      "\t Grad: tensor([-0.2248,  1.2726])\n",
      "Epoch 501, Loss 7.8266825675964355\n",
      "\t Params: tensor([ 4.0488, -9.8387])\n",
      "\t Grad: tensor([-0.2244,  1.2705])\n",
      "Epoch 502, Loss 7.810052871704102\n",
      "\t Params: tensor([ 4.0511, -9.8514])\n",
      "\t Grad: tensor([-0.2241,  1.2683])\n",
      "Epoch 503, Loss 7.793481349945068\n",
      "\t Params: tensor([ 4.0533, -9.8640])\n",
      "\t Grad: tensor([-0.2237,  1.2662])\n",
      "Epoch 504, Loss 7.776961803436279\n",
      "\t Params: tensor([ 4.0555, -9.8767])\n",
      "\t Grad: tensor([-0.2233,  1.2640])\n",
      "Epoch 505, Loss 7.760497570037842\n",
      "\t Params: tensor([ 4.0578, -9.8893])\n",
      "\t Grad: tensor([-0.2229,  1.2619])\n",
      "Epoch 506, Loss 7.744091987609863\n",
      "\t Params: tensor([ 4.0600, -9.9019])\n",
      "\t Grad: tensor([-0.2225,  1.2597])\n",
      "Epoch 507, Loss 7.7277445793151855\n",
      "\t Params: tensor([ 4.0622, -9.9145])\n",
      "\t Grad: tensor([-0.2222,  1.2576])\n",
      "Epoch 508, Loss 7.711448669433594\n",
      "\t Params: tensor([ 4.0644, -9.9270])\n",
      "\t Grad: tensor([-0.2218,  1.2554])\n",
      "Epoch 509, Loss 7.695211410522461\n",
      "\t Params: tensor([ 4.0666, -9.9396])\n",
      "\t Grad: tensor([-0.2214,  1.2533])\n",
      "Epoch 510, Loss 7.679023742675781\n",
      "\t Params: tensor([ 4.0688, -9.9521])\n",
      "\t Grad: tensor([-0.2210,  1.2512])\n",
      "Epoch 511, Loss 7.662895679473877\n",
      "\t Params: tensor([ 4.0710, -9.9646])\n",
      "\t Grad: tensor([-0.2207,  1.2490])\n",
      "Epoch 512, Loss 7.646819591522217\n",
      "\t Params: tensor([ 4.0733, -9.9770])\n",
      "\t Grad: tensor([-0.2203,  1.2469])\n",
      "Epoch 513, Loss 7.630802631378174\n",
      "\t Params: tensor([ 4.0754, -9.9895])\n",
      "\t Grad: tensor([-0.2199,  1.2448])\n",
      "Epoch 514, Loss 7.614835739135742\n",
      "\t Params: tensor([  4.0776, -10.0019])\n",
      "\t Grad: tensor([-0.2195,  1.2427])\n",
      "Epoch 515, Loss 7.59892463684082\n",
      "\t Params: tensor([  4.0798, -10.0143])\n",
      "\t Grad: tensor([-0.2192,  1.2406])\n",
      "Epoch 516, Loss 7.58306884765625\n",
      "\t Params: tensor([  4.0820, -10.0267])\n",
      "\t Grad: tensor([-0.2188,  1.2385])\n",
      "Epoch 517, Loss 7.567265033721924\n",
      "\t Params: tensor([  4.0842, -10.0391])\n",
      "\t Grad: tensor([-0.2184,  1.2364])\n",
      "Epoch 518, Loss 7.551515102386475\n",
      "\t Params: tensor([  4.0864, -10.0514])\n",
      "\t Grad: tensor([-0.2180,  1.2343])\n",
      "Epoch 519, Loss 7.535818099975586\n",
      "\t Params: tensor([  4.0886, -10.0637])\n",
      "\t Grad: tensor([-0.2177,  1.2322])\n",
      "Epoch 520, Loss 7.520176410675049\n",
      "\t Params: tensor([  4.0907, -10.0760])\n",
      "\t Grad: tensor([-0.2173,  1.2301])\n",
      "Epoch 521, Loss 7.504587173461914\n",
      "\t Params: tensor([  4.0929, -10.0883])\n",
      "\t Grad: tensor([-0.2169,  1.2280])\n",
      "Epoch 522, Loss 7.489048480987549\n",
      "\t Params: tensor([  4.0951, -10.1006])\n",
      "\t Grad: tensor([-0.2165,  1.2259])\n",
      "Epoch 523, Loss 7.473565578460693\n",
      "\t Params: tensor([  4.0972, -10.1128])\n",
      "\t Grad: tensor([-0.2162,  1.2238])\n",
      "Epoch 524, Loss 7.458134651184082\n",
      "\t Params: tensor([  4.0994, -10.1250])\n",
      "\t Grad: tensor([-0.2158,  1.2217])\n",
      "Epoch 525, Loss 7.442750453948975\n",
      "\t Params: tensor([  4.1015, -10.1372])\n",
      "\t Grad: tensor([-0.2155,  1.2197])\n",
      "Epoch 526, Loss 7.427427291870117\n",
      "\t Params: tensor([  4.1037, -10.1494])\n",
      "\t Grad: tensor([-0.2151,  1.2176])\n",
      "Epoch 527, Loss 7.412152290344238\n",
      "\t Params: tensor([  4.1058, -10.1616])\n",
      "\t Grad: tensor([-0.2147,  1.2155])\n",
      "Epoch 528, Loss 7.396928310394287\n",
      "\t Params: tensor([  4.1080, -10.1737])\n",
      "\t Grad: tensor([-0.2144,  1.2135])\n",
      "Epoch 529, Loss 7.381756782531738\n",
      "\t Params: tensor([  4.1101, -10.1858])\n",
      "\t Grad: tensor([-0.2140,  1.2114])\n",
      "Epoch 530, Loss 7.366636753082275\n",
      "\t Params: tensor([  4.1123, -10.1979])\n",
      "\t Grad: tensor([-0.2136,  1.2093])\n",
      "Epoch 531, Loss 7.351567268371582\n",
      "\t Params: tensor([  4.1144, -10.2100])\n",
      "\t Grad: tensor([-0.2133,  1.2073])\n",
      "Epoch 532, Loss 7.336549282073975\n",
      "\t Params: tensor([  4.1165, -10.2220])\n",
      "\t Grad: tensor([-0.2129,  1.2052])\n",
      "Epoch 533, Loss 7.3215837478637695\n",
      "\t Params: tensor([  4.1187, -10.2340])\n",
      "\t Grad: tensor([-0.2125,  1.2032])\n",
      "Epoch 534, Loss 7.306671142578125\n",
      "\t Params: tensor([  4.1208, -10.2461])\n",
      "\t Grad: tensor([-0.2122,  1.2012])\n",
      "Epoch 535, Loss 7.291804313659668\n",
      "\t Params: tensor([  4.1229, -10.2581])\n",
      "\t Grad: tensor([-0.2118,  1.1991])\n",
      "Epoch 536, Loss 7.276988506317139\n",
      "\t Params: tensor([  4.1250, -10.2700])\n",
      "\t Grad: tensor([-0.2115,  1.1971])\n",
      "Epoch 537, Loss 7.262226581573486\n",
      "\t Params: tensor([  4.1271, -10.2820])\n",
      "\t Grad: tensor([-0.2111,  1.1950])\n",
      "Epoch 538, Loss 7.247512340545654\n",
      "\t Params: tensor([  4.1292, -10.2939])\n",
      "\t Grad: tensor([-0.2108,  1.1930])\n",
      "Epoch 539, Loss 7.232844829559326\n",
      "\t Params: tensor([  4.1313, -10.3058])\n",
      "\t Grad: tensor([-0.2104,  1.1910])\n",
      "Epoch 540, Loss 7.218230724334717\n",
      "\t Params: tensor([  4.1334, -10.3177])\n",
      "\t Grad: tensor([-0.2100,  1.1890])\n",
      "Epoch 541, Loss 7.203665256500244\n",
      "\t Params: tensor([  4.1355, -10.3296])\n",
      "\t Grad: tensor([-0.2097,  1.1869])\n",
      "Epoch 542, Loss 7.189150810241699\n",
      "\t Params: tensor([  4.1376, -10.3414])\n",
      "\t Grad: tensor([-0.2093,  1.1849])\n",
      "Epoch 543, Loss 7.1746826171875\n",
      "\t Params: tensor([  4.1397, -10.3533])\n",
      "\t Grad: tensor([-0.2090,  1.1829])\n",
      "Epoch 544, Loss 7.160265922546387\n",
      "\t Params: tensor([  4.1418, -10.3651])\n",
      "\t Grad: tensor([-0.2086,  1.1809])\n",
      "Epoch 545, Loss 7.145896911621094\n",
      "\t Params: tensor([  4.1439, -10.3769])\n",
      "\t Grad: tensor([-0.2083,  1.1789])\n",
      "Epoch 546, Loss 7.131580829620361\n",
      "\t Params: tensor([  4.1460, -10.3886])\n",
      "\t Grad: tensor([-0.2079,  1.1769])\n",
      "Epoch 547, Loss 7.117304801940918\n",
      "\t Params: tensor([  4.1480, -10.4004])\n",
      "\t Grad: tensor([-0.2075,  1.1749])\n",
      "Epoch 548, Loss 7.103082656860352\n",
      "\t Params: tensor([  4.1501, -10.4121])\n",
      "\t Grad: tensor([-0.2072,  1.1729])\n",
      "Epoch 549, Loss 7.088911056518555\n",
      "\t Params: tensor([  4.1522, -10.4238])\n",
      "\t Grad: tensor([-0.2068,  1.1709])\n",
      "Epoch 550, Loss 7.074785232543945\n",
      "\t Params: tensor([  4.1542, -10.4355])\n",
      "\t Grad: tensor([-0.2065,  1.1689])\n",
      "Epoch 551, Loss 7.060707092285156\n",
      "\t Params: tensor([  4.1563, -10.4472])\n",
      "\t Grad: tensor([-0.2062,  1.1669])\n",
      "Epoch 552, Loss 7.046676158905029\n",
      "\t Params: tensor([  4.1584, -10.4588])\n",
      "\t Grad: tensor([-0.2058,  1.1649])\n",
      "Epoch 553, Loss 7.032695293426514\n",
      "\t Params: tensor([  4.1604, -10.4704])\n",
      "\t Grad: tensor([-0.2054,  1.1630])\n",
      "Epoch 554, Loss 7.018754959106445\n",
      "\t Params: tensor([  4.1625, -10.4821])\n",
      "\t Grad: tensor([-0.2051,  1.1610])\n",
      "Epoch 555, Loss 7.004870414733887\n",
      "\t Params: tensor([  4.1645, -10.4936])\n",
      "\t Grad: tensor([-0.2047,  1.1590])\n",
      "Epoch 556, Loss 6.991028308868408\n",
      "\t Params: tensor([  4.1666, -10.5052])\n",
      "\t Grad: tensor([-0.2044,  1.1571])\n",
      "Epoch 557, Loss 6.977232456207275\n",
      "\t Params: tensor([  4.1686, -10.5168])\n",
      "\t Grad: tensor([-0.2041,  1.1551])\n",
      "Epoch 558, Loss 6.96348762512207\n",
      "\t Params: tensor([  4.1706, -10.5283])\n",
      "\t Grad: tensor([-0.2037,  1.1531])\n",
      "Epoch 559, Loss 6.94978666305542\n",
      "\t Params: tensor([  4.1727, -10.5398])\n",
      "\t Grad: tensor([-0.2034,  1.1512])\n",
      "Epoch 560, Loss 6.9361348152160645\n",
      "\t Params: tensor([  4.1747, -10.5513])\n",
      "\t Grad: tensor([-0.2030,  1.1492])\n",
      "Epoch 561, Loss 6.922528266906738\n",
      "\t Params: tensor([  4.1767, -10.5628])\n",
      "\t Grad: tensor([-0.2027,  1.1473])\n",
      "Epoch 562, Loss 6.908966541290283\n",
      "\t Params: tensor([  4.1787, -10.5742])\n",
      "\t Grad: tensor([-0.2023,  1.1453])\n",
      "Epoch 563, Loss 6.895452499389648\n",
      "\t Params: tensor([  4.1808, -10.5857])\n",
      "\t Grad: tensor([-0.2020,  1.1434])\n",
      "Epoch 564, Loss 6.8819804191589355\n",
      "\t Params: tensor([  4.1828, -10.5971])\n",
      "\t Grad: tensor([-0.2016,  1.1414])\n",
      "Epoch 565, Loss 6.868558883666992\n",
      "\t Params: tensor([  4.1848, -10.6085])\n",
      "\t Grad: tensor([-0.2013,  1.1395])\n",
      "Epoch 566, Loss 6.855180263519287\n",
      "\t Params: tensor([  4.1868, -10.6198])\n",
      "\t Grad: tensor([-0.2010,  1.1375])\n",
      "Epoch 567, Loss 6.841848373413086\n",
      "\t Params: tensor([  4.1888, -10.6312])\n",
      "\t Grad: tensor([-0.2006,  1.1356])\n",
      "Epoch 568, Loss 6.828561305999756\n",
      "\t Params: tensor([  4.1908, -10.6425])\n",
      "\t Grad: tensor([-0.2003,  1.1337])\n",
      "Epoch 569, Loss 6.815318584442139\n",
      "\t Params: tensor([  4.1928, -10.6539])\n",
      "\t Grad: tensor([-0.1999,  1.1318])\n",
      "Epoch 570, Loss 6.802118301391602\n",
      "\t Params: tensor([  4.1948, -10.6652])\n",
      "\t Grad: tensor([-0.1996,  1.1298])\n",
      "Epoch 571, Loss 6.788968086242676\n",
      "\t Params: tensor([  4.1968, -10.6764])\n",
      "\t Grad: tensor([-0.1993,  1.1279])\n",
      "Epoch 572, Loss 6.7758636474609375\n",
      "\t Params: tensor([  4.1988, -10.6877])\n",
      "\t Grad: tensor([-0.1989,  1.1260])\n",
      "Epoch 573, Loss 6.7627973556518555\n",
      "\t Params: tensor([  4.2008, -10.6989])\n",
      "\t Grad: tensor([-0.1986,  1.1241])\n",
      "Epoch 574, Loss 6.749778747558594\n",
      "\t Params: tensor([  4.2028, -10.7102])\n",
      "\t Grad: tensor([-0.1982,  1.1222])\n",
      "Epoch 575, Loss 6.736804008483887\n",
      "\t Params: tensor([  4.2047, -10.7214])\n",
      "\t Grad: tensor([-0.1979,  1.1203])\n",
      "Epoch 576, Loss 6.723875522613525\n",
      "\t Params: tensor([  4.2067, -10.7325])\n",
      "\t Grad: tensor([-0.1976,  1.1184])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 577, Loss 6.710986614227295\n",
      "\t Params: tensor([  4.2087, -10.7437])\n",
      "\t Grad: tensor([-0.1972,  1.1165])\n",
      "Epoch 578, Loss 6.698141574859619\n",
      "\t Params: tensor([  4.2107, -10.7549])\n",
      "\t Grad: tensor([-0.1969,  1.1146])\n",
      "Epoch 579, Loss 6.68534517288208\n",
      "\t Params: tensor([  4.2126, -10.7660])\n",
      "\t Grad: tensor([-0.1966,  1.1127])\n",
      "Epoch 580, Loss 6.67258882522583\n",
      "\t Params: tensor([  4.2146, -10.7771])\n",
      "\t Grad: tensor([-0.1962,  1.1108])\n",
      "Epoch 581, Loss 6.6598734855651855\n",
      "\t Params: tensor([  4.2165, -10.7882])\n",
      "\t Grad: tensor([-0.1959,  1.1089])\n",
      "Epoch 582, Loss 6.647207260131836\n",
      "\t Params: tensor([  4.2185, -10.7992])\n",
      "\t Grad: tensor([-0.1956,  1.1070])\n",
      "Epoch 583, Loss 6.634577751159668\n",
      "\t Params: tensor([  4.2204, -10.8103])\n",
      "\t Grad: tensor([-0.1952,  1.1051])\n",
      "Epoch 584, Loss 6.621994495391846\n",
      "\t Params: tensor([  4.2224, -10.8213])\n",
      "\t Grad: tensor([-0.1949,  1.1033])\n",
      "Epoch 585, Loss 6.609454154968262\n",
      "\t Params: tensor([  4.2243, -10.8323])\n",
      "\t Grad: tensor([-0.1946,  1.1014])\n",
      "Epoch 586, Loss 6.59695291519165\n",
      "\t Params: tensor([  4.2263, -10.8433])\n",
      "\t Grad: tensor([-0.1942,  1.0995])\n",
      "Epoch 587, Loss 6.584498882293701\n",
      "\t Params: tensor([  4.2282, -10.8543])\n",
      "\t Grad: tensor([-0.1939,  1.0976])\n",
      "Epoch 588, Loss 6.572086811065674\n",
      "\t Params: tensor([  4.2302, -10.8653])\n",
      "\t Grad: tensor([-0.1936,  1.0958])\n",
      "Epoch 589, Loss 6.5597124099731445\n",
      "\t Params: tensor([  4.2321, -10.8762])\n",
      "\t Grad: tensor([-0.1932,  1.0939])\n",
      "Epoch 590, Loss 6.547384262084961\n",
      "\t Params: tensor([  4.2340, -10.8871])\n",
      "\t Grad: tensor([-0.1929,  1.0921])\n",
      "Epoch 591, Loss 6.535097122192383\n",
      "\t Params: tensor([  4.2359, -10.8980])\n",
      "\t Grad: tensor([-0.1926,  1.0902])\n",
      "Epoch 592, Loss 6.522850513458252\n",
      "\t Params: tensor([  4.2379, -10.9089])\n",
      "\t Grad: tensor([-0.1923,  1.0884])\n",
      "Epoch 593, Loss 6.510645866394043\n",
      "\t Params: tensor([  4.2398, -10.9198])\n",
      "\t Grad: tensor([-0.1919,  1.0865])\n",
      "Epoch 594, Loss 6.4984822273254395\n",
      "\t Params: tensor([  4.2417, -10.9306])\n",
      "\t Grad: tensor([-0.1916,  1.0847])\n",
      "Epoch 595, Loss 6.486360549926758\n",
      "\t Params: tensor([  4.2436, -10.9415])\n",
      "\t Grad: tensor([-0.1913,  1.0828])\n",
      "Epoch 596, Loss 6.4742817878723145\n",
      "\t Params: tensor([  4.2455, -10.9523])\n",
      "\t Grad: tensor([-0.1910,  1.0810])\n",
      "Epoch 597, Loss 6.462240695953369\n",
      "\t Params: tensor([  4.2474, -10.9631])\n",
      "\t Grad: tensor([-0.1906,  1.0791])\n",
      "Epoch 598, Loss 6.45024299621582\n",
      "\t Params: tensor([  4.2493, -10.9738])\n",
      "\t Grad: tensor([-0.1903,  1.0773])\n",
      "Epoch 599, Loss 6.438284397125244\n",
      "\t Params: tensor([  4.2512, -10.9846])\n",
      "\t Grad: tensor([-0.1900,  1.0755])\n",
      "Epoch 600, Loss 6.426368236541748\n",
      "\t Params: tensor([  4.2531, -10.9953])\n",
      "\t Grad: tensor([-0.1897,  1.0737])\n",
      "Epoch 601, Loss 6.41448974609375\n",
      "\t Params: tensor([  4.2550, -11.0060])\n",
      "\t Grad: tensor([-0.1893,  1.0718])\n",
      "Epoch 602, Loss 6.402653217315674\n",
      "\t Params: tensor([  4.2569, -11.0167])\n",
      "\t Grad: tensor([-0.1890,  1.0700])\n",
      "Epoch 603, Loss 6.3908586502075195\n",
      "\t Params: tensor([  4.2588, -11.0274])\n",
      "\t Grad: tensor([-0.1887,  1.0682])\n",
      "Epoch 604, Loss 6.37910270690918\n",
      "\t Params: tensor([  4.2607, -11.0381])\n",
      "\t Grad: tensor([-0.1884,  1.0664])\n",
      "Epoch 605, Loss 6.367385387420654\n",
      "\t Params: tensor([  4.2626, -11.0487])\n",
      "\t Grad: tensor([-0.1880,  1.0646])\n",
      "Epoch 606, Loss 6.355705738067627\n",
      "\t Params: tensor([  4.2644, -11.0594])\n",
      "\t Grad: tensor([-0.1877,  1.0628])\n",
      "Epoch 607, Loss 6.3440704345703125\n",
      "\t Params: tensor([  4.2663, -11.0700])\n",
      "\t Grad: tensor([-0.1874,  1.0609])\n",
      "Epoch 608, Loss 6.332472324371338\n",
      "\t Params: tensor([  4.2682, -11.0806])\n",
      "\t Grad: tensor([-0.1871,  1.0591])\n",
      "Epoch 609, Loss 6.320911884307861\n",
      "\t Params: tensor([  4.2701, -11.0911])\n",
      "\t Grad: tensor([-0.1868,  1.0573])\n",
      "Epoch 610, Loss 6.309394836425781\n",
      "\t Params: tensor([  4.2719, -11.1017])\n",
      "\t Grad: tensor([-0.1865,  1.0555])\n",
      "Epoch 611, Loss 6.297914505004883\n",
      "\t Params: tensor([  4.2738, -11.1122])\n",
      "\t Grad: tensor([-0.1861,  1.0538])\n",
      "Epoch 612, Loss 6.286472797393799\n",
      "\t Params: tensor([  4.2756, -11.1227])\n",
      "\t Grad: tensor([-0.1858,  1.0520])\n",
      "Epoch 613, Loss 6.275073528289795\n",
      "\t Params: tensor([  4.2775, -11.1333])\n",
      "\t Grad: tensor([-0.1855,  1.0502])\n",
      "Epoch 614, Loss 6.263708114624023\n",
      "\t Params: tensor([  4.2794, -11.1437])\n",
      "\t Grad: tensor([-0.1852,  1.0484])\n",
      "Epoch 615, Loss 6.252382278442383\n",
      "\t Params: tensor([  4.2812, -11.1542])\n",
      "\t Grad: tensor([-0.1849,  1.0466])\n",
      "Epoch 616, Loss 6.241097927093506\n",
      "\t Params: tensor([  4.2830, -11.1646])\n",
      "\t Grad: tensor([-0.1846,  1.0448])\n",
      "Epoch 617, Loss 6.229849338531494\n",
      "\t Params: tensor([  4.2849, -11.1751])\n",
      "\t Grad: tensor([-0.1843,  1.0431])\n",
      "Epoch 618, Loss 6.218638896942139\n",
      "\t Params: tensor([  4.2867, -11.1855])\n",
      "\t Grad: tensor([-0.1840,  1.0413])\n",
      "Epoch 619, Loss 6.207470417022705\n",
      "\t Params: tensor([  4.2886, -11.1959])\n",
      "\t Grad: tensor([-0.1836,  1.0395])\n",
      "Epoch 620, Loss 6.196334362030029\n",
      "\t Params: tensor([  4.2904, -11.2063])\n",
      "\t Grad: tensor([-0.1833,  1.0378])\n",
      "Epoch 621, Loss 6.185240268707275\n",
      "\t Params: tensor([  4.2922, -11.2166])\n",
      "\t Grad: tensor([-0.1830,  1.0360])\n",
      "Epoch 622, Loss 6.174180507659912\n",
      "\t Params: tensor([  4.2941, -11.2270])\n",
      "\t Grad: tensor([-0.1827,  1.0342])\n",
      "Epoch 623, Loss 6.163159370422363\n",
      "\t Params: tensor([  4.2959, -11.2373])\n",
      "\t Grad: tensor([-0.1824,  1.0325])\n",
      "Epoch 624, Loss 6.152177333831787\n",
      "\t Params: tensor([  4.2977, -11.2476])\n",
      "\t Grad: tensor([-0.1821,  1.0307])\n",
      "Epoch 625, Loss 6.141229629516602\n",
      "\t Params: tensor([  4.2995, -11.2579])\n",
      "\t Grad: tensor([-0.1818,  1.0290])\n",
      "Epoch 626, Loss 6.130321979522705\n",
      "\t Params: tensor([  4.3013, -11.2682])\n",
      "\t Grad: tensor([-0.1815,  1.0272])\n",
      "Epoch 627, Loss 6.119447708129883\n",
      "\t Params: tensor([  4.3031, -11.2784])\n",
      "\t Grad: tensor([-0.1811,  1.0255])\n",
      "Epoch 628, Loss 6.108613967895508\n",
      "\t Params: tensor([  4.3050, -11.2887])\n",
      "\t Grad: tensor([-0.1808,  1.0237])\n",
      "Epoch 629, Loss 6.097814559936523\n",
      "\t Params: tensor([  4.3068, -11.2989])\n",
      "\t Grad: tensor([-0.1805,  1.0220])\n",
      "Epoch 630, Loss 6.087054252624512\n",
      "\t Params: tensor([  4.3086, -11.3091])\n",
      "\t Grad: tensor([-0.1802,  1.0203])\n",
      "Epoch 631, Loss 6.076329231262207\n",
      "\t Params: tensor([  4.3104, -11.3193])\n",
      "\t Grad: tensor([-0.1799,  1.0185])\n",
      "Epoch 632, Loss 6.065643787384033\n",
      "\t Params: tensor([  4.3122, -11.3294])\n",
      "\t Grad: tensor([-0.1796,  1.0168])\n",
      "Epoch 633, Loss 6.054988384246826\n",
      "\t Params: tensor([  4.3139, -11.3396])\n",
      "\t Grad: tensor([-0.1793,  1.0151])\n",
      "Epoch 634, Loss 6.044372081756592\n",
      "\t Params: tensor([  4.3157, -11.3497])\n",
      "\t Grad: tensor([-0.1790,  1.0133])\n",
      "Epoch 635, Loss 6.033793926239014\n",
      "\t Params: tensor([  4.3175, -11.3598])\n",
      "\t Grad: tensor([-0.1787,  1.0116])\n",
      "Epoch 636, Loss 6.023247241973877\n",
      "\t Params: tensor([  4.3193, -11.3699])\n",
      "\t Grad: tensor([-0.1784,  1.0099])\n",
      "Epoch 637, Loss 6.01273775100708\n",
      "\t Params: tensor([  4.3211, -11.3800])\n",
      "\t Grad: tensor([-0.1781,  1.0082])\n",
      "Epoch 638, Loss 6.002264022827148\n",
      "\t Params: tensor([  4.3229, -11.3901])\n",
      "\t Grad: tensor([-0.1778,  1.0065])\n",
      "Epoch 639, Loss 5.991828441619873\n",
      "\t Params: tensor([  4.3246, -11.4001])\n",
      "\t Grad: tensor([-0.1775,  1.0048])\n",
      "Epoch 640, Loss 5.9814252853393555\n",
      "\t Params: tensor([  4.3264, -11.4102])\n",
      "\t Grad: tensor([-0.1772,  1.0031])\n",
      "Epoch 641, Loss 5.971058368682861\n",
      "\t Params: tensor([  4.3282, -11.4202])\n",
      "\t Grad: tensor([-0.1769,  1.0014])\n",
      "Epoch 642, Loss 5.960727214813232\n",
      "\t Params: tensor([  4.3300, -11.4302])\n",
      "\t Grad: tensor([-0.1766,  0.9997])\n",
      "Epoch 643, Loss 5.950432300567627\n",
      "\t Params: tensor([  4.3317, -11.4401])\n",
      "\t Grad: tensor([-0.1763,  0.9980])\n",
      "Epoch 644, Loss 5.940170764923096\n",
      "\t Params: tensor([  4.3335, -11.4501])\n",
      "\t Grad: tensor([-0.1760,  0.9963])\n",
      "Epoch 645, Loss 5.929944038391113\n",
      "\t Params: tensor([  4.3352, -11.4601])\n",
      "\t Grad: tensor([-0.1757,  0.9946])\n",
      "Epoch 646, Loss 5.91975212097168\n",
      "\t Params: tensor([  4.3370, -11.4700])\n",
      "\t Grad: tensor([-0.1754,  0.9929])\n",
      "Epoch 647, Loss 5.9095964431762695\n",
      "\t Params: tensor([  4.3387, -11.4799])\n",
      "\t Grad: tensor([-0.1751,  0.9912])\n",
      "Epoch 648, Loss 5.899472236633301\n",
      "\t Params: tensor([  4.3405, -11.4898])\n",
      "\t Grad: tensor([-0.1748,  0.9895])\n",
      "Epoch 649, Loss 5.889383316040039\n",
      "\t Params: tensor([  4.3422, -11.4997])\n",
      "\t Grad: tensor([-0.1745,  0.9878])\n",
      "Epoch 650, Loss 5.879326343536377\n",
      "\t Params: tensor([  4.3440, -11.5095])\n",
      "\t Grad: tensor([-0.1742,  0.9862])\n",
      "Epoch 651, Loss 5.86931037902832\n",
      "\t Params: tensor([  4.3457, -11.5194])\n",
      "\t Grad: tensor([-0.1739,  0.9845])\n",
      "Epoch 652, Loss 5.8593220710754395\n",
      "\t Params: tensor([  4.3474, -11.5292])\n",
      "\t Grad: tensor([-0.1736,  0.9828])\n",
      "Epoch 653, Loss 5.849374294281006\n",
      "\t Params: tensor([  4.3492, -11.5390])\n",
      "\t Grad: tensor([-0.1733,  0.9811])\n",
      "Epoch 654, Loss 5.839453220367432\n",
      "\t Params: tensor([  4.3509, -11.5488])\n",
      "\t Grad: tensor([-0.1730,  0.9795])\n",
      "Epoch 655, Loss 5.8295698165893555\n",
      "\t Params: tensor([  4.3526, -11.5586])\n",
      "\t Grad: tensor([-0.1727,  0.9778])\n",
      "Epoch 656, Loss 5.819717884063721\n",
      "\t Params: tensor([  4.3544, -11.5683])\n",
      "\t Grad: tensor([-0.1724,  0.9761])\n",
      "Epoch 657, Loss 5.809900760650635\n",
      "\t Params: tensor([  4.3561, -11.5781])\n",
      "\t Grad: tensor([-0.1722,  0.9745])\n",
      "Epoch 658, Loss 5.800116062164307\n",
      "\t Params: tensor([  4.3578, -11.5878])\n",
      "\t Grad: tensor([-0.1719,  0.9728])\n",
      "Epoch 659, Loss 5.7903666496276855\n",
      "\t Params: tensor([  4.3595, -11.5975])\n",
      "\t Grad: tensor([-0.1716,  0.9712])\n",
      "Epoch 660, Loss 5.780646324157715\n",
      "\t Params: tensor([  4.3612, -11.6072])\n",
      "\t Grad: tensor([-0.1713,  0.9695])\n",
      "Epoch 661, Loss 5.770962238311768\n",
      "\t Params: tensor([  4.3629, -11.6169])\n",
      "\t Grad: tensor([-0.1710,  0.9679])\n",
      "Epoch 662, Loss 5.7613115310668945\n",
      "\t Params: tensor([  4.3646, -11.6266])\n",
      "\t Grad: tensor([-0.1707,  0.9662])\n",
      "Epoch 663, Loss 5.7516937255859375\n",
      "\t Params: tensor([  4.3664, -11.6362])\n",
      "\t Grad: tensor([-0.1704,  0.9646])\n",
      "Epoch 664, Loss 5.742105484008789\n",
      "\t Params: tensor([  4.3681, -11.6458])\n",
      "\t Grad: tensor([-0.1701,  0.9630])\n",
      "Epoch 665, Loss 5.732549667358398\n",
      "\t Params: tensor([  4.3697, -11.6555])\n",
      "\t Grad: tensor([-0.1698,  0.9613])\n",
      "Epoch 666, Loss 5.723031044006348\n",
      "\t Params: tensor([  4.3714, -11.6651])\n",
      "\t Grad: tensor([-0.1695,  0.9597])\n",
      "Epoch 667, Loss 5.7135396003723145\n",
      "\t Params: tensor([  4.3731, -11.6746])\n",
      "\t Grad: tensor([-0.1692,  0.9581])\n",
      "Epoch 668, Loss 5.704083442687988\n",
      "\t Params: tensor([  4.3748, -11.6842])\n",
      "\t Grad: tensor([-0.1690,  0.9564])\n",
      "Epoch 669, Loss 5.6946587562561035\n",
      "\t Params: tensor([  4.3765, -11.6937])\n",
      "\t Grad: tensor([-0.1687,  0.9548])\n",
      "Epoch 670, Loss 5.68526554107666\n",
      "\t Params: tensor([  4.3782, -11.7033])\n",
      "\t Grad: tensor([-0.1684,  0.9532])\n",
      "Epoch 671, Loss 5.675903797149658\n",
      "\t Params: tensor([  4.3799, -11.7128])\n",
      "\t Grad: tensor([-0.1681,  0.9516])\n",
      "Epoch 672, Loss 5.6665730476379395\n",
      "\t Params: tensor([  4.3816, -11.7223])\n",
      "\t Grad: tensor([-0.1678,  0.9499])\n",
      "Epoch 673, Loss 5.6572771072387695\n",
      "\t Params: tensor([  4.3832, -11.7318])\n",
      "\t Grad: tensor([-0.1675,  0.9483])\n",
      "Epoch 674, Loss 5.648009777069092\n",
      "\t Params: tensor([  4.3849, -11.7412])\n",
      "\t Grad: tensor([-0.1673,  0.9467])\n",
      "Epoch 675, Loss 5.6387763023376465\n",
      "\t Params: tensor([  4.3866, -11.7507])\n",
      "\t Grad: tensor([-0.1670,  0.9451])\n",
      "Epoch 676, Loss 5.629574298858643\n",
      "\t Params: tensor([  4.3882, -11.7601])\n",
      "\t Grad: tensor([-0.1667,  0.9435])\n",
      "Epoch 677, Loss 5.6204023361206055\n",
      "\t Params: tensor([  4.3899, -11.7696])\n",
      "\t Grad: tensor([-0.1664,  0.9419])\n",
      "Epoch 678, Loss 5.611259937286377\n",
      "\t Params: tensor([  4.3916, -11.7790])\n",
      "\t Grad: tensor([-0.1661,  0.9403])\n",
      "Epoch 679, Loss 5.602148532867432\n",
      "\t Params: tensor([  4.3932, -11.7883])\n",
      "\t Grad: tensor([-0.1658,  0.9387])\n",
      "Epoch 680, Loss 5.593070983886719\n",
      "\t Params: tensor([  4.3949, -11.7977])\n",
      "\t Grad: tensor([-0.1656,  0.9371])\n",
      "Epoch 681, Loss 5.584022045135498\n",
      "\t Params: tensor([  4.3965, -11.8071])\n",
      "\t Grad: tensor([-0.1653,  0.9355])\n",
      "Epoch 682, Loss 5.575005054473877\n",
      "\t Params: tensor([  4.3982, -11.8164])\n",
      "\t Grad: tensor([-0.1650,  0.9339])\n",
      "Epoch 683, Loss 5.566019058227539\n",
      "\t Params: tensor([  4.3998, -11.8257])\n",
      "\t Grad: tensor([-0.1647,  0.9323])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 684, Loss 5.557063102722168\n",
      "\t Params: tensor([  4.4015, -11.8350])\n",
      "\t Grad: tensor([-0.1644,  0.9308])\n",
      "Epoch 685, Loss 5.548136234283447\n",
      "\t Params: tensor([  4.4031, -11.8443])\n",
      "\t Grad: tensor([-0.1641,  0.9292])\n",
      "Epoch 686, Loss 5.539241313934326\n",
      "\t Params: tensor([  4.4048, -11.8536])\n",
      "\t Grad: tensor([-0.1639,  0.9276])\n",
      "Epoch 687, Loss 5.530376434326172\n",
      "\t Params: tensor([  4.4064, -11.8629])\n",
      "\t Grad: tensor([-0.1636,  0.9260])\n",
      "Epoch 688, Loss 5.521539688110352\n",
      "\t Params: tensor([  4.4080, -11.8721])\n",
      "\t Grad: tensor([-0.1633,  0.9245])\n",
      "Epoch 689, Loss 5.5127339363098145\n",
      "\t Params: tensor([  4.4097, -11.8813])\n",
      "\t Grad: tensor([-0.1630,  0.9229])\n",
      "Epoch 690, Loss 5.503957748413086\n",
      "\t Params: tensor([  4.4113, -11.8906])\n",
      "\t Grad: tensor([-0.1628,  0.9213])\n",
      "Epoch 691, Loss 5.495211601257324\n",
      "\t Params: tensor([  4.4129, -11.8998])\n",
      "\t Grad: tensor([-0.1625,  0.9197])\n",
      "Epoch 692, Loss 5.4864959716796875\n",
      "\t Params: tensor([  4.4145, -11.9089])\n",
      "\t Grad: tensor([-0.1622,  0.9182])\n",
      "Epoch 693, Loss 5.477807998657227\n",
      "\t Params: tensor([  4.4161, -11.9181])\n",
      "\t Grad: tensor([-0.1619,  0.9166])\n",
      "Epoch 694, Loss 5.469152450561523\n",
      "\t Params: tensor([  4.4178, -11.9272])\n",
      "\t Grad: tensor([-0.1617,  0.9151])\n",
      "Epoch 695, Loss 5.460525035858154\n",
      "\t Params: tensor([  4.4194, -11.9364])\n",
      "\t Grad: tensor([-0.1614,  0.9135])\n",
      "Epoch 696, Loss 5.451927661895752\n",
      "\t Params: tensor([  4.4210, -11.9455])\n",
      "\t Grad: tensor([-0.1611,  0.9120])\n",
      "Epoch 697, Loss 5.443358421325684\n",
      "\t Params: tensor([  4.4226, -11.9546])\n",
      "\t Grad: tensor([-0.1608,  0.9104])\n",
      "Epoch 698, Loss 5.434819221496582\n",
      "\t Params: tensor([  4.4242, -11.9637])\n",
      "\t Grad: tensor([-0.1605,  0.9089])\n",
      "Epoch 699, Loss 5.426309108734131\n",
      "\t Params: tensor([  4.4258, -11.9728])\n",
      "\t Grad: tensor([-0.1603,  0.9073])\n",
      "Epoch 700, Loss 5.4178266525268555\n",
      "\t Params: tensor([  4.4274, -11.9818])\n",
      "\t Grad: tensor([-0.1600,  0.9058])\n",
      "Epoch 701, Loss 5.409372329711914\n",
      "\t Params: tensor([  4.4290, -11.9909])\n",
      "\t Grad: tensor([-0.1597,  0.9042])\n",
      "Epoch 702, Loss 5.400949478149414\n",
      "\t Params: tensor([  4.4306, -11.9999])\n",
      "\t Grad: tensor([-0.1595,  0.9027])\n",
      "Epoch 703, Loss 5.392550468444824\n",
      "\t Params: tensor([  4.4322, -12.0089])\n",
      "\t Grad: tensor([-0.1592,  0.9012])\n",
      "Epoch 704, Loss 5.38418436050415\n",
      "\t Params: tensor([  4.4338, -12.0179])\n",
      "\t Grad: tensor([-0.1589,  0.8996])\n",
      "Epoch 705, Loss 5.3758463859558105\n",
      "\t Params: tensor([  4.4354, -12.0269])\n",
      "\t Grad: tensor([-0.1586,  0.8981])\n",
      "Epoch 706, Loss 5.367536544799805\n",
      "\t Params: tensor([  4.4369, -12.0359])\n",
      "\t Grad: tensor([-0.1584,  0.8966])\n",
      "Epoch 707, Loss 5.359253406524658\n",
      "\t Params: tensor([  4.4385, -12.0448])\n",
      "\t Grad: tensor([-0.1581,  0.8951])\n",
      "Epoch 708, Loss 5.350998878479004\n",
      "\t Params: tensor([  4.4401, -12.0537])\n",
      "\t Grad: tensor([-0.1578,  0.8935])\n",
      "Epoch 709, Loss 5.342771530151367\n",
      "\t Params: tensor([  4.4417, -12.0627])\n",
      "\t Grad: tensor([-0.1576,  0.8920])\n",
      "Epoch 710, Loss 5.334575176239014\n",
      "\t Params: tensor([  4.4433, -12.0716])\n",
      "\t Grad: tensor([-0.1573,  0.8905])\n",
      "Epoch 711, Loss 5.326402187347412\n",
      "\t Params: tensor([  4.4448, -12.0805])\n",
      "\t Grad: tensor([-0.1570,  0.8890])\n",
      "Epoch 712, Loss 5.3182597160339355\n",
      "\t Params: tensor([  4.4464, -12.0893])\n",
      "\t Grad: tensor([-0.1568,  0.8875])\n",
      "Epoch 713, Loss 5.310144424438477\n",
      "\t Params: tensor([  4.4480, -12.0982])\n",
      "\t Grad: tensor([-0.1565,  0.8860])\n",
      "Epoch 714, Loss 5.3020548820495605\n",
      "\t Params: tensor([  4.4495, -12.1070])\n",
      "\t Grad: tensor([-0.1562,  0.8845])\n",
      "Epoch 715, Loss 5.293994426727295\n",
      "\t Params: tensor([  4.4511, -12.1159])\n",
      "\t Grad: tensor([-0.1560,  0.8830])\n",
      "Epoch 716, Loss 5.285963535308838\n",
      "\t Params: tensor([  4.4526, -12.1247])\n",
      "\t Grad: tensor([-0.1557,  0.8815])\n",
      "Epoch 717, Loss 5.277958393096924\n",
      "\t Params: tensor([  4.4542, -12.1335])\n",
      "\t Grad: tensor([-0.1555,  0.8800])\n",
      "Epoch 718, Loss 5.269979476928711\n",
      "\t Params: tensor([  4.4557, -12.1423])\n",
      "\t Grad: tensor([-0.1552,  0.8785])\n",
      "Epoch 719, Loss 5.262026786804199\n",
      "\t Params: tensor([  4.4573, -12.1510])\n",
      "\t Grad: tensor([-0.1549,  0.8770])\n",
      "Epoch 720, Loss 5.25410270690918\n",
      "\t Params: tensor([  4.4588, -12.1598])\n",
      "\t Grad: tensor([-0.1547,  0.8755])\n",
      "Epoch 721, Loss 5.2462053298950195\n",
      "\t Params: tensor([  4.4604, -12.1685])\n",
      "\t Grad: tensor([-0.1544,  0.8740])\n",
      "Epoch 722, Loss 5.238335132598877\n",
      "\t Params: tensor([  4.4619, -12.1773])\n",
      "\t Grad: tensor([-0.1541,  0.8725])\n",
      "Epoch 723, Loss 5.230491638183594\n",
      "\t Params: tensor([  4.4635, -12.1860])\n",
      "\t Grad: tensor([-0.1539,  0.8710])\n",
      "Epoch 724, Loss 5.2226738929748535\n",
      "\t Params: tensor([  4.4650, -12.1947])\n",
      "\t Grad: tensor([-0.1536,  0.8696])\n",
      "Epoch 725, Loss 5.21488094329834\n",
      "\t Params: tensor([  4.4665, -12.2033])\n",
      "\t Grad: tensor([-0.1533,  0.8681])\n",
      "Epoch 726, Loss 5.207119941711426\n",
      "\t Params: tensor([  4.4681, -12.2120])\n",
      "\t Grad: tensor([-0.1531,  0.8666])\n",
      "Epoch 727, Loss 5.199380874633789\n",
      "\t Params: tensor([  4.4696, -12.2207])\n",
      "\t Grad: tensor([-0.1528,  0.8651])\n",
      "Epoch 728, Loss 5.191669940948486\n",
      "\t Params: tensor([  4.4711, -12.2293])\n",
      "\t Grad: tensor([-0.1526,  0.8637])\n",
      "Epoch 729, Loss 5.183984756469727\n",
      "\t Params: tensor([  4.4726, -12.2379])\n",
      "\t Grad: tensor([-0.1523,  0.8622])\n",
      "Epoch 730, Loss 5.176324367523193\n",
      "\t Params: tensor([  4.4742, -12.2465])\n",
      "\t Grad: tensor([-0.1520,  0.8607])\n",
      "Epoch 731, Loss 5.16868782043457\n",
      "\t Params: tensor([  4.4757, -12.2551])\n",
      "\t Grad: tensor([-0.1518,  0.8593])\n",
      "Epoch 732, Loss 5.161084175109863\n",
      "\t Params: tensor([  4.4772, -12.2637])\n",
      "\t Grad: tensor([-0.1515,  0.8578])\n",
      "Epoch 733, Loss 5.153499603271484\n",
      "\t Params: tensor([  4.4787, -12.2723])\n",
      "\t Grad: tensor([-0.1513,  0.8564])\n",
      "Epoch 734, Loss 5.145944118499756\n",
      "\t Params: tensor([  4.4802, -12.2808])\n",
      "\t Grad: tensor([-0.1510,  0.8549])\n",
      "Epoch 735, Loss 5.138412952423096\n",
      "\t Params: tensor([  4.4817, -12.2893])\n",
      "\t Grad: tensor([-0.1508,  0.8535])\n",
      "Epoch 736, Loss 5.1309099197387695\n",
      "\t Params: tensor([  4.4832, -12.2979])\n",
      "\t Grad: tensor([-0.1505,  0.8520])\n",
      "Epoch 737, Loss 5.123427867889404\n",
      "\t Params: tensor([  4.4847, -12.3064])\n",
      "\t Grad: tensor([-0.1502,  0.8506])\n",
      "Epoch 738, Loss 5.115977764129639\n",
      "\t Params: tensor([  4.4862, -12.3149])\n",
      "\t Grad: tensor([-0.1500,  0.8491])\n",
      "Epoch 739, Loss 5.108546733856201\n",
      "\t Params: tensor([  4.4877, -12.3233])\n",
      "\t Grad: tensor([-0.1497,  0.8477])\n",
      "Epoch 740, Loss 5.1011433601379395\n",
      "\t Params: tensor([  4.4892, -12.3318])\n",
      "\t Grad: tensor([-0.1495,  0.8462])\n",
      "Epoch 741, Loss 5.0937652587890625\n",
      "\t Params: tensor([  4.4907, -12.3402])\n",
      "\t Grad: tensor([-0.1492,  0.8448])\n",
      "Epoch 742, Loss 5.086413860321045\n",
      "\t Params: tensor([  4.4922, -12.3487])\n",
      "\t Grad: tensor([-0.1490,  0.8434])\n",
      "Epoch 743, Loss 5.079085826873779\n",
      "\t Params: tensor([  4.4937, -12.3571])\n",
      "\t Grad: tensor([-0.1487,  0.8419])\n",
      "Epoch 744, Loss 5.071781158447266\n",
      "\t Params: tensor([  4.4952, -12.3655])\n",
      "\t Grad: tensor([-0.1485,  0.8405])\n",
      "Epoch 745, Loss 5.064504623413086\n",
      "\t Params: tensor([  4.4967, -12.3739])\n",
      "\t Grad: tensor([-0.1482,  0.8391])\n",
      "Epoch 746, Loss 5.057246685028076\n",
      "\t Params: tensor([  4.4981, -12.3823])\n",
      "\t Grad: tensor([-0.1480,  0.8376])\n",
      "Epoch 747, Loss 5.050021171569824\n",
      "\t Params: tensor([  4.4996, -12.3906])\n",
      "\t Grad: tensor([-0.1477,  0.8362])\n",
      "Epoch 748, Loss 5.042816638946533\n",
      "\t Params: tensor([  4.5011, -12.3990])\n",
      "\t Grad: tensor([-0.1475,  0.8348])\n",
      "Epoch 749, Loss 5.035635948181152\n",
      "\t Params: tensor([  4.5026, -12.4073])\n",
      "\t Grad: tensor([-0.1472,  0.8334])\n",
      "Epoch 750, Loss 5.028476238250732\n",
      "\t Params: tensor([  4.5040, -12.4156])\n",
      "\t Grad: tensor([-0.1470,  0.8320])\n",
      "Epoch 751, Loss 5.021346092224121\n",
      "\t Params: tensor([  4.5055, -12.4239])\n",
      "\t Grad: tensor([-0.1467,  0.8305])\n",
      "Epoch 752, Loss 5.01423978805542\n",
      "\t Params: tensor([  4.5070, -12.4322])\n",
      "\t Grad: tensor([-0.1465,  0.8291])\n",
      "Epoch 753, Loss 5.007157325744629\n",
      "\t Params: tensor([  4.5084, -12.4405])\n",
      "\t Grad: tensor([-0.1462,  0.8277])\n",
      "Epoch 754, Loss 5.000098705291748\n",
      "\t Params: tensor([  4.5099, -12.4488])\n",
      "\t Grad: tensor([-0.1460,  0.8263])\n",
      "Epoch 755, Loss 4.9930644035339355\n",
      "\t Params: tensor([  4.5113, -12.4570])\n",
      "\t Grad: tensor([-0.1457,  0.8249])\n",
      "Epoch 756, Loss 4.986050605773926\n",
      "\t Params: tensor([  4.5128, -12.4653])\n",
      "\t Grad: tensor([-0.1455,  0.8235])\n",
      "Epoch 757, Loss 4.979064464569092\n",
      "\t Params: tensor([  4.5143, -12.4735])\n",
      "\t Grad: tensor([-0.1452,  0.8221])\n",
      "Epoch 758, Loss 4.972100257873535\n",
      "\t Params: tensor([  4.5157, -12.4817])\n",
      "\t Grad: tensor([-0.1450,  0.8207])\n",
      "Epoch 759, Loss 4.965158939361572\n",
      "\t Params: tensor([  4.5172, -12.4899])\n",
      "\t Grad: tensor([-0.1447,  0.8193])\n",
      "Epoch 760, Loss 4.958244800567627\n",
      "\t Params: tensor([  4.5186, -12.4981])\n",
      "\t Grad: tensor([-0.1445,  0.8179])\n",
      "Epoch 761, Loss 4.951350688934326\n",
      "\t Params: tensor([  4.5200, -12.5062])\n",
      "\t Grad: tensor([-0.1443,  0.8165])\n",
      "Epoch 762, Loss 4.944478988647461\n",
      "\t Params: tensor([  4.5215, -12.5144])\n",
      "\t Grad: tensor([-0.1440,  0.8152])\n",
      "Epoch 763, Loss 4.9376325607299805\n",
      "\t Params: tensor([  4.5229, -12.5225])\n",
      "\t Grad: tensor([-0.1438,  0.8138])\n",
      "Epoch 764, Loss 4.930812358856201\n",
      "\t Params: tensor([  4.5244, -12.5306])\n",
      "\t Grad: tensor([-0.1435,  0.8124])\n",
      "Epoch 765, Loss 4.924009323120117\n",
      "\t Params: tensor([  4.5258, -12.5387])\n",
      "\t Grad: tensor([-0.1433,  0.8110])\n",
      "Epoch 766, Loss 4.917233943939209\n",
      "\t Params: tensor([  4.5272, -12.5468])\n",
      "\t Grad: tensor([-0.1430,  0.8096])\n",
      "Epoch 767, Loss 4.91048002243042\n",
      "\t Params: tensor([  4.5286, -12.5549])\n",
      "\t Grad: tensor([-0.1428,  0.8083])\n",
      "Epoch 768, Loss 4.903748989105225\n",
      "\t Params: tensor([  4.5301, -12.5630])\n",
      "\t Grad: tensor([-0.1426,  0.8069])\n",
      "Epoch 769, Loss 4.897039890289307\n",
      "\t Params: tensor([  4.5315, -12.5711])\n",
      "\t Grad: tensor([-0.1423,  0.8055])\n",
      "Epoch 770, Loss 4.890355587005615\n",
      "\t Params: tensor([  4.5329, -12.5791])\n",
      "\t Grad: tensor([-0.1420,  0.8042])\n",
      "Epoch 771, Loss 4.883691787719727\n",
      "\t Params: tensor([  4.5343, -12.5871])\n",
      "\t Grad: tensor([-0.1418,  0.8028])\n",
      "Epoch 772, Loss 4.877051830291748\n",
      "\t Params: tensor([  4.5357, -12.5951])\n",
      "\t Grad: tensor([-0.1416,  0.8014])\n",
      "Epoch 773, Loss 4.87043571472168\n",
      "\t Params: tensor([  4.5372, -12.6031])\n",
      "\t Grad: tensor([-0.1413,  0.8001])\n",
      "Epoch 774, Loss 4.8638386726379395\n",
      "\t Params: tensor([  4.5386, -12.6111])\n",
      "\t Grad: tensor([-0.1411,  0.7987])\n",
      "Epoch 775, Loss 4.8572678565979\n",
      "\t Params: tensor([  4.5400, -12.6191])\n",
      "\t Grad: tensor([-0.1408,  0.7973])\n",
      "Epoch 776, Loss 4.850717544555664\n",
      "\t Params: tensor([  4.5414, -12.6271])\n",
      "\t Grad: tensor([-0.1406,  0.7960])\n",
      "Epoch 777, Loss 4.844189167022705\n",
      "\t Params: tensor([  4.5428, -12.6350])\n",
      "\t Grad: tensor([-0.1404,  0.7946])\n",
      "Epoch 778, Loss 4.837683200836182\n",
      "\t Params: tensor([  4.5442, -12.6429])\n",
      "\t Grad: tensor([-0.1401,  0.7933])\n",
      "Epoch 779, Loss 4.831196308135986\n",
      "\t Params: tensor([  4.5456, -12.6509])\n",
      "\t Grad: tensor([-0.1399,  0.7919])\n",
      "Epoch 780, Loss 4.824736595153809\n",
      "\t Params: tensor([  4.5470, -12.6588])\n",
      "\t Grad: tensor([-0.1397,  0.7906])\n",
      "Epoch 781, Loss 4.818297863006592\n",
      "\t Params: tensor([  4.5484, -12.6667])\n",
      "\t Grad: tensor([-0.1394,  0.7893])\n",
      "Epoch 782, Loss 4.8118791580200195\n",
      "\t Params: tensor([  4.5498, -12.6745])\n",
      "\t Grad: tensor([-0.1392,  0.7879])\n",
      "Epoch 783, Loss 4.80548095703125\n",
      "\t Params: tensor([  4.5512, -12.6824])\n",
      "\t Grad: tensor([-0.1389,  0.7866])\n",
      "Epoch 784, Loss 4.799106121063232\n",
      "\t Params: tensor([  4.5525, -12.6902])\n",
      "\t Grad: tensor([-0.1387,  0.7852])\n",
      "Epoch 785, Loss 4.792754650115967\n",
      "\t Params: tensor([  4.5539, -12.6981])\n",
      "\t Grad: tensor([-0.1385,  0.7839])\n",
      "Epoch 786, Loss 4.786421775817871\n",
      "\t Params: tensor([  4.5553, -12.7059])\n",
      "\t Grad: tensor([-0.1383,  0.7826])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 787, Loss 4.780111789703369\n",
      "\t Params: tensor([  4.5567, -12.7137])\n",
      "\t Grad: tensor([-0.1380,  0.7812])\n",
      "Epoch 788, Loss 4.7738237380981445\n",
      "\t Params: tensor([  4.5581, -12.7215])\n",
      "\t Grad: tensor([-0.1378,  0.7799])\n",
      "Epoch 789, Loss 4.7675580978393555\n",
      "\t Params: tensor([  4.5594, -12.7293])\n",
      "\t Grad: tensor([-0.1375,  0.7786])\n",
      "Epoch 790, Loss 4.7613115310668945\n",
      "\t Params: tensor([  4.5608, -12.7371])\n",
      "\t Grad: tensor([-0.1373,  0.7773])\n",
      "Epoch 791, Loss 4.755086898803711\n",
      "\t Params: tensor([  4.5622, -12.7448])\n",
      "\t Grad: tensor([-0.1371,  0.7759])\n",
      "Epoch 792, Loss 4.748885154724121\n",
      "\t Params: tensor([  4.5636, -12.7526])\n",
      "\t Grad: tensor([-0.1368,  0.7746])\n",
      "Epoch 793, Loss 4.742700099945068\n",
      "\t Params: tensor([  4.5649, -12.7603])\n",
      "\t Grad: tensor([-0.1366,  0.7733])\n",
      "Epoch 794, Loss 4.736537456512451\n",
      "\t Params: tensor([  4.5663, -12.7680])\n",
      "\t Grad: tensor([-0.1364,  0.7720])\n",
      "Epoch 795, Loss 4.730396747589111\n",
      "\t Params: tensor([  4.5677, -12.7758])\n",
      "\t Grad: tensor([-0.1361,  0.7707])\n",
      "Epoch 796, Loss 4.724279403686523\n",
      "\t Params: tensor([  4.5690, -12.7834])\n",
      "\t Grad: tensor([-0.1359,  0.7694])\n",
      "Epoch 797, Loss 4.718181133270264\n",
      "\t Params: tensor([  4.5704, -12.7911])\n",
      "\t Grad: tensor([-0.1357,  0.7681])\n",
      "Epoch 798, Loss 4.712100982666016\n",
      "\t Params: tensor([  4.5717, -12.7988])\n",
      "\t Grad: tensor([-0.1354,  0.7668])\n",
      "Epoch 799, Loss 4.706046104431152\n",
      "\t Params: tensor([  4.5731, -12.8064])\n",
      "\t Grad: tensor([-0.1352,  0.7655])\n",
      "Epoch 800, Loss 4.700008869171143\n",
      "\t Params: tensor([  4.5744, -12.8141])\n",
      "\t Grad: tensor([-0.1350,  0.7642])\n",
      "Epoch 801, Loss 4.6939897537231445\n",
      "\t Params: tensor([  4.5758, -12.8217])\n",
      "\t Grad: tensor([-0.1347,  0.7629])\n",
      "Epoch 802, Loss 4.687995433807373\n",
      "\t Params: tensor([  4.5771, -12.8293])\n",
      "\t Grad: tensor([-0.1345,  0.7616])\n",
      "Epoch 803, Loss 4.6820197105407715\n",
      "\t Params: tensor([  4.5785, -12.8369])\n",
      "\t Grad: tensor([-0.1343,  0.7603])\n",
      "Epoch 804, Loss 4.676063060760498\n",
      "\t Params: tensor([  4.5798, -12.8445])\n",
      "\t Grad: tensor([-0.1341,  0.7590])\n",
      "Epoch 805, Loss 4.670130252838135\n",
      "\t Params: tensor([  4.5811, -12.8521])\n",
      "\t Grad: tensor([-0.1338,  0.7577])\n",
      "Epoch 806, Loss 4.664214134216309\n",
      "\t Params: tensor([  4.5825, -12.8597])\n",
      "\t Grad: tensor([-0.1336,  0.7564])\n",
      "Epoch 807, Loss 4.658319473266602\n",
      "\t Params: tensor([  4.5838, -12.8672])\n",
      "\t Grad: tensor([-0.1334,  0.7551])\n",
      "Epoch 808, Loss 4.652444839477539\n",
      "\t Params: tensor([  4.5851, -12.8748])\n",
      "\t Grad: tensor([-0.1332,  0.7538])\n",
      "Epoch 809, Loss 4.646592140197754\n",
      "\t Params: tensor([  4.5865, -12.8823])\n",
      "\t Grad: tensor([-0.1330,  0.7526])\n",
      "Epoch 810, Loss 4.640753746032715\n",
      "\t Params: tensor([  4.5878, -12.8898])\n",
      "\t Grad: tensor([-0.1327,  0.7513])\n",
      "Epoch 811, Loss 4.634937763214111\n",
      "\t Params: tensor([  4.5891, -12.8973])\n",
      "\t Grad: tensor([-0.1325,  0.7500])\n",
      "Epoch 812, Loss 4.6291422843933105\n",
      "\t Params: tensor([  4.5904, -12.9048])\n",
      "\t Grad: tensor([-0.1323,  0.7487])\n",
      "Epoch 813, Loss 4.6233673095703125\n",
      "\t Params: tensor([  4.5918, -12.9123])\n",
      "\t Grad: tensor([-0.1320,  0.7475])\n",
      "Epoch 814, Loss 4.617611408233643\n",
      "\t Params: tensor([  4.5931, -12.9197])\n",
      "\t Grad: tensor([-0.1318,  0.7462])\n",
      "Epoch 815, Loss 4.61187219619751\n",
      "\t Params: tensor([  4.5944, -12.9272])\n",
      "\t Grad: tensor([-0.1316,  0.7449])\n",
      "Epoch 816, Loss 4.606155872344971\n",
      "\t Params: tensor([  4.5957, -12.9346])\n",
      "\t Grad: tensor([-0.1314,  0.7437])\n",
      "Epoch 817, Loss 4.600458145141602\n",
      "\t Params: tensor([  4.5970, -12.9420])\n",
      "\t Grad: tensor([-0.1311,  0.7424])\n",
      "Epoch 818, Loss 4.594779968261719\n",
      "\t Params: tensor([  4.5983, -12.9494])\n",
      "\t Grad: tensor([-0.1309,  0.7411])\n",
      "Epoch 819, Loss 4.5891194343566895\n",
      "\t Params: tensor([  4.5996, -12.9568])\n",
      "\t Grad: tensor([-0.1307,  0.7399])\n",
      "Epoch 820, Loss 4.583479404449463\n",
      "\t Params: tensor([  4.6009, -12.9642])\n",
      "\t Grad: tensor([-0.1305,  0.7386])\n",
      "Epoch 821, Loss 4.577857494354248\n",
      "\t Params: tensor([  4.6022, -12.9716])\n",
      "\t Grad: tensor([-0.1303,  0.7374])\n",
      "Epoch 822, Loss 4.572255611419678\n",
      "\t Params: tensor([  4.6035, -12.9790])\n",
      "\t Grad: tensor([-0.1300,  0.7361])\n",
      "Epoch 823, Loss 4.566675186157227\n",
      "\t Params: tensor([  4.6048, -12.9863])\n",
      "\t Grad: tensor([-0.1298,  0.7349])\n",
      "Epoch 824, Loss 4.561108112335205\n",
      "\t Params: tensor([  4.6061, -12.9936])\n",
      "\t Grad: tensor([-0.1296,  0.7336])\n",
      "Epoch 825, Loss 4.555565357208252\n",
      "\t Params: tensor([  4.6074, -13.0010])\n",
      "\t Grad: tensor([-0.1294,  0.7324])\n",
      "Epoch 826, Loss 4.550038814544678\n",
      "\t Params: tensor([  4.6087, -13.0083])\n",
      "\t Grad: tensor([-0.1292,  0.7311])\n",
      "Epoch 827, Loss 4.544533729553223\n",
      "\t Params: tensor([  4.6100, -13.0156])\n",
      "\t Grad: tensor([-0.1289,  0.7299])\n",
      "Epoch 828, Loss 4.539044380187988\n",
      "\t Params: tensor([  4.6113, -13.0229])\n",
      "\t Grad: tensor([-0.1287,  0.7286])\n",
      "Epoch 829, Loss 4.53357458114624\n",
      "\t Params: tensor([  4.6126, -13.0301])\n",
      "\t Grad: tensor([-0.1285,  0.7274])\n",
      "Epoch 830, Loss 4.528122425079346\n",
      "\t Params: tensor([  4.6139, -13.0374])\n",
      "\t Grad: tensor([-0.1283,  0.7262])\n",
      "Epoch 831, Loss 4.522690773010254\n",
      "\t Params: tensor([  4.6152, -13.0446])\n",
      "\t Grad: tensor([-0.1280,  0.7249])\n",
      "Epoch 832, Loss 4.517275810241699\n",
      "\t Params: tensor([  4.6164, -13.0519])\n",
      "\t Grad: tensor([-0.1278,  0.7237])\n",
      "Epoch 833, Loss 4.5118794441223145\n",
      "\t Params: tensor([  4.6177, -13.0591])\n",
      "\t Grad: tensor([-0.1276,  0.7225])\n",
      "Epoch 834, Loss 4.506504535675049\n",
      "\t Params: tensor([  4.6190, -13.0663])\n",
      "\t Grad: tensor([-0.1274,  0.7212])\n",
      "Epoch 835, Loss 4.501140594482422\n",
      "\t Params: tensor([  4.6203, -13.0735])\n",
      "\t Grad: tensor([-0.1272,  0.7200])\n",
      "Epoch 836, Loss 4.495800971984863\n",
      "\t Params: tensor([  4.6215, -13.0807])\n",
      "\t Grad: tensor([-0.1270,  0.7188])\n",
      "Epoch 837, Loss 4.490474700927734\n",
      "\t Params: tensor([  4.6228, -13.0879])\n",
      "\t Grad: tensor([-0.1268,  0.7176])\n",
      "Epoch 838, Loss 4.485169410705566\n",
      "\t Params: tensor([  4.6241, -13.0950])\n",
      "\t Grad: tensor([-0.1266,  0.7163])\n",
      "Epoch 839, Loss 4.479884147644043\n",
      "\t Params: tensor([  4.6253, -13.1022])\n",
      "\t Grad: tensor([-0.1263,  0.7151])\n",
      "Epoch 840, Loss 4.474613189697266\n",
      "\t Params: tensor([  4.6266, -13.1093])\n",
      "\t Grad: tensor([-0.1261,  0.7139])\n",
      "Epoch 841, Loss 4.469363689422607\n",
      "\t Params: tensor([  4.6278, -13.1165])\n",
      "\t Grad: tensor([-0.1259,  0.7127])\n",
      "Epoch 842, Loss 4.46412992477417\n",
      "\t Params: tensor([  4.6291, -13.1236])\n",
      "\t Grad: tensor([-0.1257,  0.7115])\n",
      "Epoch 843, Loss 4.4589128494262695\n",
      "\t Params: tensor([  4.6304, -13.1307])\n",
      "\t Grad: tensor([-0.1255,  0.7103])\n",
      "Epoch 844, Loss 4.453715801239014\n",
      "\t Params: tensor([  4.6316, -13.1378])\n",
      "\t Grad: tensor([-0.1253,  0.7091])\n",
      "Epoch 845, Loss 4.448534965515137\n",
      "\t Params: tensor([  4.6329, -13.1449])\n",
      "\t Grad: tensor([-0.1250,  0.7079])\n",
      "Epoch 846, Loss 4.4433722496032715\n",
      "\t Params: tensor([  4.6341, -13.1519])\n",
      "\t Grad: tensor([-0.1249,  0.7067])\n",
      "Epoch 847, Loss 4.438226222991943\n",
      "\t Params: tensor([  4.6353, -13.1590])\n",
      "\t Grad: tensor([-0.1246,  0.7055])\n",
      "Epoch 848, Loss 4.433098793029785\n",
      "\t Params: tensor([  4.6366, -13.1660])\n",
      "\t Grad: tensor([-0.1244,  0.7043])\n",
      "Epoch 849, Loss 4.427990436553955\n",
      "\t Params: tensor([  4.6378, -13.1730])\n",
      "\t Grad: tensor([-0.1242,  0.7031])\n",
      "Epoch 850, Loss 4.422896862030029\n",
      "\t Params: tensor([  4.6391, -13.1801])\n",
      "\t Grad: tensor([-0.1240,  0.7019])\n",
      "Epoch 851, Loss 4.417819499969482\n",
      "\t Params: tensor([  4.6403, -13.1871])\n",
      "\t Grad: tensor([-0.1238,  0.7007])\n",
      "Epoch 852, Loss 4.41276216506958\n",
      "\t Params: tensor([  4.6415, -13.1941])\n",
      "\t Grad: tensor([-0.1236,  0.6995])\n",
      "Epoch 853, Loss 4.407720565795898\n",
      "\t Params: tensor([  4.6428, -13.2010])\n",
      "\t Grad: tensor([-0.1234,  0.6983])\n",
      "Epoch 854, Loss 4.402697563171387\n",
      "\t Params: tensor([  4.6440, -13.2080])\n",
      "\t Grad: tensor([-0.1232,  0.6971])\n",
      "Epoch 855, Loss 4.397687911987305\n",
      "\t Params: tensor([  4.6452, -13.2150])\n",
      "\t Grad: tensor([-0.1229,  0.6959])\n",
      "Epoch 856, Loss 4.392696857452393\n",
      "\t Params: tensor([  4.6465, -13.2219])\n",
      "\t Grad: tensor([-0.1227,  0.6948])\n",
      "Epoch 857, Loss 4.387725353240967\n",
      "\t Params: tensor([  4.6477, -13.2289])\n",
      "\t Grad: tensor([-0.1225,  0.6936])\n",
      "Epoch 858, Loss 4.382769584655762\n",
      "\t Params: tensor([  4.6489, -13.2358])\n",
      "\t Grad: tensor([-0.1223,  0.6924])\n",
      "Epoch 859, Loss 4.377828121185303\n",
      "\t Params: tensor([  4.6501, -13.2427])\n",
      "\t Grad: tensor([-0.1221,  0.6912])\n",
      "Epoch 860, Loss 4.3729047775268555\n",
      "\t Params: tensor([  4.6514, -13.2496])\n",
      "\t Grad: tensor([-0.1219,  0.6901])\n",
      "Epoch 861, Loss 4.36799955368042\n",
      "\t Params: tensor([  4.6526, -13.2565])\n",
      "\t Grad: tensor([-0.1217,  0.6889])\n",
      "Epoch 862, Loss 4.3631110191345215\n",
      "\t Params: tensor([  4.6538, -13.2634])\n",
      "\t Grad: tensor([-0.1215,  0.6877])\n",
      "Epoch 863, Loss 4.3582377433776855\n",
      "\t Params: tensor([  4.6550, -13.2702])\n",
      "\t Grad: tensor([-0.1213,  0.6865])\n",
      "Epoch 864, Loss 4.3533830642700195\n",
      "\t Params: tensor([  4.6562, -13.2771])\n",
      "\t Grad: tensor([-0.1211,  0.6854])\n",
      "Epoch 865, Loss 4.348541736602783\n",
      "\t Params: tensor([  4.6574, -13.2839])\n",
      "\t Grad: tensor([-0.1209,  0.6842])\n",
      "Epoch 866, Loss 4.343716144561768\n",
      "\t Params: tensor([  4.6586, -13.2908])\n",
      "\t Grad: tensor([-0.1207,  0.6830])\n",
      "Epoch 867, Loss 4.338911056518555\n",
      "\t Params: tensor([  4.6598, -13.2976])\n",
      "\t Grad: tensor([-0.1205,  0.6819])\n",
      "Epoch 868, Loss 4.334120273590088\n",
      "\t Params: tensor([  4.6610, -13.3044])\n",
      "\t Grad: tensor([-0.1203,  0.6807])\n",
      "Epoch 869, Loss 4.329345226287842\n",
      "\t Params: tensor([  4.6622, -13.3112])\n",
      "\t Grad: tensor([-0.1201,  0.6796])\n",
      "Epoch 870, Loss 4.324588298797607\n",
      "\t Params: tensor([  4.6634, -13.3180])\n",
      "\t Grad: tensor([-0.1198,  0.6784])\n",
      "Epoch 871, Loss 4.319845676422119\n",
      "\t Params: tensor([  4.6646, -13.3247])\n",
      "\t Grad: tensor([-0.1196,  0.6773])\n",
      "Epoch 872, Loss 4.315117359161377\n",
      "\t Params: tensor([  4.6658, -13.3315])\n",
      "\t Grad: tensor([-0.1195,  0.6761])\n",
      "Epoch 873, Loss 4.310409069061279\n",
      "\t Params: tensor([  4.6670, -13.3382])\n",
      "\t Grad: tensor([-0.1192,  0.6750])\n",
      "Epoch 874, Loss 4.305714130401611\n",
      "\t Params: tensor([  4.6682, -13.3450])\n",
      "\t Grad: tensor([-0.1190,  0.6738])\n",
      "Epoch 875, Loss 4.3010358810424805\n",
      "\t Params: tensor([  4.6694, -13.3517])\n",
      "\t Grad: tensor([-0.1188,  0.6727])\n",
      "Epoch 876, Loss 4.296375751495361\n",
      "\t Params: tensor([  4.6706, -13.3584])\n",
      "\t Grad: tensor([-0.1186,  0.6715])\n",
      "Epoch 877, Loss 4.291726589202881\n",
      "\t Params: tensor([  4.6718, -13.3651])\n",
      "\t Grad: tensor([-0.1184,  0.6704])\n",
      "Epoch 878, Loss 4.287097930908203\n",
      "\t Params: tensor([  4.6730, -13.3718])\n",
      "\t Grad: tensor([-0.1182,  0.6693])\n",
      "Epoch 879, Loss 4.282481670379639\n",
      "\t Params: tensor([  4.6741, -13.3785])\n",
      "\t Grad: tensor([-0.1180,  0.6681])\n",
      "Epoch 880, Loss 4.277881622314453\n",
      "\t Params: tensor([  4.6753, -13.3852])\n",
      "\t Grad: tensor([-0.1178,  0.6670])\n",
      "Epoch 881, Loss 4.273299217224121\n",
      "\t Params: tensor([  4.6765, -13.3918])\n",
      "\t Grad: tensor([-0.1176,  0.6658])\n",
      "Epoch 882, Loss 4.268731594085693\n",
      "\t Params: tensor([  4.6777, -13.3985])\n",
      "\t Grad: tensor([-0.1174,  0.6647])\n",
      "Epoch 883, Loss 4.2641777992248535\n",
      "\t Params: tensor([  4.6788, -13.4051])\n",
      "\t Grad: tensor([-0.1172,  0.6636])\n",
      "Epoch 884, Loss 4.259643077850342\n",
      "\t Params: tensor([  4.6800, -13.4117])\n",
      "\t Grad: tensor([-0.1170,  0.6625])\n",
      "Epoch 885, Loss 4.255119800567627\n",
      "\t Params: tensor([  4.6812, -13.4184])\n",
      "\t Grad: tensor([-0.1168,  0.6613])\n",
      "Epoch 886, Loss 4.250613689422607\n",
      "\t Params: tensor([  4.6823, -13.4250])\n",
      "\t Grad: tensor([-0.1166,  0.6602])\n",
      "Epoch 887, Loss 4.246123790740967\n",
      "\t Params: tensor([  4.6835, -13.4316])\n",
      "\t Grad: tensor([-0.1164,  0.6591])\n",
      "Epoch 888, Loss 4.241647720336914\n",
      "\t Params: tensor([  4.6847, -13.4381])\n",
      "\t Grad: tensor([-0.1162,  0.6580])\n",
      "Epoch 889, Loss 4.237185001373291\n",
      "\t Params: tensor([  4.6858, -13.4447])\n",
      "\t Grad: tensor([-0.1160,  0.6569])\n",
      "Epoch 890, Loss 4.2327399253845215\n",
      "\t Params: tensor([  4.6870, -13.4513])\n",
      "\t Grad: tensor([-0.1158,  0.6557])\n",
      "Epoch 891, Loss 4.228307723999023\n",
      "\t Params: tensor([  4.6881, -13.4578])\n",
      "\t Grad: tensor([-0.1157,  0.6546])\n",
      "Epoch 892, Loss 4.223895072937012\n",
      "\t Params: tensor([  4.6893, -13.4643])\n",
      "\t Grad: tensor([-0.1154,  0.6535])\n",
      "Epoch 893, Loss 4.219494342803955\n",
      "\t Params: tensor([  4.6904, -13.4709])\n",
      "\t Grad: tensor([-0.1153,  0.6524])\n",
      "Epoch 894, Loss 4.215109348297119\n",
      "\t Params: tensor([  4.6916, -13.4774])\n",
      "\t Grad: tensor([-0.1151,  0.6513])\n",
      "Epoch 895, Loss 4.210737228393555\n",
      "\t Params: tensor([  4.6927, -13.4839])\n",
      "\t Grad: tensor([-0.1148,  0.6502])\n",
      "Epoch 896, Loss 4.206383228302002\n",
      "\t Params: tensor([  4.6939, -13.4904])\n",
      "\t Grad: tensor([-0.1147,  0.6491])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 897, Loss 4.202042579650879\n",
      "\t Params: tensor([  4.6950, -13.4968])\n",
      "\t Grad: tensor([-0.1145,  0.6480])\n",
      "Epoch 898, Loss 4.1977152824401855\n",
      "\t Params: tensor([  4.6962, -13.5033])\n",
      "\t Grad: tensor([-0.1143,  0.6469])\n",
      "Epoch 899, Loss 4.1934051513671875\n",
      "\t Params: tensor([  4.6973, -13.5098])\n",
      "\t Grad: tensor([-0.1141,  0.6458])\n",
      "Epoch 900, Loss 4.189108371734619\n",
      "\t Params: tensor([  4.6985, -13.5162])\n",
      "\t Grad: tensor([-0.1139,  0.6447])\n",
      "Epoch 901, Loss 4.184825420379639\n",
      "\t Params: tensor([  4.6996, -13.5227])\n",
      "\t Grad: tensor([-0.1137,  0.6436])\n",
      "Epoch 902, Loss 4.180559158325195\n",
      "\t Params: tensor([  4.7007, -13.5291])\n",
      "\t Grad: tensor([-0.1135,  0.6425])\n",
      "Epoch 903, Loss 4.176304817199707\n",
      "\t Params: tensor([  4.7019, -13.5355])\n",
      "\t Grad: tensor([-0.1133,  0.6414])\n",
      "Epoch 904, Loss 4.172065258026123\n",
      "\t Params: tensor([  4.7030, -13.5419])\n",
      "\t Grad: tensor([-0.1131,  0.6403])\n",
      "Epoch 905, Loss 4.167842388153076\n",
      "\t Params: tensor([  4.7041, -13.5483])\n",
      "\t Grad: tensor([-0.1129,  0.6392])\n",
      "Epoch 906, Loss 4.163630485534668\n",
      "\t Params: tensor([  4.7053, -13.5547])\n",
      "\t Grad: tensor([-0.1127,  0.6381])\n",
      "Epoch 907, Loss 4.159435749053955\n",
      "\t Params: tensor([  4.7064, -13.5610])\n",
      "\t Grad: tensor([-0.1125,  0.6371])\n",
      "Epoch 908, Loss 4.155252933502197\n",
      "\t Params: tensor([  4.7075, -13.5674])\n",
      "\t Grad: tensor([-0.1124,  0.6360])\n",
      "Epoch 909, Loss 4.151086330413818\n",
      "\t Params: tensor([  4.7086, -13.5738])\n",
      "\t Grad: tensor([-0.1122,  0.6349])\n",
      "Epoch 910, Loss 4.1469340324401855\n",
      "\t Params: tensor([  4.7097, -13.5801])\n",
      "\t Grad: tensor([-0.1120,  0.6338])\n",
      "Epoch 911, Loss 4.142794132232666\n",
      "\t Params: tensor([  4.7109, -13.5864])\n",
      "\t Grad: tensor([-0.1118,  0.6327])\n",
      "Epoch 912, Loss 4.138669013977051\n",
      "\t Params: tensor([  4.7120, -13.5927])\n",
      "\t Grad: tensor([-0.1116,  0.6317])\n",
      "Epoch 913, Loss 4.13455867767334\n",
      "\t Params: tensor([  4.7131, -13.5990])\n",
      "\t Grad: tensor([-0.1114,  0.6306])\n",
      "Epoch 914, Loss 4.130464553833008\n",
      "\t Params: tensor([  4.7142, -13.6053])\n",
      "\t Grad: tensor([-0.1112,  0.6295])\n",
      "Epoch 915, Loss 4.126377582550049\n",
      "\t Params: tensor([  4.7153, -13.6116])\n",
      "\t Grad: tensor([-0.1110,  0.6284])\n",
      "Epoch 916, Loss 4.122309684753418\n",
      "\t Params: tensor([  4.7164, -13.6179])\n",
      "\t Grad: tensor([-0.1108,  0.6274])\n",
      "Epoch 917, Loss 4.118252754211426\n",
      "\t Params: tensor([  4.7175, -13.6242])\n",
      "\t Grad: tensor([-0.1107,  0.6263])\n",
      "Epoch 918, Loss 4.114212512969971\n",
      "\t Params: tensor([  4.7186, -13.6304])\n",
      "\t Grad: tensor([-0.1104,  0.6253])\n",
      "Epoch 919, Loss 4.110184192657471\n",
      "\t Params: tensor([  4.7197, -13.6367])\n",
      "\t Grad: tensor([-0.1103,  0.6242])\n",
      "Epoch 920, Loss 4.106170177459717\n",
      "\t Params: tensor([  4.7208, -13.6429])\n",
      "\t Grad: tensor([-0.1101,  0.6231])\n",
      "Epoch 921, Loss 4.102170944213867\n",
      "\t Params: tensor([  4.7219, -13.6491])\n",
      "\t Grad: tensor([-0.1099,  0.6221])\n",
      "Epoch 922, Loss 4.098180770874023\n",
      "\t Params: tensor([  4.7230, -13.6553])\n",
      "\t Grad: tensor([-0.1097,  0.6210])\n",
      "Epoch 923, Loss 4.09420919418335\n",
      "\t Params: tensor([  4.7241, -13.6615])\n",
      "\t Grad: tensor([-0.1095,  0.6200])\n",
      "Epoch 924, Loss 4.090250015258789\n",
      "\t Params: tensor([  4.7252, -13.6677])\n",
      "\t Grad: tensor([-0.1093,  0.6189])\n",
      "Epoch 925, Loss 4.086300373077393\n",
      "\t Params: tensor([  4.7263, -13.6739])\n",
      "\t Grad: tensor([-0.1091,  0.6179])\n",
      "Epoch 926, Loss 4.082366466522217\n",
      "\t Params: tensor([  4.7274, -13.6800])\n",
      "\t Grad: tensor([-0.1090,  0.6168])\n",
      "Epoch 927, Loss 4.078448295593262\n",
      "\t Params: tensor([  4.7285, -13.6862])\n",
      "\t Grad: tensor([-0.1088,  0.6158])\n",
      "Epoch 928, Loss 4.074540138244629\n",
      "\t Params: tensor([  4.7296, -13.6924])\n",
      "\t Grad: tensor([-0.1086,  0.6147])\n",
      "Epoch 929, Loss 4.07064962387085\n",
      "\t Params: tensor([  4.7307, -13.6985])\n",
      "\t Grad: tensor([-0.1084,  0.6137])\n",
      "Epoch 930, Loss 4.066768646240234\n",
      "\t Params: tensor([  4.7317, -13.7046])\n",
      "\t Grad: tensor([-0.1082,  0.6126])\n",
      "Epoch 931, Loss 4.062900066375732\n",
      "\t Params: tensor([  4.7328, -13.7107])\n",
      "\t Grad: tensor([-0.1080,  0.6116])\n",
      "Epoch 932, Loss 4.059047222137451\n",
      "\t Params: tensor([  4.7339, -13.7168])\n",
      "\t Grad: tensor([-0.1079,  0.6105])\n",
      "Epoch 933, Loss 4.055204391479492\n",
      "\t Params: tensor([  4.7350, -13.7229])\n",
      "\t Grad: tensor([-0.1077,  0.6095])\n",
      "Epoch 934, Loss 4.05137825012207\n",
      "\t Params: tensor([  4.7360, -13.7290])\n",
      "\t Grad: tensor([-0.1075,  0.6085])\n",
      "Epoch 935, Loss 4.047563552856445\n",
      "\t Params: tensor([  4.7371, -13.7351])\n",
      "\t Grad: tensor([-0.1073,  0.6074])\n",
      "Epoch 936, Loss 4.043761730194092\n",
      "\t Params: tensor([  4.7382, -13.7412])\n",
      "\t Grad: tensor([-0.1071,  0.6064])\n",
      "Epoch 937, Loss 4.039972305297852\n",
      "\t Params: tensor([  4.7393, -13.7472])\n",
      "\t Grad: tensor([-0.1069,  0.6054])\n",
      "Epoch 938, Loss 4.036196708679199\n",
      "\t Params: tensor([  4.7403, -13.7533])\n",
      "\t Grad: tensor([-0.1068,  0.6043])\n",
      "Epoch 939, Loss 4.032433032989502\n",
      "\t Params: tensor([  4.7414, -13.7593])\n",
      "\t Grad: tensor([-0.1066,  0.6033])\n",
      "Epoch 940, Loss 4.028685092926025\n",
      "\t Params: tensor([  4.7425, -13.7653])\n",
      "\t Grad: tensor([-0.1064,  0.6023])\n",
      "Epoch 941, Loss 4.024947166442871\n",
      "\t Params: tensor([  4.7435, -13.7713])\n",
      "\t Grad: tensor([-0.1062,  0.6013])\n",
      "Epoch 942, Loss 4.021220684051514\n",
      "\t Params: tensor([  4.7446, -13.7773])\n",
      "\t Grad: tensor([-0.1060,  0.6003])\n",
      "Epoch 943, Loss 4.017508029937744\n",
      "\t Params: tensor([  4.7456, -13.7833])\n",
      "\t Grad: tensor([-0.1058,  0.5992])\n",
      "Epoch 944, Loss 4.013808727264404\n",
      "\t Params: tensor([  4.7467, -13.7893])\n",
      "\t Grad: tensor([-0.1057,  0.5982])\n",
      "Epoch 945, Loss 4.010122776031494\n",
      "\t Params: tensor([  4.7478, -13.7953])\n",
      "\t Grad: tensor([-0.1055,  0.5972])\n",
      "Epoch 946, Loss 4.006446361541748\n",
      "\t Params: tensor([  4.7488, -13.8012])\n",
      "\t Grad: tensor([-0.1053,  0.5962])\n",
      "Epoch 947, Loss 4.002786159515381\n",
      "\t Params: tensor([  4.7499, -13.8072])\n",
      "\t Grad: tensor([-0.1051,  0.5952])\n",
      "Epoch 948, Loss 3.9991350173950195\n",
      "\t Params: tensor([  4.7509, -13.8131])\n",
      "\t Grad: tensor([-0.1050,  0.5942])\n",
      "Epoch 949, Loss 3.995497941970825\n",
      "\t Params: tensor([  4.7520, -13.8191])\n",
      "\t Grad: tensor([-0.1048,  0.5931])\n",
      "Epoch 950, Loss 3.9918737411499023\n",
      "\t Params: tensor([  4.7530, -13.8250])\n",
      "\t Grad: tensor([-0.1046,  0.5921])\n",
      "Epoch 951, Loss 3.988260507583618\n",
      "\t Params: tensor([  4.7540, -13.8309])\n",
      "\t Grad: tensor([-0.1044,  0.5911])\n",
      "Epoch 952, Loss 3.9846603870391846\n",
      "\t Params: tensor([  4.7551, -13.8368])\n",
      "\t Grad: tensor([-0.1042,  0.5901])\n",
      "Epoch 953, Loss 3.9810731410980225\n",
      "\t Params: tensor([  4.7561, -13.8427])\n",
      "\t Grad: tensor([-0.1041,  0.5891])\n",
      "Epoch 954, Loss 3.9774956703186035\n",
      "\t Params: tensor([  4.7572, -13.8486])\n",
      "\t Grad: tensor([-0.1039,  0.5881])\n",
      "Epoch 955, Loss 3.973931312561035\n",
      "\t Params: tensor([  4.7582, -13.8544])\n",
      "\t Grad: tensor([-0.1037,  0.5871])\n",
      "Epoch 956, Loss 3.970381021499634\n",
      "\t Params: tensor([  4.7592, -13.8603])\n",
      "\t Grad: tensor([-0.1035,  0.5861])\n",
      "Epoch 957, Loss 3.9668405055999756\n",
      "\t Params: tensor([  4.7603, -13.8661])\n",
      "\t Grad: tensor([-0.1034,  0.5851])\n",
      "Epoch 958, Loss 3.963313102722168\n",
      "\t Params: tensor([  4.7613, -13.8720])\n",
      "\t Grad: tensor([-0.1032,  0.5841])\n",
      "Epoch 959, Loss 3.9597959518432617\n",
      "\t Params: tensor([  4.7623, -13.8778])\n",
      "\t Grad: tensor([-0.1030,  0.5831])\n",
      "Epoch 960, Loss 3.9562947750091553\n",
      "\t Params: tensor([  4.7634, -13.8836])\n",
      "\t Grad: tensor([-0.1028,  0.5822])\n",
      "Epoch 961, Loss 3.952801465988159\n",
      "\t Params: tensor([  4.7644, -13.8895])\n",
      "\t Grad: tensor([-0.1026,  0.5812])\n",
      "Epoch 962, Loss 3.9493227005004883\n",
      "\t Params: tensor([  4.7654, -13.8953])\n",
      "\t Grad: tensor([-0.1025,  0.5802])\n",
      "Epoch 963, Loss 3.945855140686035\n",
      "\t Params: tensor([  4.7664, -13.9010])\n",
      "\t Grad: tensor([-0.1023,  0.5792])\n",
      "Epoch 964, Loss 3.9423980712890625\n",
      "\t Params: tensor([  4.7675, -13.9068])\n",
      "\t Grad: tensor([-0.1021,  0.5782])\n",
      "Epoch 965, Loss 3.9389536380767822\n",
      "\t Params: tensor([  4.7685, -13.9126])\n",
      "\t Grad: tensor([-0.1020,  0.5772])\n",
      "Epoch 966, Loss 3.9355204105377197\n",
      "\t Params: tensor([  4.7695, -13.9184])\n",
      "\t Grad: tensor([-0.1018,  0.5762])\n",
      "Epoch 967, Loss 3.932095527648926\n",
      "\t Params: tensor([  4.7705, -13.9241])\n",
      "\t Grad: tensor([-0.1016,  0.5753])\n",
      "Epoch 968, Loss 3.9286880493164062\n",
      "\t Params: tensor([  4.7715, -13.9299])\n",
      "\t Grad: tensor([-0.1015,  0.5743])\n",
      "Epoch 969, Loss 3.9252915382385254\n",
      "\t Params: tensor([  4.7725, -13.9356])\n",
      "\t Grad: tensor([-0.1013,  0.5733])\n",
      "Epoch 970, Loss 3.921905517578125\n",
      "\t Params: tensor([  4.7736, -13.9413])\n",
      "\t Grad: tensor([-0.1011,  0.5723])\n",
      "Epoch 971, Loss 3.918527364730835\n",
      "\t Params: tensor([  4.7746, -13.9470])\n",
      "\t Grad: tensor([-0.1009,  0.5714])\n",
      "Epoch 972, Loss 3.915165901184082\n",
      "\t Params: tensor([  4.7756, -13.9527])\n",
      "\t Grad: tensor([-0.1008,  0.5704])\n",
      "Epoch 973, Loss 3.9118154048919678\n",
      "\t Params: tensor([  4.7766, -13.9584])\n",
      "\t Grad: tensor([-0.1006,  0.5694])\n",
      "Epoch 974, Loss 3.9084739685058594\n",
      "\t Params: tensor([  4.7776, -13.9641])\n",
      "\t Grad: tensor([-0.1004,  0.5685])\n",
      "Epoch 975, Loss 3.9051434993743896\n",
      "\t Params: tensor([  4.7786, -13.9698])\n",
      "\t Grad: tensor([-0.1003,  0.5675])\n",
      "Epoch 976, Loss 3.901824712753296\n",
      "\t Params: tensor([  4.7796, -13.9755])\n",
      "\t Grad: tensor([-0.1001,  0.5665])\n",
      "Epoch 977, Loss 3.898517370223999\n",
      "\t Params: tensor([  4.7806, -13.9811])\n",
      "\t Grad: tensor([-0.0999,  0.5656])\n",
      "Epoch 978, Loss 3.895221710205078\n",
      "\t Params: tensor([  4.7816, -13.9868])\n",
      "\t Grad: tensor([-0.0997,  0.5646])\n",
      "Epoch 979, Loss 3.891935110092163\n",
      "\t Params: tensor([  4.7826, -13.9924])\n",
      "\t Grad: tensor([-0.0996,  0.5637])\n",
      "Epoch 980, Loss 3.8886640071868896\n",
      "\t Params: tensor([  4.7836, -13.9980])\n",
      "\t Grad: tensor([-0.0994,  0.5627])\n",
      "Epoch 981, Loss 3.8854005336761475\n",
      "\t Params: tensor([  4.7846, -14.0036])\n",
      "\t Grad: tensor([-0.0992,  0.5617])\n",
      "Epoch 982, Loss 3.882150411605835\n",
      "\t Params: tensor([  4.7856, -14.0092])\n",
      "\t Grad: tensor([-0.0991,  0.5608])\n",
      "Epoch 983, Loss 3.8789103031158447\n",
      "\t Params: tensor([  4.7865, -14.0148])\n",
      "\t Grad: tensor([-0.0989,  0.5598])\n",
      "Epoch 984, Loss 3.875680446624756\n",
      "\t Params: tensor([  4.7875, -14.0204])\n",
      "\t Grad: tensor([-0.0987,  0.5589])\n",
      "Epoch 985, Loss 3.8724632263183594\n",
      "\t Params: tensor([  4.7885, -14.0260])\n",
      "\t Grad: tensor([-0.0986,  0.5579])\n",
      "Epoch 986, Loss 3.869255781173706\n",
      "\t Params: tensor([  4.7895, -14.0316])\n",
      "\t Grad: tensor([-0.0984,  0.5570])\n",
      "Epoch 987, Loss 3.866060495376587\n",
      "\t Params: tensor([  4.7905, -14.0371])\n",
      "\t Grad: tensor([-0.0982,  0.5560])\n",
      "Epoch 988, Loss 3.862872362136841\n",
      "\t Params: tensor([  4.7915, -14.0427])\n",
      "\t Grad: tensor([-0.0981,  0.5551])\n",
      "Epoch 989, Loss 3.859699010848999\n",
      "\t Params: tensor([  4.7924, -14.0482])\n",
      "\t Grad: tensor([-0.0979,  0.5541])\n",
      "Epoch 990, Loss 3.8565351963043213\n",
      "\t Params: tensor([  4.7934, -14.0538])\n",
      "\t Grad: tensor([-0.0978,  0.5532])\n",
      "Epoch 991, Loss 3.8533811569213867\n",
      "\t Params: tensor([  4.7944, -14.0593])\n",
      "\t Grad: tensor([-0.0976,  0.5523])\n",
      "Epoch 992, Loss 3.8502371311187744\n",
      "\t Params: tensor([  4.7954, -14.0648])\n",
      "\t Grad: tensor([-0.0974,  0.5513])\n",
      "Epoch 993, Loss 3.847109079360962\n",
      "\t Params: tensor([  4.7963, -14.0703])\n",
      "\t Grad: tensor([-0.0973,  0.5504])\n",
      "Epoch 994, Loss 3.8439841270446777\n",
      "\t Params: tensor([  4.7973, -14.0758])\n",
      "\t Grad: tensor([-0.0971,  0.5495])\n",
      "Epoch 995, Loss 3.8408761024475098\n",
      "\t Params: tensor([  4.7983, -14.0813])\n",
      "\t Grad: tensor([-0.0969,  0.5485])\n",
      "Epoch 996, Loss 3.837775468826294\n",
      "\t Params: tensor([  4.7992, -14.0868])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad: tensor([-0.0967,  0.5476])\n",
      "Epoch 997, Loss 3.8346855640411377\n",
      "\t Params: tensor([  4.8002, -14.0922])\n",
      "\t Grad: tensor([-0.0966,  0.5467])\n",
      "Epoch 998, Loss 3.831606388092041\n",
      "\t Params: tensor([  4.8012, -14.0977])\n",
      "\t Grad: tensor([-0.0964,  0.5457])\n",
      "Epoch 999, Loss 3.828537940979004\n",
      "\t Params: tensor([  4.8021, -14.1031])\n",
      "\t Grad: tensor([-0.0962,  0.5448])\n",
      "Epoch 1000, Loss 3.8254828453063965\n",
      "\t Params: tensor([  4.8031, -14.1086])\n",
      "\t Grad: tensor([-0.0961,  0.5439])\n",
      "Epoch 1001, Loss 3.82243275642395\n",
      "\t Params: tensor([  4.8041, -14.1140])\n",
      "\t Grad: tensor([-0.0959,  0.5430])\n",
      "Epoch 1002, Loss 3.8193979263305664\n",
      "\t Params: tensor([  4.8050, -14.1194])\n",
      "\t Grad: tensor([-0.0957,  0.5420])\n",
      "Epoch 1003, Loss 3.816368818283081\n",
      "\t Params: tensor([  4.8060, -14.1248])\n",
      "\t Grad: tensor([-0.0956,  0.5411])\n",
      "Epoch 1004, Loss 3.8133504390716553\n",
      "\t Params: tensor([  4.8069, -14.1302])\n",
      "\t Grad: tensor([-0.0954,  0.5402])\n",
      "Epoch 1005, Loss 3.8103437423706055\n",
      "\t Params: tensor([  4.8079, -14.1356])\n",
      "\t Grad: tensor([-0.0953,  0.5393])\n",
      "Epoch 1006, Loss 3.8073484897613525\n",
      "\t Params: tensor([  4.8088, -14.1410])\n",
      "\t Grad: tensor([-0.0951,  0.5384])\n",
      "Epoch 1007, Loss 3.8043596744537354\n",
      "\t Params: tensor([  4.8098, -14.1464])\n",
      "\t Grad: tensor([-0.0949,  0.5375])\n",
      "Epoch 1008, Loss 3.8013837337493896\n",
      "\t Params: tensor([  4.8107, -14.1518])\n",
      "\t Grad: tensor([-0.0948,  0.5365])\n",
      "Epoch 1009, Loss 3.7984206676483154\n",
      "\t Params: tensor([  4.8117, -14.1571])\n",
      "\t Grad: tensor([-0.0946,  0.5356])\n",
      "Epoch 1010, Loss 3.7954649925231934\n",
      "\t Params: tensor([  4.8126, -14.1625])\n",
      "\t Grad: tensor([-0.0945,  0.5347])\n",
      "Epoch 1011, Loss 3.792518377304077\n",
      "\t Params: tensor([  4.8136, -14.1678])\n",
      "\t Grad: tensor([-0.0943,  0.5338])\n",
      "Epoch 1012, Loss 3.789584159851074\n",
      "\t Params: tensor([  4.8145, -14.1731])\n",
      "\t Grad: tensor([-0.0942,  0.5329])\n",
      "Epoch 1013, Loss 3.7866575717926025\n",
      "\t Params: tensor([  4.8154, -14.1784])\n",
      "\t Grad: tensor([-0.0940,  0.5320])\n",
      "Epoch 1014, Loss 3.7837395668029785\n",
      "\t Params: tensor([  4.8164, -14.1838])\n",
      "\t Grad: tensor([-0.0938,  0.5311])\n",
      "Epoch 1015, Loss 3.780831813812256\n",
      "\t Params: tensor([  4.8173, -14.1891])\n",
      "\t Grad: tensor([-0.0937,  0.5302])\n",
      "Epoch 1016, Loss 3.7779388427734375\n",
      "\t Params: tensor([  4.8183, -14.1943])\n",
      "\t Grad: tensor([-0.0935,  0.5293])\n",
      "Epoch 1017, Loss 3.775052785873413\n",
      "\t Params: tensor([  4.8192, -14.1996])\n",
      "\t Grad: tensor([-0.0933,  0.5284])\n",
      "Epoch 1018, Loss 3.7721731662750244\n",
      "\t Params: tensor([  4.8201, -14.2049])\n",
      "\t Grad: tensor([-0.0932,  0.5275])\n",
      "Epoch 1019, Loss 3.769310712814331\n",
      "\t Params: tensor([  4.8210, -14.2102])\n",
      "\t Grad: tensor([-0.0930,  0.5266])\n",
      "Epoch 1020, Loss 3.7664504051208496\n",
      "\t Params: tensor([  4.8220, -14.2154])\n",
      "\t Grad: tensor([-0.0929,  0.5257])\n",
      "Epoch 1021, Loss 3.7636024951934814\n",
      "\t Params: tensor([  4.8229, -14.2207])\n",
      "\t Grad: tensor([-0.0927,  0.5248])\n",
      "Epoch 1022, Loss 3.760765790939331\n",
      "\t Params: tensor([  4.8238, -14.2259])\n",
      "\t Grad: tensor([-0.0926,  0.5239])\n",
      "Epoch 1023, Loss 3.7579362392425537\n",
      "\t Params: tensor([  4.8248, -14.2311])\n",
      "\t Grad: tensor([-0.0924,  0.5230])\n",
      "Epoch 1024, Loss 3.755117654800415\n",
      "\t Params: tensor([  4.8257, -14.2364])\n",
      "\t Grad: tensor([-0.0922,  0.5221])\n",
      "Epoch 1025, Loss 3.7523093223571777\n",
      "\t Params: tensor([  4.8266, -14.2416])\n",
      "\t Grad: tensor([-0.0921,  0.5213])\n",
      "Epoch 1026, Loss 3.7495110034942627\n",
      "\t Params: tensor([  4.8275, -14.2468])\n",
      "\t Grad: tensor([-0.0919,  0.5204])\n",
      "Epoch 1027, Loss 3.7467222213745117\n",
      "\t Params: tensor([  4.8284, -14.2520])\n",
      "\t Grad: tensor([-0.0918,  0.5195])\n",
      "Epoch 1028, Loss 3.7439401149749756\n",
      "\t Params: tensor([  4.8293, -14.2572])\n",
      "\t Grad: tensor([-0.0916,  0.5186])\n",
      "Epoch 1029, Loss 3.7411692142486572\n",
      "\t Params: tensor([  4.8303, -14.2623])\n",
      "\t Grad: tensor([-0.0915,  0.5177])\n",
      "Epoch 1030, Loss 3.7384068965911865\n",
      "\t Params: tensor([  4.8312, -14.2675])\n",
      "\t Grad: tensor([-0.0913,  0.5168])\n",
      "Epoch 1031, Loss 3.7356557846069336\n",
      "\t Params: tensor([  4.8321, -14.2727])\n",
      "\t Grad: tensor([-0.0912,  0.5160])\n",
      "Epoch 1032, Loss 3.7329137325286865\n",
      "\t Params: tensor([  4.8330, -14.2778])\n",
      "\t Grad: tensor([-0.0910,  0.5151])\n",
      "Epoch 1033, Loss 3.7301812171936035\n",
      "\t Params: tensor([  4.8339, -14.2830])\n",
      "\t Grad: tensor([-0.0908,  0.5142])\n",
      "Epoch 1034, Loss 3.7274558544158936\n",
      "\t Params: tensor([  4.8348, -14.2881])\n",
      "\t Grad: tensor([-0.0907,  0.5133])\n",
      "Epoch 1035, Loss 3.7247400283813477\n",
      "\t Params: tensor([  4.8357, -14.2932])\n",
      "\t Grad: tensor([-0.0905,  0.5125])\n",
      "Epoch 1036, Loss 3.722034454345703\n",
      "\t Params: tensor([  4.8366, -14.2983])\n",
      "\t Grad: tensor([-0.0904,  0.5116])\n",
      "Epoch 1037, Loss 3.7193374633789062\n",
      "\t Params: tensor([  4.8375, -14.3034])\n",
      "\t Grad: tensor([-0.0902,  0.5107])\n",
      "Epoch 1038, Loss 3.71665096282959\n",
      "\t Params: tensor([  4.8384, -14.3085])\n",
      "\t Grad: tensor([-0.0901,  0.5099])\n",
      "Epoch 1039, Loss 3.7139716148376465\n",
      "\t Params: tensor([  4.8393, -14.3136])\n",
      "\t Grad: tensor([-0.0899,  0.5090])\n",
      "Epoch 1040, Loss 3.711301565170288\n",
      "\t Params: tensor([  4.8402, -14.3187])\n",
      "\t Grad: tensor([-0.0898,  0.5081])\n",
      "Epoch 1041, Loss 3.708643913269043\n",
      "\t Params: tensor([  4.8411, -14.3238])\n",
      "\t Grad: tensor([-0.0896,  0.5073])\n",
      "Epoch 1042, Loss 3.7059905529022217\n",
      "\t Params: tensor([  4.8420, -14.3288])\n",
      "\t Grad: tensor([-0.0895,  0.5064])\n",
      "Epoch 1043, Loss 3.7033514976501465\n",
      "\t Params: tensor([  4.8429, -14.3339])\n",
      "\t Grad: tensor([-0.0893,  0.5055])\n",
      "Epoch 1044, Loss 3.7007157802581787\n",
      "\t Params: tensor([  4.8438, -14.3390])\n",
      "\t Grad: tensor([-0.0892,  0.5047])\n",
      "Epoch 1045, Loss 3.6980912685394287\n",
      "\t Params: tensor([  4.8447, -14.3440])\n",
      "\t Grad: tensor([-0.0890,  0.5038])\n",
      "Epoch 1046, Loss 3.6954762935638428\n",
      "\t Params: tensor([  4.8456, -14.3490])\n",
      "\t Grad: tensor([-0.0888,  0.5030])\n",
      "Epoch 1047, Loss 3.692868947982788\n",
      "\t Params: tensor([  4.8465, -14.3540])\n",
      "\t Grad: tensor([-0.0887,  0.5021])\n",
      "Epoch 1048, Loss 3.690272569656372\n",
      "\t Params: tensor([  4.8473, -14.3591])\n",
      "\t Grad: tensor([-0.0886,  0.5013])\n",
      "Epoch 1049, Loss 3.687682867050171\n",
      "\t Params: tensor([  4.8482, -14.3641])\n",
      "\t Grad: tensor([-0.0884,  0.5004])\n",
      "Epoch 1050, Loss 3.6851041316986084\n",
      "\t Params: tensor([  4.8491, -14.3691])\n",
      "\t Grad: tensor([-0.0882,  0.4996])\n",
      "Epoch 1051, Loss 3.6825318336486816\n",
      "\t Params: tensor([  4.8500, -14.3740])\n",
      "\t Grad: tensor([-0.0881,  0.4987])\n",
      "Epoch 1052, Loss 3.67996883392334\n",
      "\t Params: tensor([  4.8509, -14.3790])\n",
      "\t Grad: tensor([-0.0879,  0.4979])\n",
      "Epoch 1053, Loss 3.677417039871216\n",
      "\t Params: tensor([  4.8518, -14.3840])\n",
      "\t Grad: tensor([-0.0878,  0.4970])\n",
      "Epoch 1054, Loss 3.6748709678649902\n",
      "\t Params: tensor([  4.8526, -14.3889])\n",
      "\t Grad: tensor([-0.0877,  0.4962])\n",
      "Epoch 1055, Loss 3.672334909439087\n",
      "\t Params: tensor([  4.8535, -14.3939])\n",
      "\t Grad: tensor([-0.0875,  0.4953])\n",
      "Epoch 1056, Loss 3.669804334640503\n",
      "\t Params: tensor([  4.8544, -14.3988])\n",
      "\t Grad: tensor([-0.0873,  0.4945])\n",
      "Epoch 1057, Loss 3.6672868728637695\n",
      "\t Params: tensor([  4.8552, -14.4038])\n",
      "\t Grad: tensor([-0.0872,  0.4936])\n",
      "Epoch 1058, Loss 3.6647748947143555\n",
      "\t Params: tensor([  4.8561, -14.4087])\n",
      "\t Grad: tensor([-0.0870,  0.4928])\n",
      "Epoch 1059, Loss 3.6622731685638428\n",
      "\t Params: tensor([  4.8570, -14.4136])\n",
      "\t Grad: tensor([-0.0869,  0.4920])\n",
      "Epoch 1060, Loss 3.659777879714966\n",
      "\t Params: tensor([  4.8579, -14.4185])\n",
      "\t Grad: tensor([-0.0868,  0.4911])\n",
      "Epoch 1061, Loss 3.657294988632202\n",
      "\t Params: tensor([  4.8587, -14.4234])\n",
      "\t Grad: tensor([-0.0866,  0.4903])\n",
      "Epoch 1062, Loss 3.654815912246704\n",
      "\t Params: tensor([  4.8596, -14.4283])\n",
      "\t Grad: tensor([-0.0865,  0.4895])\n",
      "Epoch 1063, Loss 3.6523489952087402\n",
      "\t Params: tensor([  4.8604, -14.4332])\n",
      "\t Grad: tensor([-0.0863,  0.4886])\n",
      "Epoch 1064, Loss 3.6498892307281494\n",
      "\t Params: tensor([  4.8613, -14.4381])\n",
      "\t Grad: tensor([-0.0862,  0.4878])\n",
      "Epoch 1065, Loss 3.647437334060669\n",
      "\t Params: tensor([  4.8622, -14.4430])\n",
      "\t Grad: tensor([-0.0860,  0.4870])\n",
      "Epoch 1066, Loss 3.644991397857666\n",
      "\t Params: tensor([  4.8630, -14.4478])\n",
      "\t Grad: tensor([-0.0859,  0.4862])\n",
      "Epoch 1067, Loss 3.642559289932251\n",
      "\t Params: tensor([  4.8639, -14.4527])\n",
      "\t Grad: tensor([-0.0857,  0.4853])\n",
      "Epoch 1068, Loss 3.640131950378418\n",
      "\t Params: tensor([  4.8647, -14.4575])\n",
      "\t Grad: tensor([-0.0856,  0.4845])\n",
      "Epoch 1069, Loss 3.6377112865448\n",
      "\t Params: tensor([  4.8656, -14.4624])\n",
      "\t Grad: tensor([-0.0854,  0.4837])\n",
      "Epoch 1070, Loss 3.6353020668029785\n",
      "\t Params: tensor([  4.8665, -14.4672])\n",
      "\t Grad: tensor([-0.0853,  0.4829])\n",
      "Epoch 1071, Loss 3.6329023838043213\n",
      "\t Params: tensor([  4.8673, -14.4720])\n",
      "\t Grad: tensor([-0.0851,  0.4820])\n",
      "Epoch 1072, Loss 3.6305084228515625\n",
      "\t Params: tensor([  4.8682, -14.4768])\n",
      "\t Grad: tensor([-0.0850,  0.4812])\n",
      "Epoch 1073, Loss 3.6281189918518066\n",
      "\t Params: tensor([  4.8690, -14.4816])\n",
      "\t Grad: tensor([-0.0849,  0.4804])\n",
      "Epoch 1074, Loss 3.6257410049438477\n",
      "\t Params: tensor([  4.8698, -14.4864])\n",
      "\t Grad: tensor([-0.0847,  0.4796])\n",
      "Epoch 1075, Loss 3.623373508453369\n",
      "\t Params: tensor([  4.8707, -14.4912])\n",
      "\t Grad: tensor([-0.0846,  0.4788])\n",
      "Epoch 1076, Loss 3.6210100650787354\n",
      "\t Params: tensor([  4.8715, -14.4960])\n",
      "\t Grad: tensor([-0.0844,  0.4780])\n",
      "Epoch 1077, Loss 3.6186585426330566\n",
      "\t Params: tensor([  4.8724, -14.5008])\n",
      "\t Grad: tensor([-0.0843,  0.4771])\n",
      "Epoch 1078, Loss 3.6163108348846436\n",
      "\t Params: tensor([  4.8732, -14.5055])\n",
      "\t Grad: tensor([-0.0841,  0.4763])\n",
      "Epoch 1079, Loss 3.6139731407165527\n",
      "\t Params: tensor([  4.8741, -14.5103])\n",
      "\t Grad: tensor([-0.0840,  0.4755])\n",
      "Epoch 1080, Loss 3.611642599105835\n",
      "\t Params: tensor([  4.8749, -14.5150])\n",
      "\t Grad: tensor([-0.0839,  0.4747])\n",
      "Epoch 1081, Loss 3.609320640563965\n",
      "\t Params: tensor([  4.8757, -14.5198])\n",
      "\t Grad: tensor([-0.0837,  0.4739])\n",
      "Epoch 1082, Loss 3.6070075035095215\n",
      "\t Params: tensor([  4.8766, -14.5245])\n",
      "\t Grad: tensor([-0.0836,  0.4731])\n",
      "Epoch 1083, Loss 3.604701042175293\n",
      "\t Params: tensor([  4.8774, -14.5292])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad: tensor([-0.0834,  0.4723])\n",
      "Epoch 1084, Loss 3.602403402328491\n",
      "\t Params: tensor([  4.8782, -14.5339])\n",
      "\t Grad: tensor([-0.0833,  0.4715])\n",
      "Epoch 1085, Loss 3.6001136302948\n",
      "\t Params: tensor([  4.8791, -14.5387])\n",
      "\t Grad: tensor([-0.0832,  0.4707])\n",
      "Epoch 1086, Loss 3.5978310108184814\n",
      "\t Params: tensor([  4.8799, -14.5434])\n",
      "\t Grad: tensor([-0.0830,  0.4699])\n",
      "Epoch 1087, Loss 3.595552921295166\n",
      "\t Params: tensor([  4.8807, -14.5480])\n",
      "\t Grad: tensor([-0.0829,  0.4691])\n",
      "Epoch 1088, Loss 3.593287467956543\n",
      "\t Params: tensor([  4.8816, -14.5527])\n",
      "\t Grad: tensor([-0.0827,  0.4683])\n",
      "Epoch 1089, Loss 3.5910298824310303\n",
      "\t Params: tensor([  4.8824, -14.5574])\n",
      "\t Grad: tensor([-0.0826,  0.4675])\n",
      "Epoch 1090, Loss 3.5887763500213623\n",
      "\t Params: tensor([  4.8832, -14.5621])\n",
      "\t Grad: tensor([-0.0824,  0.4667])\n",
      "Epoch 1091, Loss 3.586533546447754\n",
      "\t Params: tensor([  4.8840, -14.5667])\n",
      "\t Grad: tensor([-0.0823,  0.4659])\n",
      "Epoch 1092, Loss 3.584294319152832\n",
      "\t Params: tensor([  4.8849, -14.5714])\n",
      "\t Grad: tensor([-0.0822,  0.4651])\n",
      "Epoch 1093, Loss 3.5820672512054443\n",
      "\t Params: tensor([  4.8857, -14.5760])\n",
      "\t Grad: tensor([-0.0820,  0.4643])\n",
      "Epoch 1094, Loss 3.579845428466797\n",
      "\t Params: tensor([  4.8865, -14.5807])\n",
      "\t Grad: tensor([-0.0819,  0.4636])\n",
      "Epoch 1095, Loss 3.5776307582855225\n",
      "\t Params: tensor([  4.8873, -14.5853])\n",
      "\t Grad: tensor([-0.0818,  0.4628])\n",
      "Epoch 1096, Loss 3.5754239559173584\n",
      "\t Params: tensor([  4.8881, -14.5899])\n",
      "\t Grad: tensor([-0.0816,  0.4620])\n",
      "Epoch 1097, Loss 3.573225498199463\n",
      "\t Params: tensor([  4.8889, -14.5945])\n",
      "\t Grad: tensor([-0.0815,  0.4612])\n",
      "Epoch 1098, Loss 3.5710349082946777\n",
      "\t Params: tensor([  4.8898, -14.5991])\n",
      "\t Grad: tensor([-0.0813,  0.4604])\n",
      "Epoch 1099, Loss 3.568847894668579\n",
      "\t Params: tensor([  4.8906, -14.6037])\n",
      "\t Grad: tensor([-0.0812,  0.4596])\n",
      "Epoch 1100, Loss 3.5666725635528564\n",
      "\t Params: tensor([  4.8914, -14.6083])\n",
      "\t Grad: tensor([-0.0810,  0.4588])\n",
      "Epoch 1101, Loss 3.5645058155059814\n",
      "\t Params: tensor([  4.8922, -14.6129])\n",
      "\t Grad: tensor([-0.0809,  0.4581])\n",
      "Epoch 1102, Loss 3.56234073638916\n",
      "\t Params: tensor([  4.8930, -14.6175])\n",
      "\t Grad: tensor([-0.0808,  0.4573])\n",
      "Epoch 1103, Loss 3.5601847171783447\n",
      "\t Params: tensor([  4.8938, -14.6220])\n",
      "\t Grad: tensor([-0.0806,  0.4565])\n",
      "Epoch 1104, Loss 3.558039903640747\n",
      "\t Params: tensor([  4.8946, -14.6266])\n",
      "\t Grad: tensor([-0.0805,  0.4557])\n",
      "Epoch 1105, Loss 3.555901288986206\n",
      "\t Params: tensor([  4.8954, -14.6311])\n",
      "\t Grad: tensor([-0.0804,  0.4550])\n",
      "Epoch 1106, Loss 3.553767204284668\n",
      "\t Params: tensor([  4.8962, -14.6357])\n",
      "\t Grad: tensor([-0.0802,  0.4542])\n",
      "Epoch 1107, Loss 3.551640510559082\n",
      "\t Params: tensor([  4.8970, -14.6402])\n",
      "\t Grad: tensor([-0.0801,  0.4534])\n",
      "Epoch 1108, Loss 3.5495238304138184\n",
      "\t Params: tensor([  4.8978, -14.6447])\n",
      "\t Grad: tensor([-0.0799,  0.4527])\n",
      "Epoch 1109, Loss 3.5474114418029785\n",
      "\t Params: tensor([  4.8986, -14.6493])\n",
      "\t Grad: tensor([-0.0798,  0.4519])\n",
      "Epoch 1110, Loss 3.5453085899353027\n",
      "\t Params: tensor([  4.8994, -14.6538])\n",
      "\t Grad: tensor([-0.0797,  0.4511])\n",
      "Epoch 1111, Loss 3.543210506439209\n",
      "\t Params: tensor([  4.9002, -14.6583])\n",
      "\t Grad: tensor([-0.0796,  0.4503])\n",
      "Epoch 1112, Loss 3.541123867034912\n",
      "\t Params: tensor([  4.9010, -14.6628])\n",
      "\t Grad: tensor([-0.0794,  0.4496])\n",
      "Epoch 1113, Loss 3.53904128074646\n",
      "\t Params: tensor([  4.9018, -14.6673])\n",
      "\t Grad: tensor([-0.0793,  0.4488])\n",
      "Epoch 1114, Loss 3.536966562271118\n",
      "\t Params: tensor([  4.9026, -14.6717])\n",
      "\t Grad: tensor([-0.0791,  0.4481])\n",
      "Epoch 1115, Loss 3.5348961353302\n",
      "\t Params: tensor([  4.9034, -14.6762])\n",
      "\t Grad: tensor([-0.0790,  0.4473])\n",
      "Epoch 1116, Loss 3.532834529876709\n",
      "\t Params: tensor([  4.9042, -14.6807])\n",
      "\t Grad: tensor([-0.0789,  0.4465])\n",
      "Epoch 1117, Loss 3.530780553817749\n",
      "\t Params: tensor([  4.9049, -14.6851])\n",
      "\t Grad: tensor([-0.0787,  0.4458])\n",
      "Epoch 1118, Loss 3.528733730316162\n",
      "\t Params: tensor([  4.9057, -14.6896])\n",
      "\t Grad: tensor([-0.0786,  0.4450])\n",
      "Epoch 1119, Loss 3.52669358253479\n",
      "\t Params: tensor([  4.9065, -14.6940])\n",
      "\t Grad: tensor([-0.0785,  0.4443])\n",
      "Epoch 1120, Loss 3.5246617794036865\n",
      "\t Params: tensor([  4.9073, -14.6985])\n",
      "\t Grad: tensor([-0.0784,  0.4435])\n",
      "Epoch 1121, Loss 3.522632598876953\n",
      "\t Params: tensor([  4.9081, -14.7029])\n",
      "\t Grad: tensor([-0.0782,  0.4428])\n",
      "Epoch 1122, Loss 3.5206143856048584\n",
      "\t Params: tensor([  4.9089, -14.7073])\n",
      "\t Grad: tensor([-0.0781,  0.4420])\n",
      "Epoch 1123, Loss 3.518601179122925\n",
      "\t Params: tensor([  4.9096, -14.7117])\n",
      "\t Grad: tensor([-0.0779,  0.4413])\n",
      "Epoch 1124, Loss 3.516594171524048\n",
      "\t Params: tensor([  4.9104, -14.7161])\n",
      "\t Grad: tensor([-0.0778,  0.4405])\n",
      "Epoch 1125, Loss 3.5145936012268066\n",
      "\t Params: tensor([  4.9112, -14.7205])\n",
      "\t Grad: tensor([-0.0777,  0.4398])\n",
      "Epoch 1126, Loss 3.5126020908355713\n",
      "\t Params: tensor([  4.9120, -14.7249])\n",
      "\t Grad: tensor([-0.0775,  0.4390])\n",
      "Epoch 1127, Loss 3.5106186866760254\n",
      "\t Params: tensor([  4.9128, -14.7293])\n",
      "\t Grad: tensor([-0.0774,  0.4383])\n",
      "Epoch 1128, Loss 3.508636713027954\n",
      "\t Params: tensor([  4.9135, -14.7337])\n",
      "\t Grad: tensor([-0.0773,  0.4375])\n",
      "Epoch 1129, Loss 3.5066652297973633\n",
      "\t Params: tensor([  4.9143, -14.7380])\n",
      "\t Grad: tensor([-0.0772,  0.4368])\n",
      "Epoch 1130, Loss 3.5046989917755127\n",
      "\t Params: tensor([  4.9151, -14.7424])\n",
      "\t Grad: tensor([-0.0770,  0.4360])\n",
      "Epoch 1131, Loss 3.5027406215667725\n",
      "\t Params: tensor([  4.9158, -14.7467])\n",
      "\t Grad: tensor([-0.0769,  0.4353])\n",
      "Epoch 1132, Loss 3.500788927078247\n",
      "\t Params: tensor([  4.9166, -14.7511])\n",
      "\t Grad: tensor([-0.0767,  0.4346])\n",
      "Epoch 1133, Loss 3.498843193054199\n",
      "\t Params: tensor([  4.9174, -14.7554])\n",
      "\t Grad: tensor([-0.0766,  0.4338])\n",
      "Epoch 1134, Loss 3.4969053268432617\n",
      "\t Params: tensor([  4.9181, -14.7598])\n",
      "\t Grad: tensor([-0.0765,  0.4331])\n",
      "Epoch 1135, Loss 3.494971513748169\n",
      "\t Params: tensor([  4.9189, -14.7641])\n",
      "\t Grad: tensor([-0.0764,  0.4323])\n",
      "Epoch 1136, Loss 3.4930458068847656\n",
      "\t Params: tensor([  4.9197, -14.7684])\n",
      "\t Grad: tensor([-0.0763,  0.4316])\n",
      "Epoch 1137, Loss 3.4911270141601562\n",
      "\t Params: tensor([  4.9204, -14.7727])\n",
      "\t Grad: tensor([-0.0761,  0.4309])\n",
      "Epoch 1138, Loss 3.4892144203186035\n",
      "\t Params: tensor([  4.9212, -14.7770])\n",
      "\t Grad: tensor([-0.0760,  0.4301])\n",
      "Epoch 1139, Loss 3.487307548522949\n",
      "\t Params: tensor([  4.9219, -14.7813])\n",
      "\t Grad: tensor([-0.0759,  0.4294])\n",
      "Epoch 1140, Loss 3.485409736633301\n",
      "\t Params: tensor([  4.9227, -14.7856])\n",
      "\t Grad: tensor([-0.0757,  0.4287])\n",
      "Epoch 1141, Loss 3.4835150241851807\n",
      "\t Params: tensor([  4.9235, -14.7899])\n",
      "\t Grad: tensor([-0.0756,  0.4280])\n",
      "Epoch 1142, Loss 3.4816269874572754\n",
      "\t Params: tensor([  4.9242, -14.7941])\n",
      "\t Grad: tensor([-0.0755,  0.4272])\n",
      "Epoch 1143, Loss 3.479745626449585\n",
      "\t Params: tensor([  4.9250, -14.7984])\n",
      "\t Grad: tensor([-0.0753,  0.4265])\n",
      "Epoch 1144, Loss 3.477871894836426\n",
      "\t Params: tensor([  4.9257, -14.8027])\n",
      "\t Grad: tensor([-0.0752,  0.4258])\n",
      "Epoch 1145, Loss 3.4760048389434814\n",
      "\t Params: tensor([  4.9265, -14.8069])\n",
      "\t Grad: tensor([-0.0751,  0.4250])\n",
      "Epoch 1146, Loss 3.4741432666778564\n",
      "\t Params: tensor([  4.9272, -14.8112])\n",
      "\t Grad: tensor([-0.0750,  0.4243])\n",
      "Epoch 1147, Loss 3.4722883701324463\n",
      "\t Params: tensor([  4.9280, -14.8154])\n",
      "\t Grad: tensor([-0.0748,  0.4236])\n",
      "Epoch 1148, Loss 3.4704408645629883\n",
      "\t Params: tensor([  4.9287, -14.8196])\n",
      "\t Grad: tensor([-0.0747,  0.4229])\n",
      "Epoch 1149, Loss 3.468597173690796\n",
      "\t Params: tensor([  4.9295, -14.8238])\n",
      "\t Grad: tensor([-0.0746,  0.4222])\n",
      "Epoch 1150, Loss 3.466761827468872\n",
      "\t Params: tensor([  4.9302, -14.8281])\n",
      "\t Grad: tensor([-0.0745,  0.4215])\n",
      "Epoch 1151, Loss 3.4649300575256348\n",
      "\t Params: tensor([  4.9309, -14.8323])\n",
      "\t Grad: tensor([-0.0743,  0.4207])\n",
      "Epoch 1152, Loss 3.4631049633026123\n",
      "\t Params: tensor([  4.9317, -14.8365])\n",
      "\t Grad: tensor([-0.0742,  0.4200])\n",
      "Epoch 1153, Loss 3.461289882659912\n",
      "\t Params: tensor([  4.9324, -14.8407])\n",
      "\t Grad: tensor([-0.0741,  0.4193])\n",
      "Epoch 1154, Loss 3.459477424621582\n",
      "\t Params: tensor([  4.9332, -14.8448])\n",
      "\t Grad: tensor([-0.0739,  0.4186])\n",
      "Epoch 1155, Loss 3.4576711654663086\n",
      "\t Params: tensor([  4.9339, -14.8490])\n",
      "\t Grad: tensor([-0.0738,  0.4179])\n",
      "Epoch 1156, Loss 3.4558732509613037\n",
      "\t Params: tensor([  4.9346, -14.8532])\n",
      "\t Grad: tensor([-0.0737,  0.4172])\n",
      "Epoch 1157, Loss 3.45408034324646\n",
      "\t Params: tensor([  4.9354, -14.8574])\n",
      "\t Grad: tensor([-0.0736,  0.4165])\n",
      "Epoch 1158, Loss 3.4522926807403564\n",
      "\t Params: tensor([  4.9361, -14.8615])\n",
      "\t Grad: tensor([-0.0734,  0.4158])\n",
      "Epoch 1159, Loss 3.450512647628784\n",
      "\t Params: tensor([  4.9368, -14.8657])\n",
      "\t Grad: tensor([-0.0733,  0.4151])\n",
      "Epoch 1160, Loss 3.4487359523773193\n",
      "\t Params: tensor([  4.9376, -14.8698])\n",
      "\t Grad: tensor([-0.0732,  0.4143])\n",
      "Epoch 1161, Loss 3.446967840194702\n",
      "\t Params: tensor([  4.9383, -14.8739])\n",
      "\t Grad: tensor([-0.0731,  0.4136])\n",
      "Epoch 1162, Loss 3.445202589035034\n",
      "\t Params: tensor([  4.9390, -14.8781])\n",
      "\t Grad: tensor([-0.0730,  0.4129])\n",
      "Epoch 1163, Loss 3.4434492588043213\n",
      "\t Params: tensor([  4.9398, -14.8822])\n",
      "\t Grad: tensor([-0.0728,  0.4122])\n",
      "Epoch 1164, Loss 3.441696882247925\n",
      "\t Params: tensor([  4.9405, -14.8863])\n",
      "\t Grad: tensor([-0.0727,  0.4115])\n",
      "Epoch 1165, Loss 3.4399521350860596\n",
      "\t Params: tensor([  4.9412, -14.8904])\n",
      "\t Grad: tensor([-0.0726,  0.4108])\n",
      "Epoch 1166, Loss 3.4382104873657227\n",
      "\t Params: tensor([  4.9419, -14.8945])\n",
      "\t Grad: tensor([-0.0725,  0.4101])\n",
      "Epoch 1167, Loss 3.436479091644287\n",
      "\t Params: tensor([  4.9427, -14.8986])\n",
      "\t Grad: tensor([-0.0723,  0.4094])\n",
      "Epoch 1168, Loss 3.434753179550171\n",
      "\t Params: tensor([  4.9434, -14.9027])\n",
      "\t Grad: tensor([-0.0722,  0.4087])\n",
      "Epoch 1169, Loss 3.433029890060425\n",
      "\t Params: tensor([  4.9441, -14.9068])\n",
      "\t Grad: tensor([-0.0721,  0.4081])\n",
      "Epoch 1170, Loss 3.431313991546631\n",
      "\t Params: tensor([  4.9448, -14.9109])\n",
      "\t Grad: tensor([-0.0720,  0.4074])\n",
      "Epoch 1171, Loss 3.429607391357422\n",
      "\t Params: tensor([  4.9455, -14.9149])\n",
      "\t Grad: tensor([-0.0719,  0.4067])\n",
      "Epoch 1172, Loss 3.427902936935425\n",
      "\t Params: tensor([  4.9463, -14.9190])\n",
      "\t Grad: tensor([-0.0717,  0.4060])\n",
      "Epoch 1173, Loss 3.426203966140747\n",
      "\t Params: tensor([  4.9470, -14.9230])\n",
      "\t Grad: tensor([-0.0716,  0.4053])\n",
      "Epoch 1174, Loss 3.4245095252990723\n",
      "\t Params: tensor([  4.9477, -14.9271])\n",
      "\t Grad: tensor([-0.0715,  0.4046])\n",
      "Epoch 1175, Loss 3.422823190689087\n",
      "\t Params: tensor([  4.9484, -14.9311])\n",
      "\t Grad: tensor([-0.0714,  0.4039])\n",
      "Epoch 1176, Loss 3.4211440086364746\n",
      "\t Params: tensor([  4.9491, -14.9352])\n",
      "\t Grad: tensor([-0.0712,  0.4032])\n",
      "Epoch 1177, Loss 3.4194676876068115\n",
      "\t Params: tensor([  4.9498, -14.9392])\n",
      "\t Grad: tensor([-0.0711,  0.4025])\n",
      "Epoch 1178, Loss 3.417797803878784\n",
      "\t Params: tensor([  4.9505, -14.9432])\n",
      "\t Grad: tensor([-0.0710,  0.4019])\n",
      "Epoch 1179, Loss 3.4161338806152344\n",
      "\t Params: tensor([  4.9512, -14.9472])\n",
      "\t Grad: tensor([-0.0709,  0.4012])\n",
      "Epoch 1180, Loss 3.414476156234741\n",
      "\t Params: tensor([  4.9520, -14.9512])\n",
      "\t Grad: tensor([-0.0708,  0.4005])\n",
      "Epoch 1181, Loss 3.4128243923187256\n",
      "\t Params: tensor([  4.9527, -14.9552])\n",
      "\t Grad: tensor([-0.0706,  0.3998])\n",
      "Epoch 1182, Loss 3.4111764430999756\n",
      "\t Params: tensor([  4.9534, -14.9592])\n",
      "\t Grad: tensor([-0.0705,  0.3991])\n",
      "Epoch 1183, Loss 3.409534454345703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params: tensor([  4.9541, -14.9632])\n",
      "\t Grad: tensor([-0.0704,  0.3985])\n",
      "Epoch 1184, Loss 3.4078996181488037\n",
      "\t Params: tensor([  4.9548, -14.9672])\n",
      "\t Grad: tensor([-0.0703,  0.3978])\n",
      "Epoch 1185, Loss 3.40627121925354\n",
      "\t Params: tensor([  4.9555, -14.9711])\n",
      "\t Grad: tensor([-0.0701,  0.3971])\n",
      "Epoch 1186, Loss 3.4046454429626465\n",
      "\t Params: tensor([  4.9562, -14.9751])\n",
      "\t Grad: tensor([-0.0700,  0.3964])\n",
      "Epoch 1187, Loss 3.403024435043335\n",
      "\t Params: tensor([  4.9569, -14.9791])\n",
      "\t Grad: tensor([-0.0699,  0.3958])\n",
      "Epoch 1188, Loss 3.4014132022857666\n",
      "\t Params: tensor([  4.9576, -14.9830])\n",
      "\t Grad: tensor([-0.0698,  0.3951])\n",
      "Epoch 1189, Loss 3.3998024463653564\n",
      "\t Params: tensor([  4.9583, -14.9870])\n",
      "\t Grad: tensor([-0.0697,  0.3944])\n",
      "Epoch 1190, Loss 3.3981995582580566\n",
      "\t Params: tensor([  4.9590, -14.9909])\n",
      "\t Grad: tensor([-0.0696,  0.3937])\n",
      "Epoch 1191, Loss 3.3966023921966553\n",
      "\t Params: tensor([  4.9597, -14.9948])\n",
      "\t Grad: tensor([-0.0694,  0.3931])\n",
      "Epoch 1192, Loss 3.3950111865997314\n",
      "\t Params: tensor([  4.9604, -14.9988])\n",
      "\t Grad: tensor([-0.0693,  0.3924])\n",
      "Epoch 1193, Loss 3.3934249877929688\n",
      "\t Params: tensor([  4.9610, -15.0027])\n",
      "\t Grad: tensor([-0.0692,  0.3917])\n",
      "Epoch 1194, Loss 3.3918442726135254\n",
      "\t Params: tensor([  4.9617, -15.0066])\n",
      "\t Grad: tensor([-0.0691,  0.3911])\n",
      "Epoch 1195, Loss 3.3902664184570312\n",
      "\t Params: tensor([  4.9624, -15.0105])\n",
      "\t Grad: tensor([-0.0690,  0.3904])\n",
      "Epoch 1196, Loss 3.3886969089508057\n",
      "\t Params: tensor([  4.9631, -15.0144])\n",
      "\t Grad: tensor([-0.0689,  0.3897])\n",
      "Epoch 1197, Loss 3.387131452560425\n",
      "\t Params: tensor([  4.9638, -15.0183])\n",
      "\t Grad: tensor([-0.0687,  0.3891])\n",
      "Epoch 1198, Loss 3.385570764541626\n",
      "\t Params: tensor([  4.9645, -15.0222])\n",
      "\t Grad: tensor([-0.0686,  0.3884])\n",
      "Epoch 1199, Loss 3.3840177059173584\n",
      "\t Params: tensor([  4.9652, -15.0260])\n",
      "\t Grad: tensor([-0.0685,  0.3878])\n",
      "Epoch 1200, Loss 3.3824667930603027\n",
      "\t Params: tensor([  4.9659, -15.0299])\n",
      "\t Grad: tensor([-0.0684,  0.3871])\n",
      "Epoch 1201, Loss 3.380925178527832\n",
      "\t Params: tensor([  4.9665, -15.0338])\n",
      "\t Grad: tensor([-0.0683,  0.3864])\n",
      "Epoch 1202, Loss 3.379385471343994\n",
      "\t Params: tensor([  4.9672, -15.0376])\n",
      "\t Grad: tensor([-0.0681,  0.3858])\n",
      "Epoch 1203, Loss 3.3778510093688965\n",
      "\t Params: tensor([  4.9679, -15.0415])\n",
      "\t Grad: tensor([-0.0680,  0.3851])\n",
      "Epoch 1204, Loss 3.3763234615325928\n",
      "\t Params: tensor([  4.9686, -15.0453])\n",
      "\t Grad: tensor([-0.0679,  0.3845])\n",
      "Epoch 1205, Loss 3.374800205230713\n",
      "\t Params: tensor([  4.9693, -15.0492])\n",
      "\t Grad: tensor([-0.0678,  0.3838])\n",
      "Epoch 1206, Loss 3.373284101486206\n",
      "\t Params: tensor([  4.9699, -15.0530])\n",
      "\t Grad: tensor([-0.0677,  0.3832])\n",
      "Epoch 1207, Loss 3.3717689514160156\n",
      "\t Params: tensor([  4.9706, -15.0568])\n",
      "\t Grad: tensor([-0.0676,  0.3825])\n",
      "Epoch 1208, Loss 3.3702611923217773\n",
      "\t Params: tensor([  4.9713, -15.0606])\n",
      "\t Grad: tensor([-0.0675,  0.3819])\n",
      "Epoch 1209, Loss 3.368760108947754\n",
      "\t Params: tensor([  4.9720, -15.0645])\n",
      "\t Grad: tensor([-0.0673,  0.3812])\n",
      "Epoch 1210, Loss 3.3672616481781006\n",
      "\t Params: tensor([  4.9726, -15.0683])\n",
      "\t Grad: tensor([-0.0672,  0.3806])\n",
      "Epoch 1211, Loss 3.3657712936401367\n",
      "\t Params: tensor([  4.9733, -15.0721])\n",
      "\t Grad: tensor([-0.0671,  0.3799])\n",
      "Epoch 1212, Loss 3.3642821311950684\n",
      "\t Params: tensor([  4.9740, -15.0758])\n",
      "\t Grad: tensor([-0.0670,  0.3793])\n",
      "Epoch 1213, Loss 3.362799644470215\n",
      "\t Params: tensor([  4.9746, -15.0796])\n",
      "\t Grad: tensor([-0.0669,  0.3786])\n",
      "Epoch 1214, Loss 3.3613243103027344\n",
      "\t Params: tensor([  4.9753, -15.0834])\n",
      "\t Grad: tensor([-0.0668,  0.3780])\n",
      "Epoch 1215, Loss 3.3598504066467285\n",
      "\t Params: tensor([  4.9760, -15.0872])\n",
      "\t Grad: tensor([-0.0667,  0.3774])\n",
      "Epoch 1216, Loss 3.358383893966675\n",
      "\t Params: tensor([  4.9766, -15.0910])\n",
      "\t Grad: tensor([-0.0665,  0.3767])\n",
      "Epoch 1217, Loss 3.3569211959838867\n",
      "\t Params: tensor([  4.9773, -15.0947])\n",
      "\t Grad: tensor([-0.0664,  0.3761])\n",
      "Epoch 1218, Loss 3.355463981628418\n",
      "\t Params: tensor([  4.9780, -15.0985])\n",
      "\t Grad: tensor([-0.0663,  0.3754])\n",
      "Epoch 1219, Loss 3.3540124893188477\n",
      "\t Params: tensor([  4.9786, -15.1022])\n",
      "\t Grad: tensor([-0.0662,  0.3748])\n",
      "Epoch 1220, Loss 3.3525640964508057\n",
      "\t Params: tensor([  4.9793, -15.1060])\n",
      "\t Grad: tensor([-0.0661,  0.3742])\n",
      "Epoch 1221, Loss 3.3511223793029785\n",
      "\t Params: tensor([  4.9799, -15.1097])\n",
      "\t Grad: tensor([-0.0660,  0.3735])\n",
      "Epoch 1222, Loss 3.349684715270996\n",
      "\t Params: tensor([  4.9806, -15.1134])\n",
      "\t Grad: tensor([-0.0659,  0.3729])\n",
      "Epoch 1223, Loss 3.3482511043548584\n",
      "\t Params: tensor([  4.9813, -15.1171])\n",
      "\t Grad: tensor([-0.0657,  0.3723])\n",
      "Epoch 1224, Loss 3.3468244075775146\n",
      "\t Params: tensor([  4.9819, -15.1209])\n",
      "\t Grad: tensor([-0.0656,  0.3716])\n",
      "Epoch 1225, Loss 3.345402717590332\n",
      "\t Params: tensor([  4.9826, -15.1246])\n",
      "\t Grad: tensor([-0.0655,  0.3710])\n",
      "Epoch 1226, Loss 3.3439817428588867\n",
      "\t Params: tensor([  4.9832, -15.1283])\n",
      "\t Grad: tensor([-0.0654,  0.3704])\n",
      "Epoch 1227, Loss 3.3425705432891846\n",
      "\t Params: tensor([  4.9839, -15.1320])\n",
      "\t Grad: tensor([-0.0653,  0.3697])\n",
      "Epoch 1228, Loss 3.3411600589752197\n",
      "\t Params: tensor([  4.9845, -15.1357])\n",
      "\t Grad: tensor([-0.0652,  0.3691])\n",
      "Epoch 1229, Loss 3.3397583961486816\n",
      "\t Params: tensor([  4.9852, -15.1393])\n",
      "\t Grad: tensor([-0.0651,  0.3685])\n",
      "Epoch 1230, Loss 3.3383591175079346\n",
      "\t Params: tensor([  4.9858, -15.1430])\n",
      "\t Grad: tensor([-0.0650,  0.3679])\n",
      "Epoch 1231, Loss 3.3369646072387695\n",
      "\t Params: tensor([  4.9865, -15.1467])\n",
      "\t Grad: tensor([-0.0649,  0.3672])\n",
      "Epoch 1232, Loss 3.3355767726898193\n",
      "\t Params: tensor([  4.9871, -15.1504])\n",
      "\t Grad: tensor([-0.0648,  0.3666])\n",
      "Epoch 1233, Loss 3.3341917991638184\n",
      "\t Params: tensor([  4.9878, -15.1540])\n",
      "\t Grad: tensor([-0.0646,  0.3660])\n",
      "Epoch 1234, Loss 3.332810878753662\n",
      "\t Params: tensor([  4.9884, -15.1577])\n",
      "\t Grad: tensor([-0.0645,  0.3654])\n",
      "Epoch 1235, Loss 3.3314359188079834\n",
      "\t Params: tensor([  4.9891, -15.1613])\n",
      "\t Grad: tensor([-0.0644,  0.3647])\n",
      "Epoch 1236, Loss 3.3300652503967285\n",
      "\t Params: tensor([  4.9897, -15.1650])\n",
      "\t Grad: tensor([-0.0643,  0.3641])\n",
      "Epoch 1237, Loss 3.3286988735198975\n",
      "\t Params: tensor([  4.9904, -15.1686])\n",
      "\t Grad: tensor([-0.0642,  0.3635])\n",
      "Epoch 1238, Loss 3.327338933944702\n",
      "\t Params: tensor([  4.9910, -15.1722])\n",
      "\t Grad: tensor([-0.0641,  0.3629])\n",
      "Epoch 1239, Loss 3.325979709625244\n",
      "\t Params: tensor([  4.9916, -15.1759])\n",
      "\t Grad: tensor([-0.0640,  0.3623])\n",
      "Epoch 1240, Loss 3.324627637863159\n",
      "\t Params: tensor([  4.9923, -15.1795])\n",
      "\t Grad: tensor([-0.0639,  0.3617])\n",
      "Epoch 1241, Loss 3.32327938079834\n",
      "\t Params: tensor([  4.9929, -15.1831])\n",
      "\t Grad: tensor([-0.0638,  0.3610])\n",
      "Epoch 1242, Loss 3.321934700012207\n",
      "\t Params: tensor([  4.9936, -15.1867])\n",
      "\t Grad: tensor([-0.0637,  0.3604])\n",
      "Epoch 1243, Loss 3.3205995559692383\n",
      "\t Params: tensor([  4.9942, -15.1903])\n",
      "\t Grad: tensor([-0.0636,  0.3598])\n",
      "Epoch 1244, Loss 3.3192641735076904\n",
      "\t Params: tensor([  4.9948, -15.1939])\n",
      "\t Grad: tensor([-0.0635,  0.3592])\n",
      "Epoch 1245, Loss 3.317934513092041\n",
      "\t Params: tensor([  4.9955, -15.1975])\n",
      "\t Grad: tensor([-0.0633,  0.3586])\n",
      "Epoch 1246, Loss 3.31661057472229\n",
      "\t Params: tensor([  4.9961, -15.2010])\n",
      "\t Grad: tensor([-0.0633,  0.3580])\n",
      "Epoch 1247, Loss 3.3152894973754883\n",
      "\t Params: tensor([  4.9967, -15.2046])\n",
      "\t Grad: tensor([-0.0631,  0.3574])\n",
      "Epoch 1248, Loss 3.3139734268188477\n",
      "\t Params: tensor([  4.9973, -15.2082])\n",
      "\t Grad: tensor([-0.0630,  0.3568])\n",
      "Epoch 1249, Loss 3.3126630783081055\n",
      "\t Params: tensor([  4.9980, -15.2117])\n",
      "\t Grad: tensor([-0.0629,  0.3562])\n",
      "Epoch 1250, Loss 3.3113532066345215\n",
      "\t Params: tensor([  4.9986, -15.2153])\n",
      "\t Grad: tensor([-0.0628,  0.3556])\n",
      "Epoch 1251, Loss 3.3100526332855225\n",
      "\t Params: tensor([  4.9992, -15.2189])\n",
      "\t Grad: tensor([-0.0627,  0.3550])\n",
      "Epoch 1252, Loss 3.3087563514709473\n",
      "\t Params: tensor([  4.9999, -15.2224])\n",
      "\t Grad: tensor([-0.0626,  0.3543])\n",
      "Epoch 1253, Loss 3.3074629306793213\n",
      "\t Params: tensor([  5.0005, -15.2259])\n",
      "\t Grad: tensor([-0.0625,  0.3537])\n",
      "Epoch 1254, Loss 3.3061704635620117\n",
      "\t Params: tensor([  5.0011, -15.2295])\n",
      "\t Grad: tensor([-0.0624,  0.3531])\n",
      "Epoch 1255, Loss 3.304886817932129\n",
      "\t Params: tensor([  5.0017, -15.2330])\n",
      "\t Grad: tensor([-0.0623,  0.3525])\n",
      "Epoch 1256, Loss 3.303605079650879\n",
      "\t Params: tensor([  5.0024, -15.2365])\n",
      "\t Grad: tensor([-0.0622,  0.3519])\n",
      "Epoch 1257, Loss 3.3023290634155273\n",
      "\t Params: tensor([  5.0030, -15.2400])\n",
      "\t Grad: tensor([-0.0620,  0.3514])\n",
      "Epoch 1258, Loss 3.3010573387145996\n",
      "\t Params: tensor([  5.0036, -15.2435])\n",
      "\t Grad: tensor([-0.0620,  0.3508])\n",
      "Epoch 1259, Loss 3.299790859222412\n",
      "\t Params: tensor([  5.0042, -15.2470])\n",
      "\t Grad: tensor([-0.0619,  0.3502])\n",
      "Epoch 1260, Loss 3.298527717590332\n",
      "\t Params: tensor([  5.0048, -15.2505])\n",
      "\t Grad: tensor([-0.0618,  0.3496])\n",
      "Epoch 1261, Loss 3.297266960144043\n",
      "\t Params: tensor([  5.0054, -15.2540])\n",
      "\t Grad: tensor([-0.0616,  0.3490])\n",
      "Epoch 1262, Loss 3.296013593673706\n",
      "\t Params: tensor([  5.0061, -15.2575])\n",
      "\t Grad: tensor([-0.0615,  0.3484])\n",
      "Epoch 1263, Loss 3.294762372970581\n",
      "\t Params: tensor([  5.0067, -15.2610])\n",
      "\t Grad: tensor([-0.0614,  0.3478])\n",
      "Epoch 1264, Loss 3.2935173511505127\n",
      "\t Params: tensor([  5.0073, -15.2645])\n",
      "\t Grad: tensor([-0.0613,  0.3472])\n",
      "Epoch 1265, Loss 3.292275905609131\n",
      "\t Params: tensor([  5.0079, -15.2679])\n",
      "\t Grad: tensor([-0.0612,  0.3466])\n",
      "Epoch 1266, Loss 3.291036367416382\n",
      "\t Params: tensor([  5.0085, -15.2714])\n",
      "\t Grad: tensor([-0.0611,  0.3460])\n",
      "Epoch 1267, Loss 3.289803981781006\n",
      "\t Params: tensor([  5.0091, -15.2748])\n",
      "\t Grad: tensor([-0.0610,  0.3454])\n",
      "Epoch 1268, Loss 3.2885732650756836\n",
      "\t Params: tensor([  5.0097, -15.2783])\n",
      "\t Grad: tensor([-0.0609,  0.3448])\n",
      "Epoch 1269, Loss 3.287346839904785\n",
      "\t Params: tensor([  5.0103, -15.2817])\n",
      "\t Grad: tensor([-0.0608,  0.3443])\n",
      "Epoch 1270, Loss 3.2861289978027344\n",
      "\t Params: tensor([  5.0109, -15.2852])\n",
      "\t Grad: tensor([-0.0607,  0.3437])\n",
      "Epoch 1271, Loss 3.2849111557006836\n",
      "\t Params: tensor([  5.0116, -15.2886])\n",
      "\t Grad: tensor([-0.0606,  0.3431])\n",
      "Epoch 1272, Loss 3.2836976051330566\n",
      "\t Params: tensor([  5.0122, -15.2920])\n",
      "\t Grad: tensor([-0.0605,  0.3425])\n",
      "Epoch 1273, Loss 3.2824883460998535\n",
      "\t Params: tensor([  5.0128, -15.2954])\n",
      "\t Grad: tensor([-0.0604,  0.3419])\n",
      "Epoch 1274, Loss 3.2812840938568115\n",
      "\t Params: tensor([  5.0134, -15.2988])\n",
      "\t Grad: tensor([-0.0603,  0.3413])\n",
      "Epoch 1275, Loss 3.2800850868225098\n",
      "\t Params: tensor([  5.0140, -15.3023])\n",
      "\t Grad: tensor([-0.0602,  0.3408])\n",
      "Epoch 1276, Loss 3.278887987136841\n",
      "\t Params: tensor([  5.0146, -15.3057])\n",
      "\t Grad: tensor([-0.0601,  0.3402])\n",
      "Epoch 1277, Loss 3.277696132659912\n",
      "\t Params: tensor([  5.0152, -15.3091])\n",
      "\t Grad: tensor([-0.0600,  0.3396])\n",
      "Epoch 1278, Loss 3.276506185531616\n",
      "\t Params: tensor([  5.0158, -15.3124])\n",
      "\t Grad: tensor([-0.0599,  0.3390])\n",
      "Epoch 1279, Loss 3.275322198867798\n",
      "\t Params: tensor([  5.0164, -15.3158])\n",
      "\t Grad: tensor([-0.0598,  0.3384])\n",
      "Epoch 1280, Loss 3.274141550064087\n",
      "\t Params: tensor([  5.0170, -15.3192])\n",
      "\t Grad: tensor([-0.0597,  0.3379])\n",
      "Epoch 1281, Loss 3.272967576980591\n",
      "\t Params: tensor([  5.0176, -15.3226])\n",
      "\t Grad: tensor([-0.0596,  0.3373])\n",
      "Epoch 1282, Loss 3.2717931270599365\n",
      "\t Params: tensor([  5.0182, -15.3259])\n",
      "\t Grad: tensor([-0.0595,  0.3367])\n",
      "Epoch 1283, Loss 3.270625114440918\n",
      "\t Params: tensor([  5.0187, -15.3293])\n",
      "\t Grad: tensor([-0.0594,  0.3362])\n",
      "Epoch 1284, Loss 3.269460439682007\n",
      "\t Params: tensor([  5.0193, -15.3327])\n",
      "\t Grad: tensor([-0.0593,  0.3356])\n",
      "Epoch 1285, Loss 3.268301248550415\n",
      "\t Params: tensor([  5.0199, -15.3360])\n",
      "\t Grad: tensor([-0.0592,  0.3350])\n",
      "Epoch 1286, Loss 3.267143487930298\n",
      "\t Params: tensor([  5.0205, -15.3394])\n",
      "\t Grad: tensor([-0.0591,  0.3344])\n",
      "Epoch 1287, Loss 3.265991449356079\n",
      "\t Params: tensor([  5.0211, -15.3427])\n",
      "\t Grad: tensor([-0.0590,  0.3339])\n",
      "Epoch 1288, Loss 3.2648420333862305\n",
      "\t Params: tensor([  5.0217, -15.3460])\n",
      "\t Grad: tensor([-0.0589,  0.3333])\n",
      "Epoch 1289, Loss 3.263699531555176\n",
      "\t Params: tensor([  5.0223, -15.3494])\n",
      "\t Grad: tensor([-0.0588,  0.3327])\n",
      "Epoch 1290, Loss 3.2625558376312256\n",
      "\t Params: tensor([  5.0229, -15.3527])\n",
      "\t Grad: tensor([-0.0587,  0.3322])\n",
      "Epoch 1291, Loss 3.2614214420318604\n",
      "\t Params: tensor([  5.0235, -15.3560])\n",
      "\t Grad: tensor([-0.0586,  0.3316])\n",
      "Epoch 1292, Loss 3.260287284851074\n",
      "\t Params: tensor([  5.0240, -15.3593])\n",
      "\t Grad: tensor([-0.0585,  0.3311])\n",
      "Epoch 1293, Loss 3.259160041809082\n",
      "\t Params: tensor([  5.0246, -15.3626])\n",
      "\t Grad: tensor([-0.0584,  0.3305])\n",
      "Epoch 1294, Loss 3.258033037185669\n",
      "\t Params: tensor([  5.0252, -15.3659])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad: tensor([-0.0583,  0.3299])\n",
      "Epoch 1295, Loss 3.256911516189575\n",
      "\t Params: tensor([  5.0258, -15.3692])\n",
      "\t Grad: tensor([-0.0582,  0.3294])\n",
      "Epoch 1296, Loss 3.255795478820801\n",
      "\t Params: tensor([  5.0264, -15.3725])\n",
      "\t Grad: tensor([-0.0581,  0.3288])\n",
      "Epoch 1297, Loss 3.254680871963501\n",
      "\t Params: tensor([  5.0270, -15.3758])\n",
      "\t Grad: tensor([-0.0580,  0.3282])\n",
      "Epoch 1298, Loss 3.2535688877105713\n",
      "\t Params: tensor([  5.0275, -15.3791])\n",
      "\t Grad: tensor([-0.0579,  0.3277])\n",
      "Epoch 1299, Loss 3.2524619102478027\n",
      "\t Params: tensor([  5.0281, -15.3823])\n",
      "\t Grad: tensor([-0.0578,  0.3271])\n",
      "Epoch 1300, Loss 3.251361608505249\n",
      "\t Params: tensor([  5.0287, -15.3856])\n",
      "\t Grad: tensor([-0.0577,  0.3266])\n",
      "Epoch 1301, Loss 3.2502634525299072\n",
      "\t Params: tensor([  5.0293, -15.3888])\n",
      "\t Grad: tensor([-0.0576,  0.3260])\n",
      "Epoch 1302, Loss 3.2491676807403564\n",
      "\t Params: tensor([  5.0298, -15.3921])\n",
      "\t Grad: tensor([-0.0575,  0.3255])\n",
      "Epoch 1303, Loss 3.2480766773223877\n",
      "\t Params: tensor([  5.0304, -15.3954])\n",
      "\t Grad: tensor([-0.0574,  0.3249])\n",
      "Epoch 1304, Loss 3.24698805809021\n",
      "\t Params: tensor([  5.0310, -15.3986])\n",
      "\t Grad: tensor([-0.0573,  0.3244])\n",
      "Epoch 1305, Loss 3.245903968811035\n",
      "\t Params: tensor([  5.0316, -15.4018])\n",
      "\t Grad: tensor([-0.0572,  0.3238])\n",
      "Epoch 1306, Loss 3.244824171066284\n",
      "\t Params: tensor([  5.0321, -15.4051])\n",
      "\t Grad: tensor([-0.0571,  0.3233])\n",
      "Epoch 1307, Loss 3.243746757507324\n",
      "\t Params: tensor([  5.0327, -15.4083])\n",
      "\t Grad: tensor([-0.0570,  0.3227])\n",
      "Epoch 1308, Loss 3.2426743507385254\n",
      "\t Params: tensor([  5.0333, -15.4115])\n",
      "\t Grad: tensor([-0.0569,  0.3222])\n",
      "Epoch 1309, Loss 3.2416059970855713\n",
      "\t Params: tensor([  5.0338, -15.4147])\n",
      "\t Grad: tensor([-0.0568,  0.3216])\n",
      "Epoch 1310, Loss 3.2405378818511963\n",
      "\t Params: tensor([  5.0344, -15.4179])\n",
      "\t Grad: tensor([-0.0567,  0.3211])\n",
      "Epoch 1311, Loss 3.2394745349884033\n",
      "\t Params: tensor([  5.0350, -15.4211])\n",
      "\t Grad: tensor([-0.0566,  0.3205])\n",
      "Epoch 1312, Loss 3.2384192943573\n",
      "\t Params: tensor([  5.0355, -15.4243])\n",
      "\t Grad: tensor([-0.0565,  0.3200])\n",
      "Epoch 1313, Loss 3.237363338470459\n",
      "\t Params: tensor([  5.0361, -15.4275])\n",
      "\t Grad: tensor([-0.0564,  0.3194])\n",
      "Epoch 1314, Loss 3.236313819885254\n",
      "\t Params: tensor([  5.0367, -15.4307])\n",
      "\t Grad: tensor([-0.0563,  0.3189])\n",
      "Epoch 1315, Loss 3.235264778137207\n",
      "\t Params: tensor([  5.0372, -15.4339])\n",
      "\t Grad: tensor([-0.0562,  0.3184])\n",
      "Epoch 1316, Loss 3.2342183589935303\n",
      "\t Params: tensor([  5.0378, -15.4371])\n",
      "\t Grad: tensor([-0.0561,  0.3178])\n",
      "Epoch 1317, Loss 3.2331793308258057\n",
      "\t Params: tensor([  5.0383, -15.4403])\n",
      "\t Grad: tensor([-0.0561,  0.3173])\n",
      "Epoch 1318, Loss 3.2321431636810303\n",
      "\t Params: tensor([  5.0389, -15.4434])\n",
      "\t Grad: tensor([-0.0560,  0.3167])\n",
      "Epoch 1319, Loss 3.2311086654663086\n",
      "\t Params: tensor([  5.0395, -15.4466])\n",
      "\t Grad: tensor([-0.0558,  0.3162])\n",
      "Epoch 1320, Loss 3.2300782203674316\n",
      "\t Params: tensor([  5.0400, -15.4498])\n",
      "\t Grad: tensor([-0.0558,  0.3157])\n",
      "Epoch 1321, Loss 3.229050636291504\n",
      "\t Params: tensor([  5.0406, -15.4529])\n",
      "\t Grad: tensor([-0.0557,  0.3151])\n",
      "Epoch 1322, Loss 3.228027105331421\n",
      "\t Params: tensor([  5.0411, -15.4560])\n",
      "\t Grad: tensor([-0.0556,  0.3146])\n",
      "Epoch 1323, Loss 3.2270095348358154\n",
      "\t Params: tensor([  5.0417, -15.4592])\n",
      "\t Grad: tensor([-0.0555,  0.3141])\n",
      "Epoch 1324, Loss 3.225992441177368\n",
      "\t Params: tensor([  5.0422, -15.4623])\n",
      "\t Grad: tensor([-0.0554,  0.3135])\n",
      "Epoch 1325, Loss 3.2249794006347656\n",
      "\t Params: tensor([  5.0428, -15.4655])\n",
      "\t Grad: tensor([-0.0553,  0.3130])\n",
      "Epoch 1326, Loss 3.223970890045166\n",
      "\t Params: tensor([  5.0433, -15.4686])\n",
      "\t Grad: tensor([-0.0552,  0.3125])\n",
      "Epoch 1327, Loss 3.2229654788970947\n",
      "\t Params: tensor([  5.0439, -15.4717])\n",
      "\t Grad: tensor([-0.0551,  0.3119])\n",
      "Epoch 1328, Loss 3.2219603061676025\n",
      "\t Params: tensor([  5.0444, -15.4748])\n",
      "\t Grad: tensor([-0.0550,  0.3114])\n",
      "Epoch 1329, Loss 3.2209622859954834\n",
      "\t Params: tensor([  5.0450, -15.4779])\n",
      "\t Grad: tensor([-0.0549,  0.3109])\n",
      "Epoch 1330, Loss 3.2199668884277344\n",
      "\t Params: tensor([  5.0455, -15.4810])\n",
      "\t Grad: tensor([-0.0548,  0.3103])\n",
      "Epoch 1331, Loss 3.218975067138672\n",
      "\t Params: tensor([  5.0461, -15.4841])\n",
      "\t Grad: tensor([-0.0547,  0.3098])\n",
      "Epoch 1332, Loss 3.2179863452911377\n",
      "\t Params: tensor([  5.0466, -15.4872])\n",
      "\t Grad: tensor([-0.0546,  0.3093])\n",
      "Epoch 1333, Loss 3.2169997692108154\n",
      "\t Params: tensor([  5.0472, -15.4903])\n",
      "\t Grad: tensor([-0.0545,  0.3088])\n",
      "Epoch 1334, Loss 3.216017246246338\n",
      "\t Params: tensor([  5.0477, -15.4934])\n",
      "\t Grad: tensor([-0.0544,  0.3082])\n",
      "Epoch 1335, Loss 3.215038299560547\n",
      "\t Params: tensor([  5.0483, -15.4965])\n",
      "\t Grad: tensor([-0.0543,  0.3077])\n",
      "Epoch 1336, Loss 3.214062452316284\n",
      "\t Params: tensor([  5.0488, -15.4995])\n",
      "\t Grad: tensor([-0.0543,  0.3072])\n",
      "Epoch 1337, Loss 3.213092088699341\n",
      "\t Params: tensor([  5.0494, -15.5026])\n",
      "\t Grad: tensor([-0.0542,  0.3067])\n",
      "Epoch 1338, Loss 3.2121224403381348\n",
      "\t Params: tensor([  5.0499, -15.5057])\n",
      "\t Grad: tensor([-0.0541,  0.3061])\n",
      "Epoch 1339, Loss 3.2111566066741943\n",
      "\t Params: tensor([  5.0504, -15.5087])\n",
      "\t Grad: tensor([-0.0540,  0.3056])\n",
      "Epoch 1340, Loss 3.2101919651031494\n",
      "\t Params: tensor([  5.0510, -15.5118])\n",
      "\t Grad: tensor([-0.0539,  0.3051])\n",
      "Epoch 1341, Loss 3.2092347145080566\n",
      "\t Params: tensor([  5.0515, -15.5148])\n",
      "\t Grad: tensor([-0.0538,  0.3046])\n",
      "Epoch 1342, Loss 3.2082788944244385\n",
      "\t Params: tensor([  5.0521, -15.5179])\n",
      "\t Grad: tensor([-0.0537,  0.3041])\n",
      "Epoch 1343, Loss 3.2073256969451904\n",
      "\t Params: tensor([  5.0526, -15.5209])\n",
      "\t Grad: tensor([-0.0536,  0.3036])\n",
      "Epoch 1344, Loss 3.206376075744629\n",
      "\t Params: tensor([  5.0531, -15.5239])\n",
      "\t Grad: tensor([-0.0535,  0.3030])\n",
      "Epoch 1345, Loss 3.205430030822754\n",
      "\t Params: tensor([  5.0537, -15.5269])\n",
      "\t Grad: tensor([-0.0534,  0.3025])\n",
      "Epoch 1346, Loss 3.2044878005981445\n",
      "\t Params: tensor([  5.0542, -15.5300])\n",
      "\t Grad: tensor([-0.0533,  0.3020])\n",
      "Epoch 1347, Loss 3.203547477722168\n",
      "\t Params: tensor([  5.0547, -15.5330])\n",
      "\t Grad: tensor([-0.0532,  0.3015])\n",
      "Epoch 1348, Loss 3.2026102542877197\n",
      "\t Params: tensor([  5.0553, -15.5360])\n",
      "\t Grad: tensor([-0.0532,  0.3010])\n",
      "Epoch 1349, Loss 3.2016775608062744\n",
      "\t Params: tensor([  5.0558, -15.5390])\n",
      "\t Grad: tensor([-0.0531,  0.3005])\n",
      "Epoch 1350, Loss 3.200746774673462\n",
      "\t Params: tensor([  5.0563, -15.5420])\n",
      "\t Grad: tensor([-0.0530,  0.3000])\n",
      "Epoch 1351, Loss 3.199820041656494\n",
      "\t Params: tensor([  5.0568, -15.5450])\n",
      "\t Grad: tensor([-0.0529,  0.2995])\n",
      "Epoch 1352, Loss 3.198896884918213\n",
      "\t Params: tensor([  5.0574, -15.5480])\n",
      "\t Grad: tensor([-0.0528,  0.2989])\n",
      "Epoch 1353, Loss 3.1979761123657227\n",
      "\t Params: tensor([  5.0579, -15.5510])\n",
      "\t Grad: tensor([-0.0527,  0.2984])\n",
      "Epoch 1354, Loss 3.1970596313476562\n",
      "\t Params: tensor([  5.0584, -15.5539])\n",
      "\t Grad: tensor([-0.0526,  0.2979])\n",
      "Epoch 1355, Loss 3.196143388748169\n",
      "\t Params: tensor([  5.0590, -15.5569])\n",
      "\t Grad: tensor([-0.0525,  0.2974])\n",
      "Epoch 1356, Loss 3.1952309608459473\n",
      "\t Params: tensor([  5.0595, -15.5599])\n",
      "\t Grad: tensor([-0.0524,  0.2969])\n",
      "Epoch 1357, Loss 3.194324254989624\n",
      "\t Params: tensor([  5.0600, -15.5629])\n",
      "\t Grad: tensor([-0.0524,  0.2964])\n",
      "Epoch 1358, Loss 3.1934196949005127\n",
      "\t Params: tensor([  5.0605, -15.5658])\n",
      "\t Grad: tensor([-0.0523,  0.2959])\n",
      "Epoch 1359, Loss 3.192516565322876\n",
      "\t Params: tensor([  5.0610, -15.5688])\n",
      "\t Grad: tensor([-0.0522,  0.2954])\n",
      "Epoch 1360, Loss 3.1916162967681885\n",
      "\t Params: tensor([  5.0616, -15.5717])\n",
      "\t Grad: tensor([-0.0521,  0.2949])\n",
      "Epoch 1361, Loss 3.190720319747925\n",
      "\t Params: tensor([  5.0621, -15.5747])\n",
      "\t Grad: tensor([-0.0520,  0.2944])\n",
      "Epoch 1362, Loss 3.189828634262085\n",
      "\t Params: tensor([  5.0626, -15.5776])\n",
      "\t Grad: tensor([-0.0519,  0.2939])\n",
      "Epoch 1363, Loss 3.1889379024505615\n",
      "\t Params: tensor([  5.0631, -15.5805])\n",
      "\t Grad: tensor([-0.0518,  0.2934])\n",
      "Epoch 1364, Loss 3.1880507469177246\n",
      "\t Params: tensor([  5.0636, -15.5835])\n",
      "\t Grad: tensor([-0.0517,  0.2929])\n",
      "Epoch 1365, Loss 3.1871659755706787\n",
      "\t Params: tensor([  5.0642, -15.5864])\n",
      "\t Grad: tensor([-0.0516,  0.2924])\n",
      "Epoch 1366, Loss 3.1862871646881104\n",
      "\t Params: tensor([  5.0647, -15.5893])\n",
      "\t Grad: tensor([-0.0516,  0.2919])\n",
      "Epoch 1367, Loss 3.185408592224121\n",
      "\t Params: tensor([  5.0652, -15.5922])\n",
      "\t Grad: tensor([-0.0515,  0.2914])\n",
      "Epoch 1368, Loss 3.1845343112945557\n",
      "\t Params: tensor([  5.0657, -15.5951])\n",
      "\t Grad: tensor([-0.0514,  0.2909])\n",
      "Epoch 1369, Loss 3.1836624145507812\n",
      "\t Params: tensor([  5.0662, -15.5980])\n",
      "\t Grad: tensor([-0.0513,  0.2904])\n",
      "Epoch 1370, Loss 3.1827917098999023\n",
      "\t Params: tensor([  5.0667, -15.6009])\n",
      "\t Grad: tensor([-0.0512,  0.2899])\n",
      "Epoch 1371, Loss 3.1819252967834473\n",
      "\t Params: tensor([  5.0672, -15.6038])\n",
      "\t Grad: tensor([-0.0511,  0.2894])\n",
      "Epoch 1372, Loss 3.181062936782837\n",
      "\t Params: tensor([  5.0678, -15.6067])\n",
      "\t Grad: tensor([-0.0510,  0.2890])\n",
      "Epoch 1373, Loss 3.1802008152008057\n",
      "\t Params: tensor([  5.0683, -15.6096])\n",
      "\t Grad: tensor([-0.0509,  0.2885])\n",
      "Epoch 1374, Loss 3.1793465614318848\n",
      "\t Params: tensor([  5.0688, -15.6125])\n",
      "\t Grad: tensor([-0.0509,  0.2880])\n",
      "Epoch 1375, Loss 3.178490400314331\n",
      "\t Params: tensor([  5.0693, -15.6154])\n",
      "\t Grad: tensor([-0.0508,  0.2875])\n",
      "Epoch 1376, Loss 3.177638053894043\n",
      "\t Params: tensor([  5.0698, -15.6182])\n",
      "\t Grad: tensor([-0.0507,  0.2870])\n",
      "Epoch 1377, Loss 3.1767892837524414\n",
      "\t Params: tensor([  5.0703, -15.6211])\n",
      "\t Grad: tensor([-0.0506,  0.2865])\n",
      "Epoch 1378, Loss 3.1759445667266846\n",
      "\t Params: tensor([  5.0708, -15.6240])\n",
      "\t Grad: tensor([-0.0505,  0.2860])\n",
      "Epoch 1379, Loss 3.1751012802124023\n",
      "\t Params: tensor([  5.0713, -15.6268])\n",
      "\t Grad: tensor([-0.0504,  0.2855])\n",
      "Epoch 1380, Loss 3.1742615699768066\n",
      "\t Params: tensor([  5.0718, -15.6297])\n",
      "\t Grad: tensor([-0.0504,  0.2850])\n",
      "Epoch 1381, Loss 3.17342472076416\n",
      "\t Params: tensor([  5.0723, -15.6325])\n",
      "\t Grad: tensor([-0.0503,  0.2846])\n",
      "Epoch 1382, Loss 3.1725900173187256\n",
      "\t Params: tensor([  5.0728, -15.6353])\n",
      "\t Grad: tensor([-0.0502,  0.2841])\n",
      "Epoch 1383, Loss 3.1717588901519775\n",
      "\t Params: tensor([  5.0733, -15.6382])\n",
      "\t Grad: tensor([-0.0501,  0.2836])\n",
      "Epoch 1384, Loss 3.170928716659546\n",
      "\t Params: tensor([  5.0738, -15.6410])\n",
      "\t Grad: tensor([-0.0500,  0.2831])\n",
      "Epoch 1385, Loss 3.170102834701538\n",
      "\t Params: tensor([  5.0743, -15.6438])\n",
      "\t Grad: tensor([-0.0499,  0.2826])\n",
      "Epoch 1386, Loss 3.1692795753479004\n",
      "\t Params: tensor([  5.0748, -15.6467])\n",
      "\t Grad: tensor([-0.0498,  0.2822])\n",
      "Epoch 1387, Loss 3.168461799621582\n",
      "\t Params: tensor([  5.0753, -15.6495])\n",
      "\t Grad: tensor([-0.0498,  0.2817])\n",
      "Epoch 1388, Loss 3.1676435470581055\n",
      "\t Params: tensor([  5.0758, -15.6523])\n",
      "\t Grad: tensor([-0.0497,  0.2812])\n",
      "Epoch 1389, Loss 3.166827440261841\n",
      "\t Params: tensor([  5.0763, -15.6551])\n",
      "\t Grad: tensor([-0.0496,  0.2807])\n",
      "Epoch 1390, Loss 3.1660165786743164\n",
      "\t Params: tensor([  5.0768, -15.6579])\n",
      "\t Grad: tensor([-0.0495,  0.2802])\n",
      "Epoch 1391, Loss 3.1652066707611084\n",
      "\t Params: tensor([  5.0773, -15.6607])\n",
      "\t Grad: tensor([-0.0494,  0.2798])\n",
      "Epoch 1392, Loss 3.164400577545166\n",
      "\t Params: tensor([  5.0778, -15.6635])\n",
      "\t Grad: tensor([-0.0493,  0.2793])\n",
      "Epoch 1393, Loss 3.1635942459106445\n",
      "\t Params: tensor([  5.0783, -15.6663])\n",
      "\t Grad: tensor([-0.0492,  0.2788])\n",
      "Epoch 1394, Loss 3.162795066833496\n",
      "\t Params: tensor([  5.0788, -15.6691])\n",
      "\t Grad: tensor([-0.0492,  0.2783])\n",
      "Epoch 1395, Loss 3.161996364593506\n",
      "\t Params: tensor([  5.0793, -15.6718])\n",
      "\t Grad: tensor([-0.0491,  0.2779])\n",
      "Epoch 1396, Loss 3.161201238632202\n",
      "\t Params: tensor([  5.0797, -15.6746])\n",
      "\t Grad: tensor([-0.0490,  0.2774])\n",
      "Epoch 1397, Loss 3.160410165786743\n",
      "\t Params: tensor([  5.0802, -15.6774])\n",
      "\t Grad: tensor([-0.0489,  0.2769])\n",
      "Epoch 1398, Loss 3.1596181392669678\n",
      "\t Params: tensor([  5.0807, -15.6802])\n",
      "\t Grad: tensor([-0.0488,  0.2765])\n",
      "Epoch 1399, Loss 3.158830404281616\n",
      "\t Params: tensor([  5.0812, -15.6829])\n",
      "\t Grad: tensor([-0.0488,  0.2760])\n",
      "Epoch 1400, Loss 3.1580464839935303\n",
      "\t Params: tensor([  5.0817, -15.6857])\n",
      "\t Grad: tensor([-0.0487,  0.2755])\n",
      "Epoch 1401, Loss 3.1572632789611816\n",
      "\t Params: tensor([  5.0822, -15.6884])\n",
      "\t Grad: tensor([-0.0486,  0.2751])\n",
      "Epoch 1402, Loss 3.156484365463257\n",
      "\t Params: tensor([  5.0827, -15.6912])\n",
      "\t Grad: tensor([-0.0485,  0.2746])\n",
      "Epoch 1403, Loss 3.155707597732544\n",
      "\t Params: tensor([  5.0832, -15.6939])\n",
      "\t Grad: tensor([-0.0484,  0.2741])\n",
      "Epoch 1404, Loss 3.154932975769043\n",
      "\t Params: tensor([  5.0836, -15.6966])\n",
      "\t Grad: tensor([-0.0483,  0.2736])\n",
      "Epoch 1405, Loss 3.1541619300842285\n",
      "\t Params: tensor([  5.0841, -15.6994])\n",
      "\t Grad: tensor([-0.0483,  0.2732])\n",
      "Epoch 1406, Loss 3.153392791748047\n",
      "\t Params: tensor([  5.0846, -15.7021])\n",
      "\t Grad: tensor([-0.0482,  0.2727])\n",
      "Epoch 1407, Loss 3.15262770652771\n",
      "\t Params: tensor([  5.0851, -15.7048])\n",
      "\t Grad: tensor([-0.0481,  0.2723])\n",
      "Epoch 1408, Loss 3.151864528656006\n",
      "\t Params: tensor([  5.0856, -15.7075])\n",
      "\t Grad: tensor([-0.0480,  0.2718])\n",
      "Epoch 1409, Loss 3.1511011123657227\n",
      "\t Params: tensor([  5.0860, -15.7103])\n",
      "\t Grad: tensor([-0.0479,  0.2713])\n",
      "Epoch 1410, Loss 3.150343179702759\n",
      "\t Params: tensor([  5.0865, -15.7130])\n",
      "\t Grad: tensor([-0.0479,  0.2709])\n",
      "Epoch 1411, Loss 3.1495866775512695\n",
      "\t Params: tensor([  5.0870, -15.7157])\n",
      "\t Grad: tensor([-0.0478,  0.2704])\n",
      "Epoch 1412, Loss 3.1488332748413086\n",
      "\t Params: tensor([  5.0875, -15.7184])\n",
      "\t Grad: tensor([-0.0477,  0.2700])\n",
      "Epoch 1413, Loss 3.148082733154297\n",
      "\t Params: tensor([  5.0879, -15.7211])\n",
      "\t Grad: tensor([-0.0476,  0.2695])\n",
      "Epoch 1414, Loss 3.1473350524902344\n",
      "\t Params: tensor([  5.0884, -15.7238])\n",
      "\t Grad: tensor([-0.0475,  0.2690])\n",
      "Epoch 1415, Loss 3.146588087081909\n",
      "\t Params: tensor([  5.0889, -15.7264])\n",
      "\t Grad: tensor([-0.0474,  0.2686])\n",
      "Epoch 1416, Loss 3.1458449363708496\n",
      "\t Params: tensor([  5.0894, -15.7291])\n",
      "\t Grad: tensor([-0.0474,  0.2681])\n",
      "Epoch 1417, Loss 3.1451051235198975\n",
      "\t Params: tensor([  5.0898, -15.7318])\n",
      "\t Grad: tensor([-0.0473,  0.2677])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1418, Loss 3.1443674564361572\n",
      "\t Params: tensor([  5.0903, -15.7345])\n",
      "\t Grad: tensor([-0.0472,  0.2672])\n",
      "Epoch 1419, Loss 3.143630027770996\n",
      "\t Params: tensor([  5.0908, -15.7371])\n",
      "\t Grad: tensor([-0.0471,  0.2668])\n",
      "Epoch 1420, Loss 3.1428985595703125\n",
      "\t Params: tensor([  5.0913, -15.7398])\n",
      "\t Grad: tensor([-0.0470,  0.2663])\n",
      "Epoch 1421, Loss 3.1421663761138916\n",
      "\t Params: tensor([  5.0917, -15.7425])\n",
      "\t Grad: tensor([-0.0469,  0.2659])\n",
      "Epoch 1422, Loss 3.1414389610290527\n",
      "\t Params: tensor([  5.0922, -15.7451])\n",
      "\t Grad: tensor([-0.0469,  0.2654])\n",
      "Epoch 1423, Loss 3.1407124996185303\n",
      "\t Params: tensor([  5.0927, -15.7478])\n",
      "\t Grad: tensor([-0.0468,  0.2649])\n",
      "Epoch 1424, Loss 3.139988899230957\n",
      "\t Params: tensor([  5.0931, -15.7504])\n",
      "\t Grad: tensor([-0.0467,  0.2645])\n",
      "Epoch 1425, Loss 3.1392714977264404\n",
      "\t Params: tensor([  5.0936, -15.7530])\n",
      "\t Grad: tensor([-0.0466,  0.2641])\n",
      "Epoch 1426, Loss 3.1385505199432373\n",
      "\t Params: tensor([  5.0941, -15.7557])\n",
      "\t Grad: tensor([-0.0466,  0.2636])\n",
      "Epoch 1427, Loss 3.1378347873687744\n",
      "\t Params: tensor([  5.0945, -15.7583])\n",
      "\t Grad: tensor([-0.0465,  0.2632])\n",
      "Epoch 1428, Loss 3.1371209621429443\n",
      "\t Params: tensor([  5.0950, -15.7609])\n",
      "\t Grad: tensor([-0.0464,  0.2627])\n",
      "Epoch 1429, Loss 3.136409044265747\n",
      "\t Params: tensor([  5.0955, -15.7636])\n",
      "\t Grad: tensor([-0.0463,  0.2623])\n",
      "Epoch 1430, Loss 3.1357016563415527\n",
      "\t Params: tensor([  5.0959, -15.7662])\n",
      "\t Grad: tensor([-0.0462,  0.2618])\n",
      "Epoch 1431, Loss 3.1349942684173584\n",
      "\t Params: tensor([  5.0964, -15.7688])\n",
      "\t Grad: tensor([-0.0461,  0.2614])\n",
      "Epoch 1432, Loss 3.134291648864746\n",
      "\t Params: tensor([  5.0968, -15.7714])\n",
      "\t Grad: tensor([-0.0461,  0.2609])\n",
      "Epoch 1433, Loss 3.13358998298645\n",
      "\t Params: tensor([  5.0973, -15.7740])\n",
      "\t Grad: tensor([-0.0460,  0.2605])\n",
      "Epoch 1434, Loss 3.1328887939453125\n",
      "\t Params: tensor([  5.0978, -15.7766])\n",
      "\t Grad: tensor([-0.0459,  0.2600])\n",
      "Epoch 1435, Loss 3.1321942806243896\n",
      "\t Params: tensor([  5.0982, -15.7792])\n",
      "\t Grad: tensor([-0.0459,  0.2596])\n",
      "Epoch 1436, Loss 3.131500244140625\n",
      "\t Params: tensor([  5.0987, -15.7818])\n",
      "\t Grad: tensor([-0.0458,  0.2592])\n",
      "Epoch 1437, Loss 3.130810022354126\n",
      "\t Params: tensor([  5.0991, -15.7844])\n",
      "\t Grad: tensor([-0.0457,  0.2587])\n",
      "Epoch 1438, Loss 3.1301186084747314\n",
      "\t Params: tensor([  5.0996, -15.7870])\n",
      "\t Grad: tensor([-0.0456,  0.2583])\n",
      "Epoch 1439, Loss 3.129431962966919\n",
      "\t Params: tensor([  5.1000, -15.7895])\n",
      "\t Grad: tensor([-0.0455,  0.2578])\n",
      "Epoch 1440, Loss 3.1287457942962646\n",
      "\t Params: tensor([  5.1005, -15.7921])\n",
      "\t Grad: tensor([-0.0455,  0.2574])\n",
      "Epoch 1441, Loss 3.1280641555786133\n",
      "\t Params: tensor([  5.1010, -15.7947])\n",
      "\t Grad: tensor([-0.0454,  0.2570])\n",
      "Epoch 1442, Loss 3.1273818016052246\n",
      "\t Params: tensor([  5.1014, -15.7973])\n",
      "\t Grad: tensor([-0.0453,  0.2565])\n",
      "Epoch 1443, Loss 3.1267051696777344\n",
      "\t Params: tensor([  5.1019, -15.7998])\n",
      "\t Grad: tensor([-0.0453,  0.2561])\n",
      "Epoch 1444, Loss 3.126030206680298\n",
      "\t Params: tensor([  5.1023, -15.8024])\n",
      "\t Grad: tensor([-0.0452,  0.2557])\n",
      "Epoch 1445, Loss 3.125356435775757\n",
      "\t Params: tensor([  5.1028, -15.8049])\n",
      "\t Grad: tensor([-0.0451,  0.2552])\n",
      "Epoch 1446, Loss 3.124683380126953\n",
      "\t Params: tensor([  5.1032, -15.8075])\n",
      "\t Grad: tensor([-0.0450,  0.2548])\n",
      "Epoch 1447, Loss 3.124016046524048\n",
      "\t Params: tensor([  5.1037, -15.8100])\n",
      "\t Grad: tensor([-0.0449,  0.2544])\n",
      "Epoch 1448, Loss 3.1233489513397217\n",
      "\t Params: tensor([  5.1041, -15.8126])\n",
      "\t Grad: tensor([-0.0449,  0.2539])\n",
      "Epoch 1449, Loss 3.1226859092712402\n",
      "\t Params: tensor([  5.1046, -15.8151])\n",
      "\t Grad: tensor([-0.0448,  0.2535])\n",
      "Epoch 1450, Loss 3.1220223903656006\n",
      "\t Params: tensor([  5.1050, -15.8176])\n",
      "\t Grad: tensor([-0.0447,  0.2531])\n",
      "Epoch 1451, Loss 3.1213622093200684\n",
      "\t Params: tensor([  5.1055, -15.8201])\n",
      "\t Grad: tensor([-0.0446,  0.2526])\n",
      "Epoch 1452, Loss 3.120706796646118\n",
      "\t Params: tensor([  5.1059, -15.8227])\n",
      "\t Grad: tensor([-0.0445,  0.2522])\n",
      "Epoch 1453, Loss 3.1200485229492188\n",
      "\t Params: tensor([  5.1063, -15.8252])\n",
      "\t Grad: tensor([-0.0445,  0.2518])\n",
      "Epoch 1454, Loss 3.119396924972534\n",
      "\t Params: tensor([  5.1068, -15.8277])\n",
      "\t Grad: tensor([-0.0444,  0.2513])\n",
      "Epoch 1455, Loss 3.118746042251587\n",
      "\t Params: tensor([  5.1072, -15.8302])\n",
      "\t Grad: tensor([-0.0443,  0.2509])\n",
      "Epoch 1456, Loss 3.1180977821350098\n",
      "\t Params: tensor([  5.1077, -15.8327])\n",
      "\t Grad: tensor([-0.0442,  0.2505])\n",
      "Epoch 1457, Loss 3.1174514293670654\n",
      "\t Params: tensor([  5.1081, -15.8352])\n",
      "\t Grad: tensor([-0.0442,  0.2501])\n",
      "Epoch 1458, Loss 3.116805076599121\n",
      "\t Params: tensor([  5.1086, -15.8377])\n",
      "\t Grad: tensor([-0.0441,  0.2496])\n",
      "Epoch 1459, Loss 3.116163730621338\n",
      "\t Params: tensor([  5.1090, -15.8402])\n",
      "\t Grad: tensor([-0.0440,  0.2492])\n",
      "Epoch 1460, Loss 3.1155245304107666\n",
      "\t Params: tensor([  5.1094, -15.8427])\n",
      "\t Grad: tensor([-0.0439,  0.2488])\n",
      "Epoch 1461, Loss 3.1148862838745117\n",
      "\t Params: tensor([  5.1099, -15.8452])\n",
      "\t Grad: tensor([-0.0439,  0.2484])\n",
      "Epoch 1462, Loss 3.114250898361206\n",
      "\t Params: tensor([  5.1103, -15.8477])\n",
      "\t Grad: tensor([-0.0438,  0.2480])\n",
      "Epoch 1463, Loss 3.113616704940796\n",
      "\t Params: tensor([  5.1107, -15.8501])\n",
      "\t Grad: tensor([-0.0437,  0.2475])\n",
      "Epoch 1464, Loss 3.1129846572875977\n",
      "\t Params: tensor([  5.1112, -15.8526])\n",
      "\t Grad: tensor([-0.0437,  0.2471])\n",
      "Epoch 1465, Loss 3.1123578548431396\n",
      "\t Params: tensor([  5.1116, -15.8551])\n",
      "\t Grad: tensor([-0.0436,  0.2467])\n",
      "Epoch 1466, Loss 3.1117308139801025\n",
      "\t Params: tensor([  5.1121, -15.8575])\n",
      "\t Grad: tensor([-0.0435,  0.2463])\n",
      "Epoch 1467, Loss 3.111102819442749\n",
      "\t Params: tensor([  5.1125, -15.8600])\n",
      "\t Grad: tensor([-0.0434,  0.2459])\n",
      "Epoch 1468, Loss 3.1104841232299805\n",
      "\t Params: tensor([  5.1129, -15.8624])\n",
      "\t Grad: tensor([-0.0433,  0.2454])\n",
      "Epoch 1469, Loss 3.1098592281341553\n",
      "\t Params: tensor([  5.1134, -15.8649])\n",
      "\t Grad: tensor([-0.0433,  0.2450])\n",
      "Epoch 1470, Loss 3.1092429161071777\n",
      "\t Params: tensor([  5.1138, -15.8673])\n",
      "\t Grad: tensor([-0.0432,  0.2446])\n",
      "Epoch 1471, Loss 3.1086266040802\n",
      "\t Params: tensor([  5.1142, -15.8698])\n",
      "\t Grad: tensor([-0.0431,  0.2442])\n",
      "Epoch 1472, Loss 3.10801100730896\n",
      "\t Params: tensor([  5.1147, -15.8722])\n",
      "\t Grad: tensor([-0.0430,  0.2438])\n",
      "Epoch 1473, Loss 3.10740065574646\n",
      "\t Params: tensor([  5.1151, -15.8747])\n",
      "\t Grad: tensor([-0.0430,  0.2434])\n",
      "Epoch 1474, Loss 3.106790781021118\n",
      "\t Params: tensor([  5.1155, -15.8771])\n",
      "\t Grad: tensor([-0.0429,  0.2429])\n",
      "Epoch 1475, Loss 3.106180429458618\n",
      "\t Params: tensor([  5.1159, -15.8795])\n",
      "\t Grad: tensor([-0.0428,  0.2425])\n",
      "Epoch 1476, Loss 3.1055753231048584\n",
      "\t Params: tensor([  5.1164, -15.8819])\n",
      "\t Grad: tensor([-0.0428,  0.2421])\n",
      "Epoch 1477, Loss 3.1049718856811523\n",
      "\t Params: tensor([  5.1168, -15.8843])\n",
      "\t Grad: tensor([-0.0427,  0.2417])\n",
      "Epoch 1478, Loss 3.1043701171875\n",
      "\t Params: tensor([  5.1172, -15.8868])\n",
      "\t Grad: tensor([-0.0426,  0.2413])\n",
      "Epoch 1479, Loss 3.1037697792053223\n",
      "\t Params: tensor([  5.1176, -15.8892])\n",
      "\t Grad: tensor([-0.0425,  0.2409])\n",
      "Epoch 1480, Loss 3.1031720638275146\n",
      "\t Params: tensor([  5.1181, -15.8916])\n",
      "\t Grad: tensor([-0.0425,  0.2405])\n",
      "Epoch 1481, Loss 3.102576494216919\n",
      "\t Params: tensor([  5.1185, -15.8940])\n",
      "\t Grad: tensor([-0.0424,  0.2401])\n",
      "Epoch 1482, Loss 3.1019821166992188\n",
      "\t Params: tensor([  5.1189, -15.8964])\n",
      "\t Grad: tensor([-0.0423,  0.2397])\n",
      "Epoch 1483, Loss 3.1013898849487305\n",
      "\t Params: tensor([  5.1193, -15.8988])\n",
      "\t Grad: tensor([-0.0423,  0.2393])\n",
      "Epoch 1484, Loss 3.100801706314087\n",
      "\t Params: tensor([  5.1198, -15.9011])\n",
      "\t Grad: tensor([-0.0422,  0.2388])\n",
      "Epoch 1485, Loss 3.100213050842285\n",
      "\t Params: tensor([  5.1202, -15.9035])\n",
      "\t Grad: tensor([-0.0421,  0.2384])\n",
      "Epoch 1486, Loss 3.0996274948120117\n",
      "\t Params: tensor([  5.1206, -15.9059])\n",
      "\t Grad: tensor([-0.0421,  0.2380])\n",
      "Epoch 1487, Loss 3.099043846130371\n",
      "\t Params: tensor([  5.1210, -15.9083])\n",
      "\t Grad: tensor([-0.0420,  0.2376])\n",
      "Epoch 1488, Loss 3.0984625816345215\n",
      "\t Params: tensor([  5.1214, -15.9107])\n",
      "\t Grad: tensor([-0.0419,  0.2372])\n",
      "Epoch 1489, Loss 3.097883462905884\n",
      "\t Params: tensor([  5.1219, -15.9130])\n",
      "\t Grad: tensor([-0.0418,  0.2368])\n",
      "Epoch 1490, Loss 3.097302198410034\n",
      "\t Params: tensor([  5.1223, -15.9154])\n",
      "\t Grad: tensor([-0.0418,  0.2364])\n",
      "Epoch 1491, Loss 3.096727132797241\n",
      "\t Params: tensor([  5.1227, -15.9178])\n",
      "\t Grad: tensor([-0.0417,  0.2360])\n",
      "Epoch 1492, Loss 3.0961530208587646\n",
      "\t Params: tensor([  5.1231, -15.9201])\n",
      "\t Grad: tensor([-0.0416,  0.2356])\n",
      "Epoch 1493, Loss 3.095583200454712\n",
      "\t Params: tensor([  5.1235, -15.9225])\n",
      "\t Grad: tensor([-0.0416,  0.2352])\n",
      "Epoch 1494, Loss 3.0950112342834473\n",
      "\t Params: tensor([  5.1239, -15.9248])\n",
      "\t Grad: tensor([-0.0415,  0.2348])\n",
      "Epoch 1495, Loss 3.0944442749023438\n",
      "\t Params: tensor([  5.1244, -15.9272])\n",
      "\t Grad: tensor([-0.0414,  0.2344])\n",
      "Epoch 1496, Loss 3.093876600265503\n",
      "\t Params: tensor([  5.1248, -15.9295])\n",
      "\t Grad: tensor([-0.0413,  0.2340])\n",
      "Epoch 1497, Loss 3.0933141708374023\n",
      "\t Params: tensor([  5.1252, -15.9318])\n",
      "\t Grad: tensor([-0.0413,  0.2336])\n",
      "Epoch 1498, Loss 3.0927505493164062\n",
      "\t Params: tensor([  5.1256, -15.9342])\n",
      "\t Grad: tensor([-0.0412,  0.2332])\n",
      "Epoch 1499, Loss 3.092191219329834\n",
      "\t Params: tensor([  5.1260, -15.9365])\n",
      "\t Grad: tensor([-0.0411,  0.2328])\n",
      "Epoch 1500, Loss 3.091630458831787\n",
      "\t Params: tensor([  5.1264, -15.9388])\n",
      "\t Grad: tensor([-0.0411,  0.2324])\n",
      "Epoch 1501, Loss 3.091074228286743\n",
      "\t Params: tensor([  5.1268, -15.9411])\n",
      "\t Grad: tensor([-0.0410,  0.2320])\n",
      "Epoch 1502, Loss 3.0905203819274902\n",
      "\t Params: tensor([  5.1272, -15.9435])\n",
      "\t Grad: tensor([-0.0409,  0.2317])\n",
      "Epoch 1503, Loss 3.089968681335449\n",
      "\t Params: tensor([  5.1276, -15.9458])\n",
      "\t Grad: tensor([-0.0408,  0.2313])\n",
      "Epoch 1504, Loss 3.0894172191619873\n",
      "\t Params: tensor([  5.1281, -15.9481])\n",
      "\t Grad: tensor([-0.0408,  0.2309])\n",
      "Epoch 1505, Loss 3.0888671875\n",
      "\t Params: tensor([  5.1285, -15.9504])\n",
      "\t Grad: tensor([-0.0407,  0.2305])\n",
      "Epoch 1506, Loss 3.088320255279541\n",
      "\t Params: tensor([  5.1289, -15.9527])\n",
      "\t Grad: tensor([-0.0406,  0.2301])\n",
      "Epoch 1507, Loss 3.0877745151519775\n",
      "\t Params: tensor([  5.1293, -15.9550])\n",
      "\t Grad: tensor([-0.0406,  0.2297])\n",
      "Epoch 1508, Loss 3.0872323513031006\n",
      "\t Params: tensor([  5.1297, -15.9573])\n",
      "\t Grad: tensor([-0.0405,  0.2293])\n",
      "Epoch 1509, Loss 3.0866899490356445\n",
      "\t Params: tensor([  5.1301, -15.9596])\n",
      "\t Grad: tensor([-0.0404,  0.2289])\n",
      "Epoch 1510, Loss 3.0861496925354004\n",
      "\t Params: tensor([  5.1305, -15.9618])\n",
      "\t Grad: tensor([-0.0404,  0.2285])\n",
      "Epoch 1511, Loss 3.0856118202209473\n",
      "\t Params: tensor([  5.1309, -15.9641])\n",
      "\t Grad: tensor([-0.0403,  0.2281])\n",
      "Epoch 1512, Loss 3.0850746631622314\n",
      "\t Params: tensor([  5.1313, -15.9664])\n",
      "\t Grad: tensor([-0.0402,  0.2277])\n",
      "Epoch 1513, Loss 3.0845420360565186\n",
      "\t Params: tensor([  5.1317, -15.9687])\n",
      "\t Grad: tensor([-0.0402,  0.2274])\n",
      "Epoch 1514, Loss 3.0840089321136475\n",
      "\t Params: tensor([  5.1321, -15.9709])\n",
      "\t Grad: tensor([-0.0401,  0.2270])\n",
      "Epoch 1515, Loss 3.0834779739379883\n",
      "\t Params: tensor([  5.1325, -15.9732])\n",
      "\t Grad: tensor([-0.0400,  0.2266])\n",
      "Epoch 1516, Loss 3.0829484462738037\n",
      "\t Params: tensor([  5.1329, -15.9755])\n",
      "\t Grad: tensor([-0.0400,  0.2262])\n",
      "Epoch 1517, Loss 3.0824220180511475\n",
      "\t Params: tensor([  5.1333, -15.9777])\n",
      "\t Grad: tensor([-0.0399,  0.2258])\n",
      "Epoch 1518, Loss 3.081897497177124\n",
      "\t Params: tensor([  5.1337, -15.9800])\n",
      "\t Grad: tensor([-0.0398,  0.2254])\n",
      "Epoch 1519, Loss 3.0813727378845215\n",
      "\t Params: tensor([  5.1341, -15.9822])\n",
      "\t Grad: tensor([-0.0398,  0.2250])\n",
      "Epoch 1520, Loss 3.080850124359131\n",
      "\t Params: tensor([  5.1345, -15.9845])\n",
      "\t Grad: tensor([-0.0397,  0.2247])\n",
      "Epoch 1521, Loss 3.080331325531006\n",
      "\t Params: tensor([  5.1349, -15.9867])\n",
      "\t Grad: tensor([-0.0396,  0.2243])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1522, Loss 3.079810857772827\n",
      "\t Params: tensor([  5.1353, -15.9890])\n",
      "\t Grad: tensor([-0.0396,  0.2239])\n",
      "Epoch 1523, Loss 3.079296350479126\n",
      "\t Params: tensor([  5.1357, -15.9912])\n",
      "\t Grad: tensor([-0.0395,  0.2235])\n",
      "Epoch 1524, Loss 3.0787813663482666\n",
      "\t Params: tensor([  5.1361, -15.9934])\n",
      "\t Grad: tensor([-0.0394,  0.2231])\n",
      "Epoch 1525, Loss 3.078267812728882\n",
      "\t Params: tensor([  5.1365, -15.9957])\n",
      "\t Grad: tensor([-0.0394,  0.2228])\n",
      "Epoch 1526, Loss 3.0777578353881836\n",
      "\t Params: tensor([  5.1369, -15.9979])\n",
      "\t Grad: tensor([-0.0393,  0.2224])\n",
      "Epoch 1527, Loss 3.0772476196289062\n",
      "\t Params: tensor([  5.1372, -16.0001])\n",
      "\t Grad: tensor([-0.0392,  0.2220])\n",
      "Epoch 1528, Loss 3.0767388343811035\n",
      "\t Params: tensor([  5.1376, -16.0023])\n",
      "\t Grad: tensor([-0.0391,  0.2216])\n",
      "Epoch 1529, Loss 3.0762319564819336\n",
      "\t Params: tensor([  5.1380, -16.0045])\n",
      "\t Grad: tensor([-0.0391,  0.2213])\n",
      "Epoch 1530, Loss 3.07572865486145\n",
      "\t Params: tensor([  5.1384, -16.0067])\n",
      "\t Grad: tensor([-0.0390,  0.2209])\n",
      "Epoch 1531, Loss 3.0752248764038086\n",
      "\t Params: tensor([  5.1388, -16.0089])\n",
      "\t Grad: tensor([-0.0390,  0.2205])\n",
      "Epoch 1532, Loss 3.074723958969116\n",
      "\t Params: tensor([  5.1392, -16.0111])\n",
      "\t Grad: tensor([-0.0389,  0.2201])\n",
      "Epoch 1533, Loss 3.0742266178131104\n",
      "\t Params: tensor([  5.1396, -16.0133])\n",
      "\t Grad: tensor([-0.0388,  0.2198])\n",
      "Epoch 1534, Loss 3.073725938796997\n",
      "\t Params: tensor([  5.1400, -16.0155])\n",
      "\t Grad: tensor([-0.0387,  0.2194])\n",
      "Epoch 1535, Loss 3.073232412338257\n",
      "\t Params: tensor([  5.1404, -16.0177])\n",
      "\t Grad: tensor([-0.0387,  0.2190])\n",
      "Epoch 1536, Loss 3.0727386474609375\n",
      "\t Params: tensor([  5.1407, -16.0199])\n",
      "\t Grad: tensor([-0.0386,  0.2186])\n",
      "Epoch 1537, Loss 3.0722451210021973\n",
      "\t Params: tensor([  5.1411, -16.0221])\n",
      "\t Grad: tensor([-0.0385,  0.2183])\n",
      "Epoch 1538, Loss 3.0717530250549316\n",
      "\t Params: tensor([  5.1415, -16.0243])\n",
      "\t Grad: tensor([-0.0385,  0.2179])\n",
      "Epoch 1539, Loss 3.0712647438049316\n",
      "\t Params: tensor([  5.1419, -16.0264])\n",
      "\t Grad: tensor([-0.0384,  0.2175])\n",
      "Epoch 1540, Loss 3.070777654647827\n",
      "\t Params: tensor([  5.1423, -16.0286])\n",
      "\t Grad: tensor([-0.0383,  0.2172])\n",
      "Epoch 1541, Loss 3.0702927112579346\n",
      "\t Params: tensor([  5.1427, -16.0308])\n",
      "\t Grad: tensor([-0.0383,  0.2168])\n",
      "Epoch 1542, Loss 3.069808006286621\n",
      "\t Params: tensor([  5.1430, -16.0330])\n",
      "\t Grad: tensor([-0.0382,  0.2164])\n",
      "Epoch 1543, Loss 3.0693259239196777\n",
      "\t Params: tensor([  5.1434, -16.0351])\n",
      "\t Grad: tensor([-0.0382,  0.2161])\n",
      "Epoch 1544, Loss 3.068845272064209\n",
      "\t Params: tensor([  5.1438, -16.0373])\n",
      "\t Grad: tensor([-0.0381,  0.2157])\n",
      "Epoch 1545, Loss 3.0683655738830566\n",
      "\t Params: tensor([  5.1442, -16.0394])\n",
      "\t Grad: tensor([-0.0380,  0.2153])\n",
      "Epoch 1546, Loss 3.067887306213379\n",
      "\t Params: tensor([  5.1446, -16.0416])\n",
      "\t Grad: tensor([-0.0380,  0.2150])\n",
      "Epoch 1547, Loss 3.0674118995666504\n",
      "\t Params: tensor([  5.1449, -16.0437])\n",
      "\t Grad: tensor([-0.0379,  0.2146])\n",
      "Epoch 1548, Loss 3.066936731338501\n",
      "\t Params: tensor([  5.1453, -16.0459])\n",
      "\t Grad: tensor([-0.0378,  0.2142])\n",
      "Epoch 1549, Loss 3.0664634704589844\n",
      "\t Params: tensor([  5.1457, -16.0480])\n",
      "\t Grad: tensor([-0.0378,  0.2139])\n",
      "Epoch 1550, Loss 3.065992832183838\n",
      "\t Params: tensor([  5.1461, -16.0501])\n",
      "\t Grad: tensor([-0.0377,  0.2135])\n",
      "Epoch 1551, Loss 3.0655243396759033\n",
      "\t Params: tensor([  5.1465, -16.0523])\n",
      "\t Grad: tensor([-0.0376,  0.2131])\n",
      "Epoch 1552, Loss 3.0650551319122314\n",
      "\t Params: tensor([  5.1468, -16.0544])\n",
      "\t Grad: tensor([-0.0376,  0.2128])\n",
      "Epoch 1553, Loss 3.0645883083343506\n",
      "\t Params: tensor([  5.1472, -16.0565])\n",
      "\t Grad: tensor([-0.0375,  0.2124])\n",
      "Epoch 1554, Loss 3.0641226768493652\n",
      "\t Params: tensor([  5.1476, -16.0586])\n",
      "\t Grad: tensor([-0.0375,  0.2120])\n",
      "Epoch 1555, Loss 3.06365966796875\n",
      "\t Params: tensor([  5.1480, -16.0608])\n",
      "\t Grad: tensor([-0.0374,  0.2117])\n",
      "Epoch 1556, Loss 3.0631988048553467\n",
      "\t Params: tensor([  5.1483, -16.0629])\n",
      "\t Grad: tensor([-0.0373,  0.2113])\n",
      "Epoch 1557, Loss 3.0627379417419434\n",
      "\t Params: tensor([  5.1487, -16.0650])\n",
      "\t Grad: tensor([-0.0373,  0.2110])\n",
      "Epoch 1558, Loss 3.0622804164886475\n",
      "\t Params: tensor([  5.1491, -16.0671])\n",
      "\t Grad: tensor([-0.0372,  0.2106])\n",
      "Epoch 1559, Loss 3.061821699142456\n",
      "\t Params: tensor([  5.1494, -16.0692])\n",
      "\t Grad: tensor([-0.0371,  0.2103])\n",
      "Epoch 1560, Loss 3.0613672733306885\n",
      "\t Params: tensor([  5.1498, -16.0713])\n",
      "\t Grad: tensor([-0.0371,  0.2099])\n",
      "Epoch 1561, Loss 3.0609130859375\n",
      "\t Params: tensor([  5.1502, -16.0734])\n",
      "\t Grad: tensor([-0.0370,  0.2095])\n",
      "Epoch 1562, Loss 3.0604615211486816\n",
      "\t Params: tensor([  5.1506, -16.0755])\n",
      "\t Grad: tensor([-0.0370,  0.2092])\n",
      "Epoch 1563, Loss 3.060011386871338\n",
      "\t Params: tensor([  5.1509, -16.0776])\n",
      "\t Grad: tensor([-0.0369,  0.2088])\n",
      "Epoch 1564, Loss 3.059560537338257\n",
      "\t Params: tensor([  5.1513, -16.0796])\n",
      "\t Grad: tensor([-0.0368,  0.2085])\n",
      "Epoch 1565, Loss 3.0591139793395996\n",
      "\t Params: tensor([  5.1517, -16.0817])\n",
      "\t Grad: tensor([-0.0368,  0.2081])\n",
      "Epoch 1566, Loss 3.0586678981781006\n",
      "\t Params: tensor([  5.1520, -16.0838])\n",
      "\t Grad: tensor([-0.0367,  0.2078])\n",
      "Epoch 1567, Loss 3.0582213401794434\n",
      "\t Params: tensor([  5.1524, -16.0859])\n",
      "\t Grad: tensor([-0.0366,  0.2074])\n",
      "Epoch 1568, Loss 3.0577805042266846\n",
      "\t Params: tensor([  5.1528, -16.0880])\n",
      "\t Grad: tensor([-0.0366,  0.2071])\n",
      "Epoch 1569, Loss 3.057337999343872\n",
      "\t Params: tensor([  5.1531, -16.0900])\n",
      "\t Grad: tensor([-0.0365,  0.2067])\n",
      "Epoch 1570, Loss 3.0568978786468506\n",
      "\t Params: tensor([  5.1535, -16.0921])\n",
      "\t Grad: tensor([-0.0364,  0.2064])\n",
      "Epoch 1571, Loss 3.0564582347869873\n",
      "\t Params: tensor([  5.1539, -16.0941])\n",
      "\t Grad: tensor([-0.0364,  0.2060])\n",
      "Epoch 1572, Loss 3.0560190677642822\n",
      "\t Params: tensor([  5.1542, -16.0962])\n",
      "\t Grad: tensor([-0.0363,  0.2057])\n",
      "Epoch 1573, Loss 3.0555853843688965\n",
      "\t Params: tensor([  5.1546, -16.0983])\n",
      "\t Grad: tensor([-0.0363,  0.2053])\n",
      "Epoch 1574, Loss 3.0551505088806152\n",
      "\t Params: tensor([  5.1549, -16.1003])\n",
      "\t Grad: tensor([-0.0362,  0.2050])\n",
      "Epoch 1575, Loss 3.0547170639038086\n",
      "\t Params: tensor([  5.1553, -16.1023])\n",
      "\t Grad: tensor([-0.0361,  0.2046])\n",
      "Epoch 1576, Loss 3.054286241531372\n",
      "\t Params: tensor([  5.1557, -16.1044])\n",
      "\t Grad: tensor([-0.0361,  0.2043])\n",
      "Epoch 1577, Loss 3.05385684967041\n",
      "\t Params: tensor([  5.1560, -16.1064])\n",
      "\t Grad: tensor([-0.0360,  0.2039])\n",
      "Epoch 1578, Loss 3.053427219390869\n",
      "\t Params: tensor([  5.1564, -16.1085])\n",
      "\t Grad: tensor([-0.0360,  0.2036])\n",
      "Epoch 1579, Loss 3.0530004501342773\n",
      "\t Params: tensor([  5.1567, -16.1105])\n",
      "\t Grad: tensor([-0.0359,  0.2032])\n",
      "Epoch 1580, Loss 3.0525755882263184\n",
      "\t Params: tensor([  5.1571, -16.1125])\n",
      "\t Grad: tensor([-0.0358,  0.2029])\n",
      "Epoch 1581, Loss 3.052151679992676\n",
      "\t Params: tensor([  5.1575, -16.1146])\n",
      "\t Grad: tensor([-0.0358,  0.2025])\n",
      "Epoch 1582, Loss 3.051730155944824\n",
      "\t Params: tensor([  5.1578, -16.1166])\n",
      "\t Grad: tensor([-0.0357,  0.2022])\n",
      "Epoch 1583, Loss 3.0513062477111816\n",
      "\t Params: tensor([  5.1582, -16.1186])\n",
      "\t Grad: tensor([-0.0357,  0.2018])\n",
      "Epoch 1584, Loss 3.0508880615234375\n",
      "\t Params: tensor([  5.1585, -16.1206])\n",
      "\t Grad: tensor([-0.0356,  0.2015])\n",
      "Epoch 1585, Loss 3.0504705905914307\n",
      "\t Params: tensor([  5.1589, -16.1226])\n",
      "\t Grad: tensor([-0.0355,  0.2012])\n",
      "Epoch 1586, Loss 3.0500524044036865\n",
      "\t Params: tensor([  5.1592, -16.1246])\n",
      "\t Grad: tensor([-0.0355,  0.2008])\n",
      "Epoch 1587, Loss 3.049638509750366\n",
      "\t Params: tensor([  5.1596, -16.1266])\n",
      "\t Grad: tensor([-0.0354,  0.2005])\n",
      "Epoch 1588, Loss 3.0492234230041504\n",
      "\t Params: tensor([  5.1599, -16.1286])\n",
      "\t Grad: tensor([-0.0354,  0.2001])\n",
      "Epoch 1589, Loss 3.0488107204437256\n",
      "\t Params: tensor([  5.1603, -16.1306])\n",
      "\t Grad: tensor([-0.0353,  0.1998])\n",
      "Epoch 1590, Loss 3.0483977794647217\n",
      "\t Params: tensor([  5.1607, -16.1326])\n",
      "\t Grad: tensor([-0.0353,  0.1995])\n",
      "Epoch 1591, Loss 3.0479910373687744\n",
      "\t Params: tensor([  5.1610, -16.1346])\n",
      "\t Grad: tensor([-0.0352,  0.1991])\n",
      "Epoch 1592, Loss 3.047581434249878\n",
      "\t Params: tensor([  5.1614, -16.1366])\n",
      "\t Grad: tensor([-0.0351,  0.1988])\n",
      "Epoch 1593, Loss 3.047173261642456\n",
      "\t Params: tensor([  5.1617, -16.1386])\n",
      "\t Grad: tensor([-0.0351,  0.1984])\n",
      "Epoch 1594, Loss 3.0467679500579834\n",
      "\t Params: tensor([  5.1621, -16.1406])\n",
      "\t Grad: tensor([-0.0350,  0.1981])\n",
      "Epoch 1595, Loss 3.0463624000549316\n",
      "\t Params: tensor([  5.1624, -16.1425])\n",
      "\t Grad: tensor([-0.0349,  0.1978])\n",
      "Epoch 1596, Loss 3.045959711074829\n",
      "\t Params: tensor([  5.1628, -16.1445])\n",
      "\t Grad: tensor([-0.0349,  0.1974])\n",
      "Epoch 1597, Loss 3.0455591678619385\n",
      "\t Params: tensor([  5.1631, -16.1465])\n",
      "\t Grad: tensor([-0.0348,  0.1971])\n",
      "Epoch 1598, Loss 3.0451598167419434\n",
      "\t Params: tensor([  5.1635, -16.1485])\n",
      "\t Grad: tensor([-0.0348,  0.1968])\n",
      "Epoch 1599, Loss 3.0447587966918945\n",
      "\t Params: tensor([  5.1638, -16.1504])\n",
      "\t Grad: tensor([-0.0347,  0.1964])\n",
      "Epoch 1600, Loss 3.0443613529205322\n",
      "\t Params: tensor([  5.1641, -16.1524])\n",
      "\t Grad: tensor([-0.0346,  0.1961])\n",
      "Epoch 1601, Loss 3.043966054916382\n",
      "\t Params: tensor([  5.1645, -16.1543])\n",
      "\t Grad: tensor([-0.0346,  0.1958])\n",
      "Epoch 1602, Loss 3.0435714721679688\n",
      "\t Params: tensor([  5.1648, -16.1563])\n",
      "\t Grad: tensor([-0.0345,  0.1954])\n",
      "Epoch 1603, Loss 3.0431761741638184\n",
      "\t Params: tensor([  5.1652, -16.1582])\n",
      "\t Grad: tensor([-0.0345,  0.1951])\n",
      "Epoch 1604, Loss 3.0427849292755127\n",
      "\t Params: tensor([  5.1655, -16.1602])\n",
      "\t Grad: tensor([-0.0344,  0.1948])\n",
      "Epoch 1605, Loss 3.0423948764801025\n",
      "\t Params: tensor([  5.1659, -16.1621])\n",
      "\t Grad: tensor([-0.0343,  0.1944])\n",
      "Epoch 1606, Loss 3.0420045852661133\n",
      "\t Params: tensor([  5.1662, -16.1641])\n",
      "\t Grad: tensor([-0.0343,  0.1941])\n",
      "Epoch 1607, Loss 3.0416154861450195\n",
      "\t Params: tensor([  5.1666, -16.1660])\n",
      "\t Grad: tensor([-0.0342,  0.1938])\n",
      "Epoch 1608, Loss 3.0412299633026123\n",
      "\t Params: tensor([  5.1669, -16.1680])\n",
      "\t Grad: tensor([-0.0342,  0.1934])\n",
      "Epoch 1609, Loss 3.040844202041626\n",
      "\t Params: tensor([  5.1672, -16.1699])\n",
      "\t Grad: tensor([-0.0341,  0.1931])\n",
      "Epoch 1610, Loss 3.0404608249664307\n",
      "\t Params: tensor([  5.1676, -16.1718])\n",
      "\t Grad: tensor([-0.0341,  0.1928])\n",
      "Epoch 1611, Loss 3.040076971054077\n",
      "\t Params: tensor([  5.1679, -16.1737])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad: tensor([-0.0340,  0.1925])\n",
      "Epoch 1612, Loss 3.0396950244903564\n",
      "\t Params: tensor([  5.1683, -16.1757])\n",
      "\t Grad: tensor([-0.0339,  0.1921])\n",
      "Epoch 1613, Loss 3.039314031600952\n",
      "\t Params: tensor([  5.1686, -16.1776])\n",
      "\t Grad: tensor([-0.0339,  0.1918])\n",
      "Epoch 1614, Loss 3.038933753967285\n",
      "\t Params: tensor([  5.1689, -16.1795])\n",
      "\t Grad: tensor([-0.0338,  0.1915])\n",
      "Epoch 1615, Loss 3.0385565757751465\n",
      "\t Params: tensor([  5.1693, -16.1814])\n",
      "\t Grad: tensor([-0.0338,  0.1912])\n",
      "Epoch 1616, Loss 3.0381813049316406\n",
      "\t Params: tensor([  5.1696, -16.1833])\n",
      "\t Grad: tensor([-0.0337,  0.1908])\n",
      "Epoch 1617, Loss 3.0378053188323975\n",
      "\t Params: tensor([  5.1699, -16.1852])\n",
      "\t Grad: tensor([-0.0337,  0.1905])\n",
      "Epoch 1618, Loss 3.0374319553375244\n",
      "\t Params: tensor([  5.1703, -16.1871])\n",
      "\t Grad: tensor([-0.0336,  0.1902])\n",
      "Epoch 1619, Loss 3.0370588302612305\n",
      "\t Params: tensor([  5.1706, -16.1890])\n",
      "\t Grad: tensor([-0.0335,  0.1899])\n",
      "Epoch 1620, Loss 3.036688804626465\n",
      "\t Params: tensor([  5.1710, -16.1909])\n",
      "\t Grad: tensor([-0.0335,  0.1895])\n",
      "Epoch 1621, Loss 3.036318778991699\n",
      "\t Params: tensor([  5.1713, -16.1928])\n",
      "\t Grad: tensor([-0.0334,  0.1892])\n",
      "Epoch 1622, Loss 3.0359489917755127\n",
      "\t Params: tensor([  5.1716, -16.1947])\n",
      "\t Grad: tensor([-0.0334,  0.1889])\n",
      "Epoch 1623, Loss 3.0355825424194336\n",
      "\t Params: tensor([  5.1720, -16.1966])\n",
      "\t Grad: tensor([-0.0333,  0.1886])\n",
      "Epoch 1624, Loss 3.0352156162261963\n",
      "\t Params: tensor([  5.1723, -16.1985])\n",
      "\t Grad: tensor([-0.0333,  0.1883])\n",
      "Epoch 1625, Loss 3.0348494052886963\n",
      "\t Params: tensor([  5.1726, -16.2003])\n",
      "\t Grad: tensor([-0.0332,  0.1879])\n",
      "Epoch 1626, Loss 3.03448486328125\n",
      "\t Params: tensor([  5.1729, -16.2022])\n",
      "\t Grad: tensor([-0.0331,  0.1876])\n",
      "Epoch 1627, Loss 3.0341227054595947\n",
      "\t Params: tensor([  5.1733, -16.2041])\n",
      "\t Grad: tensor([-0.0331,  0.1873])\n",
      "Epoch 1628, Loss 3.033762216567993\n",
      "\t Params: tensor([  5.1736, -16.2060])\n",
      "\t Grad: tensor([-0.0330,  0.1870])\n",
      "Epoch 1629, Loss 3.0334017276763916\n",
      "\t Params: tensor([  5.1739, -16.2078])\n",
      "\t Grad: tensor([-0.0330,  0.1867])\n",
      "Epoch 1630, Loss 3.033041477203369\n",
      "\t Params: tensor([  5.1743, -16.2097])\n",
      "\t Grad: tensor([-0.0329,  0.1863])\n",
      "Epoch 1631, Loss 3.0326850414276123\n",
      "\t Params: tensor([  5.1746, -16.2116])\n",
      "\t Grad: tensor([-0.0329,  0.1860])\n",
      "Epoch 1632, Loss 3.0323286056518555\n",
      "\t Params: tensor([  5.1749, -16.2134])\n",
      "\t Grad: tensor([-0.0328,  0.1857])\n",
      "Epoch 1633, Loss 3.031973361968994\n",
      "\t Params: tensor([  5.1753, -16.2153])\n",
      "\t Grad: tensor([-0.0327,  0.1854])\n",
      "Epoch 1634, Loss 3.0316193103790283\n",
      "\t Params: tensor([  5.1756, -16.2171])\n",
      "\t Grad: tensor([-0.0327,  0.1851])\n",
      "Epoch 1635, Loss 3.0312652587890625\n",
      "\t Params: tensor([  5.1759, -16.2190])\n",
      "\t Grad: tensor([-0.0326,  0.1848])\n",
      "Epoch 1636, Loss 3.0309133529663086\n",
      "\t Params: tensor([  5.1762, -16.2208])\n",
      "\t Grad: tensor([-0.0326,  0.1845])\n",
      "Epoch 1637, Loss 3.030564308166504\n",
      "\t Params: tensor([  5.1766, -16.2226])\n",
      "\t Grad: tensor([-0.0325,  0.1841])\n",
      "Epoch 1638, Loss 3.030214548110962\n",
      "\t Params: tensor([  5.1769, -16.2245])\n",
      "\t Grad: tensor([-0.0325,  0.1838])\n",
      "Epoch 1639, Loss 3.0298666954040527\n",
      "\t Params: tensor([  5.1772, -16.2263])\n",
      "\t Grad: tensor([-0.0324,  0.1835])\n",
      "Epoch 1640, Loss 3.0295183658599854\n",
      "\t Params: tensor([  5.1775, -16.2282])\n",
      "\t Grad: tensor([-0.0324,  0.1832])\n",
      "Epoch 1641, Loss 3.029172658920288\n",
      "\t Params: tensor([  5.1779, -16.2300])\n",
      "\t Grad: tensor([-0.0323,  0.1829])\n",
      "Epoch 1642, Loss 3.0288283824920654\n",
      "\t Params: tensor([  5.1782, -16.2318])\n",
      "\t Grad: tensor([-0.0323,  0.1826])\n",
      "Epoch 1643, Loss 3.0284860134124756\n",
      "\t Params: tensor([  5.1785, -16.2336])\n",
      "\t Grad: tensor([-0.0322,  0.1823])\n",
      "Epoch 1644, Loss 3.028141975402832\n",
      "\t Params: tensor([  5.1788, -16.2355])\n",
      "\t Grad: tensor([-0.0321,  0.1820])\n",
      "Epoch 1645, Loss 3.027801752090454\n",
      "\t Params: tensor([  5.1791, -16.2373])\n",
      "\t Grad: tensor([-0.0321,  0.1817])\n",
      "Epoch 1646, Loss 3.0274627208709717\n",
      "\t Params: tensor([  5.1795, -16.2391])\n",
      "\t Grad: tensor([-0.0320,  0.1813])\n",
      "Epoch 1647, Loss 3.0271222591400146\n",
      "\t Params: tensor([  5.1798, -16.2409])\n",
      "\t Grad: tensor([-0.0320,  0.1810])\n",
      "Epoch 1648, Loss 3.0267839431762695\n",
      "\t Params: tensor([  5.1801, -16.2427])\n",
      "\t Grad: tensor([-0.0319,  0.1807])\n",
      "Epoch 1649, Loss 3.026447057723999\n",
      "\t Params: tensor([  5.1804, -16.2445])\n",
      "\t Grad: tensor([-0.0319,  0.1804])\n",
      "Epoch 1650, Loss 3.026111364364624\n",
      "\t Params: tensor([  5.1807, -16.2463])\n",
      "\t Grad: tensor([-0.0318,  0.1801])\n",
      "Epoch 1651, Loss 3.0257797241210938\n",
      "\t Params: tensor([  5.1811, -16.2481])\n",
      "\t Grad: tensor([-0.0318,  0.1798])\n",
      "Epoch 1652, Loss 3.025446891784668\n",
      "\t Params: tensor([  5.1814, -16.2499])\n",
      "\t Grad: tensor([-0.0317,  0.1795])\n",
      "Epoch 1653, Loss 3.025113821029663\n",
      "\t Params: tensor([  5.1817, -16.2517])\n",
      "\t Grad: tensor([-0.0317,  0.1792])\n",
      "Epoch 1654, Loss 3.0247817039489746\n",
      "\t Params: tensor([  5.1820, -16.2535])\n",
      "\t Grad: tensor([-0.0316,  0.1789])\n",
      "Epoch 1655, Loss 3.0244522094726562\n",
      "\t Params: tensor([  5.1823, -16.2553])\n",
      "\t Grad: tensor([-0.0316,  0.1786])\n",
      "Epoch 1656, Loss 3.02412486076355\n",
      "\t Params: tensor([  5.1826, -16.2570])\n",
      "\t Grad: tensor([-0.0315,  0.1783])\n",
      "Epoch 1657, Loss 3.023796319961548\n",
      "\t Params: tensor([  5.1829, -16.2588])\n",
      "\t Grad: tensor([-0.0315,  0.1780])\n",
      "Epoch 1658, Loss 3.0234711170196533\n",
      "\t Params: tensor([  5.1833, -16.2606])\n",
      "\t Grad: tensor([-0.0314,  0.1777])\n",
      "Epoch 1659, Loss 3.0231454372406006\n",
      "\t Params: tensor([  5.1836, -16.2624])\n",
      "\t Grad: tensor([-0.0313,  0.1774])\n",
      "Epoch 1660, Loss 3.022820472717285\n",
      "\t Params: tensor([  5.1839, -16.2641])\n",
      "\t Grad: tensor([-0.0313,  0.1771])\n",
      "Epoch 1661, Loss 3.022498369216919\n",
      "\t Params: tensor([  5.1842, -16.2659])\n",
      "\t Grad: tensor([-0.0312,  0.1768])\n",
      "Epoch 1662, Loss 3.022176504135132\n",
      "\t Params: tensor([  5.1845, -16.2677])\n",
      "\t Grad: tensor([-0.0312,  0.1765])\n",
      "Epoch 1663, Loss 3.0218546390533447\n",
      "\t Params: tensor([  5.1848, -16.2694])\n",
      "\t Grad: tensor([-0.0311,  0.1762])\n",
      "Epoch 1664, Loss 3.021533966064453\n",
      "\t Params: tensor([  5.1851, -16.2712])\n",
      "\t Grad: tensor([-0.0311,  0.1759])\n",
      "Epoch 1665, Loss 3.021216630935669\n",
      "\t Params: tensor([  5.1854, -16.2730])\n",
      "\t Grad: tensor([-0.0310,  0.1756])\n",
      "Epoch 1666, Loss 3.0208983421325684\n",
      "\t Params: tensor([  5.1858, -16.2747])\n",
      "\t Grad: tensor([-0.0310,  0.1753])\n",
      "Epoch 1667, Loss 3.020582437515259\n",
      "\t Params: tensor([  5.1861, -16.2765])\n",
      "\t Grad: tensor([-0.0309,  0.1750])\n",
      "Epoch 1668, Loss 3.0202653408050537\n",
      "\t Params: tensor([  5.1864, -16.2782])\n",
      "\t Grad: tensor([-0.0309,  0.1747])\n",
      "Epoch 1669, Loss 3.019951820373535\n",
      "\t Params: tensor([  5.1867, -16.2800])\n",
      "\t Grad: tensor([-0.0308,  0.1744])\n",
      "Epoch 1670, Loss 3.019639492034912\n",
      "\t Params: tensor([  5.1870, -16.2817])\n",
      "\t Grad: tensor([-0.0308,  0.1741])\n",
      "Epoch 1671, Loss 3.0193254947662354\n",
      "\t Params: tensor([  5.1873, -16.2834])\n",
      "\t Grad: tensor([-0.0307,  0.1738])\n",
      "Epoch 1672, Loss 3.0190162658691406\n",
      "\t Params: tensor([  5.1876, -16.2852])\n",
      "\t Grad: tensor([-0.0307,  0.1735])\n",
      "Epoch 1673, Loss 3.0187063217163086\n",
      "\t Params: tensor([  5.1879, -16.2869])\n",
      "\t Grad: tensor([-0.0306,  0.1732])\n",
      "Epoch 1674, Loss 3.018394708633423\n",
      "\t Params: tensor([  5.1882, -16.2886])\n",
      "\t Grad: tensor([-0.0305,  0.1729])\n",
      "Epoch 1675, Loss 3.0180890560150146\n",
      "\t Params: tensor([  5.1885, -16.2904])\n",
      "\t Grad: tensor([-0.0305,  0.1726])\n",
      "Epoch 1676, Loss 3.017779588699341\n",
      "\t Params: tensor([  5.1888, -16.2921])\n",
      "\t Grad: tensor([-0.0304,  0.1723])\n",
      "Epoch 1677, Loss 3.017474889755249\n",
      "\t Params: tensor([  5.1891, -16.2938])\n",
      "\t Grad: tensor([-0.0304,  0.1720])\n",
      "Epoch 1678, Loss 3.017169713973999\n",
      "\t Params: tensor([  5.1894, -16.2955])\n",
      "\t Grad: tensor([-0.0303,  0.1717])\n",
      "Epoch 1679, Loss 3.016867160797119\n",
      "\t Params: tensor([  5.1897, -16.2972])\n",
      "\t Grad: tensor([-0.0303,  0.1715])\n",
      "Epoch 1680, Loss 3.016563653945923\n",
      "\t Params: tensor([  5.1900, -16.2989])\n",
      "\t Grad: tensor([-0.0302,  0.1712])\n",
      "Epoch 1681, Loss 3.0162618160247803\n",
      "\t Params: tensor([  5.1903, -16.3006])\n",
      "\t Grad: tensor([-0.0302,  0.1709])\n",
      "Epoch 1682, Loss 3.0159590244293213\n",
      "\t Params: tensor([  5.1906, -16.3024])\n",
      "\t Grad: tensor([-0.0301,  0.1706])\n",
      "Epoch 1683, Loss 3.0156617164611816\n",
      "\t Params: tensor([  5.1909, -16.3041])\n",
      "\t Grad: tensor([-0.0301,  0.1703])\n",
      "Epoch 1684, Loss 3.0153610706329346\n",
      "\t Params: tensor([  5.1912, -16.3058])\n",
      "\t Grad: tensor([-0.0300,  0.1700])\n",
      "Epoch 1685, Loss 3.015064001083374\n",
      "\t Params: tensor([  5.1915, -16.3075])\n",
      "\t Grad: tensor([-0.0300,  0.1697])\n",
      "Epoch 1686, Loss 3.014767646789551\n",
      "\t Params: tensor([  5.1918, -16.3091])\n",
      "\t Grad: tensor([-0.0299,  0.1694])\n",
      "Epoch 1687, Loss 3.014472246170044\n",
      "\t Params: tensor([  5.1921, -16.3108])\n",
      "\t Grad: tensor([-0.0299,  0.1691])\n",
      "Epoch 1688, Loss 3.014178991317749\n",
      "\t Params: tensor([  5.1924, -16.3125])\n",
      "\t Grad: tensor([-0.0298,  0.1688])\n",
      "Epoch 1689, Loss 3.0138838291168213\n",
      "\t Params: tensor([  5.1927, -16.3142])\n",
      "\t Grad: tensor([-0.0298,  0.1686])\n",
      "Epoch 1690, Loss 3.0135910511016846\n",
      "\t Params: tensor([  5.1930, -16.3159])\n",
      "\t Grad: tensor([-0.0297,  0.1683])\n",
      "Epoch 1691, Loss 3.0132994651794434\n",
      "\t Params: tensor([  5.1933, -16.3176])\n",
      "\t Grad: tensor([-0.0297,  0.1680])\n",
      "Epoch 1692, Loss 3.013007879257202\n",
      "\t Params: tensor([  5.1936, -16.3193])\n",
      "\t Grad: tensor([-0.0296,  0.1677])\n",
      "Epoch 1693, Loss 3.012718915939331\n",
      "\t Params: tensor([  5.1939, -16.3209])\n",
      "\t Grad: tensor([-0.0296,  0.1674])\n",
      "Epoch 1694, Loss 3.0124306678771973\n",
      "\t Params: tensor([  5.1942, -16.3226])\n",
      "\t Grad: tensor([-0.0295,  0.1671])\n",
      "Epoch 1695, Loss 3.0121407508850098\n",
      "\t Params: tensor([  5.1945, -16.3243])\n",
      "\t Grad: tensor([-0.0295,  0.1668])\n",
      "Epoch 1696, Loss 3.011855125427246\n",
      "\t Params: tensor([  5.1948, -16.3259])\n",
      "\t Grad: tensor([-0.0294,  0.1666])\n",
      "Epoch 1697, Loss 3.0115699768066406\n",
      "\t Params: tensor([  5.1951, -16.3276])\n",
      "\t Grad: tensor([-0.0294,  0.1663])\n",
      "Epoch 1698, Loss 3.0112838745117188\n",
      "\t Params: tensor([  5.1954, -16.3293])\n",
      "\t Grad: tensor([-0.0293,  0.1660])\n",
      "Epoch 1699, Loss 3.0110013484954834\n",
      "\t Params: tensor([  5.1957, -16.3309])\n",
      "\t Grad: tensor([-0.0293,  0.1657])\n",
      "Epoch 1700, Loss 3.01071834564209\n",
      "\t Params: tensor([  5.1960, -16.3326])\n",
      "\t Grad: tensor([-0.0292,  0.1654])\n",
      "Epoch 1701, Loss 3.0104362964630127\n",
      "\t Params: tensor([  5.1963, -16.3342])\n",
      "\t Grad: tensor([-0.0292,  0.1652])\n",
      "Epoch 1702, Loss 3.01015567779541\n",
      "\t Params: tensor([  5.1966, -16.3359])\n",
      "\t Grad: tensor([-0.0291,  0.1649])\n",
      "Epoch 1703, Loss 3.009875535964966\n",
      "\t Params: tensor([  5.1968, -16.3375])\n",
      "\t Grad: tensor([-0.0291,  0.1646])\n",
      "Epoch 1704, Loss 3.0095953941345215\n",
      "\t Params: tensor([  5.1971, -16.3392])\n",
      "\t Grad: tensor([-0.0290,  0.1643])\n",
      "Epoch 1705, Loss 3.0093190670013428\n",
      "\t Params: tensor([  5.1974, -16.3408])\n",
      "\t Grad: tensor([-0.0290,  0.1640])\n",
      "Epoch 1706, Loss 3.009039878845215\n",
      "\t Params: tensor([  5.1977, -16.3424])\n",
      "\t Grad: tensor([-0.0289,  0.1638])\n",
      "Epoch 1707, Loss 3.008763313293457\n",
      "\t Params: tensor([  5.1980, -16.3441])\n",
      "\t Grad: tensor([-0.0289,  0.1635])\n",
      "Epoch 1708, Loss 3.0084874629974365\n",
      "\t Params: tensor([  5.1983, -16.3457])\n",
      "\t Grad: tensor([-0.0288,  0.1632])\n",
      "Epoch 1709, Loss 3.0082147121429443\n",
      "\t Params: tensor([  5.1986, -16.3473])\n",
      "\t Grad: tensor([-0.0288,  0.1629])\n",
      "Epoch 1710, Loss 3.0079407691955566\n",
      "\t Params: tensor([  5.1989, -16.3490])\n",
      "\t Grad: tensor([-0.0287,  0.1626])\n",
      "Epoch 1711, Loss 3.0076675415039062\n",
      "\t Params: tensor([  5.1992, -16.3506])\n",
      "\t Grad: tensor([-0.0287,  0.1624])\n",
      "Epoch 1712, Loss 3.007396697998047\n",
      "\t Params: tensor([  5.1994, -16.3522])\n",
      "\t Grad: tensor([-0.0286,  0.1621])\n",
      "Epoch 1713, Loss 3.0071260929107666\n",
      "\t Params: tensor([  5.1997, -16.3538])\n",
      "\t Grad: tensor([-0.0286,  0.1618])\n",
      "Epoch 1714, Loss 3.006856679916382\n",
      "\t Params: tensor([  5.2000, -16.3554])\n",
      "\t Grad: tensor([-0.0285,  0.1615])\n",
      "Epoch 1715, Loss 3.0065863132476807\n",
      "\t Params: tensor([  5.2003, -16.3570])\n",
      "\t Grad: tensor([-0.0285,  0.1613])\n",
      "Epoch 1716, Loss 3.0063180923461914\n",
      "\t Params: tensor([  5.2006, -16.3587])\n",
      "\t Grad: tensor([-0.0284,  0.1610])\n",
      "Epoch 1717, Loss 3.006052255630493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params: tensor([  5.2009, -16.3603])\n",
      "\t Grad: tensor([-0.0284,  0.1607])\n",
      "Epoch 1718, Loss 3.0057852268218994\n",
      "\t Params: tensor([  5.2012, -16.3619])\n",
      "\t Grad: tensor([-0.0284,  0.1604])\n",
      "Epoch 1719, Loss 3.0055205821990967\n",
      "\t Params: tensor([  5.2014, -16.3635])\n",
      "\t Grad: tensor([-0.0283,  0.1602])\n",
      "Epoch 1720, Loss 3.005256414413452\n",
      "\t Params: tensor([  5.2017, -16.3651])\n",
      "\t Grad: tensor([-0.0283,  0.1599])\n",
      "Epoch 1721, Loss 3.004993200302124\n",
      "\t Params: tensor([  5.2020, -16.3667])\n",
      "\t Grad: tensor([-0.0282,  0.1596])\n",
      "Epoch 1722, Loss 3.0047292709350586\n",
      "\t Params: tensor([  5.2023, -16.3683])\n",
      "\t Grad: tensor([-0.0281,  0.1594])\n",
      "Epoch 1723, Loss 3.0044667720794678\n",
      "\t Params: tensor([  5.2026, -16.3699])\n",
      "\t Grad: tensor([-0.0281,  0.1591])\n",
      "Epoch 1724, Loss 3.004206895828247\n",
      "\t Params: tensor([  5.2028, -16.3714])\n",
      "\t Grad: tensor([-0.0280,  0.1588])\n",
      "Epoch 1725, Loss 3.0039467811584473\n",
      "\t Params: tensor([  5.2031, -16.3730])\n",
      "\t Grad: tensor([-0.0280,  0.1586])\n",
      "Epoch 1726, Loss 3.003689765930176\n",
      "\t Params: tensor([  5.2034, -16.3746])\n",
      "\t Grad: tensor([-0.0280,  0.1583])\n",
      "Epoch 1727, Loss 3.0034308433532715\n",
      "\t Params: tensor([  5.2037, -16.3762])\n",
      "\t Grad: tensor([-0.0279,  0.1580])\n",
      "Epoch 1728, Loss 3.003173589706421\n",
      "\t Params: tensor([  5.2040, -16.3778])\n",
      "\t Grad: tensor([-0.0279,  0.1577])\n",
      "Epoch 1729, Loss 3.002917528152466\n",
      "\t Params: tensor([  5.2042, -16.3793])\n",
      "\t Grad: tensor([-0.0278,  0.1575])\n",
      "Epoch 1730, Loss 3.0026609897613525\n",
      "\t Params: tensor([  5.2045, -16.3809])\n",
      "\t Grad: tensor([-0.0278,  0.1572])\n",
      "Epoch 1731, Loss 3.002406120300293\n",
      "\t Params: tensor([  5.2048, -16.3825])\n",
      "\t Grad: tensor([-0.0277,  0.1569])\n",
      "Epoch 1732, Loss 3.00215220451355\n",
      "\t Params: tensor([  5.2051, -16.3840])\n",
      "\t Grad: tensor([-0.0277,  0.1567])\n",
      "Epoch 1733, Loss 3.001901149749756\n",
      "\t Params: tensor([  5.2053, -16.3856])\n",
      "\t Grad: tensor([-0.0276,  0.1564])\n",
      "Epoch 1734, Loss 3.0016493797302246\n",
      "\t Params: tensor([  5.2056, -16.3872])\n",
      "\t Grad: tensor([-0.0276,  0.1561])\n",
      "Epoch 1735, Loss 3.0013954639434814\n",
      "\t Params: tensor([  5.2059, -16.3887])\n",
      "\t Grad: tensor([-0.0275,  0.1559])\n",
      "Epoch 1736, Loss 3.0011446475982666\n",
      "\t Params: tensor([  5.2062, -16.3903])\n",
      "\t Grad: tensor([-0.0275,  0.1556])\n",
      "Epoch 1737, Loss 3.0008978843688965\n",
      "\t Params: tensor([  5.2064, -16.3918])\n",
      "\t Grad: tensor([-0.0274,  0.1553])\n",
      "Epoch 1738, Loss 3.000647783279419\n",
      "\t Params: tensor([  5.2067, -16.3934])\n",
      "\t Grad: tensor([-0.0274,  0.1551])\n",
      "Epoch 1739, Loss 3.0004003047943115\n",
      "\t Params: tensor([  5.2070, -16.3949])\n",
      "\t Grad: tensor([-0.0273,  0.1548])\n",
      "Epoch 1740, Loss 3.0001542568206787\n",
      "\t Params: tensor([  5.2073, -16.3965])\n",
      "\t Grad: tensor([-0.0273,  0.1546])\n",
      "Epoch 1741, Loss 2.9999074935913086\n",
      "\t Params: tensor([  5.2075, -16.3980])\n",
      "\t Grad: tensor([-0.0273,  0.1543])\n",
      "Epoch 1742, Loss 2.999662160873413\n",
      "\t Params: tensor([  5.2078, -16.3996])\n",
      "\t Grad: tensor([-0.0272,  0.1540])\n",
      "Epoch 1743, Loss 2.999417304992676\n",
      "\t Params: tensor([  5.2081, -16.4011])\n",
      "\t Grad: tensor([-0.0272,  0.1538])\n",
      "Epoch 1744, Loss 2.999173641204834\n",
      "\t Params: tensor([  5.2084, -16.4026])\n",
      "\t Grad: tensor([-0.0271,  0.1535])\n",
      "Epoch 1745, Loss 2.998929738998413\n",
      "\t Params: tensor([  5.2086, -16.4042])\n",
      "\t Grad: tensor([-0.0271,  0.1533])\n",
      "Epoch 1746, Loss 2.998687744140625\n",
      "\t Params: tensor([  5.2089, -16.4057])\n",
      "\t Grad: tensor([-0.0270,  0.1530])\n",
      "Epoch 1747, Loss 2.998448133468628\n",
      "\t Params: tensor([  5.2092, -16.4072])\n",
      "\t Grad: tensor([-0.0270,  0.1527])\n",
      "Epoch 1748, Loss 2.9982080459594727\n",
      "\t Params: tensor([  5.2094, -16.4088])\n",
      "\t Grad: tensor([-0.0269,  0.1525])\n",
      "Epoch 1749, Loss 2.9979681968688965\n",
      "\t Params: tensor([  5.2097, -16.4103])\n",
      "\t Grad: tensor([-0.0269,  0.1522])\n",
      "Epoch 1750, Loss 2.997730016708374\n",
      "\t Params: tensor([  5.2100, -16.4118])\n",
      "\t Grad: tensor([-0.0268,  0.1520])\n",
      "Epoch 1751, Loss 2.997490167617798\n",
      "\t Params: tensor([  5.2102, -16.4133])\n",
      "\t Grad: tensor([-0.0268,  0.1517])\n",
      "Epoch 1752, Loss 2.997253656387329\n",
      "\t Params: tensor([  5.2105, -16.4148])\n",
      "\t Grad: tensor([-0.0267,  0.1514])\n",
      "Epoch 1753, Loss 2.9970178604125977\n",
      "\t Params: tensor([  5.2108, -16.4163])\n",
      "\t Grad: tensor([-0.0267,  0.1512])\n",
      "Epoch 1754, Loss 2.996782064437866\n",
      "\t Params: tensor([  5.2110, -16.4179])\n",
      "\t Grad: tensor([-0.0266,  0.1509])\n",
      "Epoch 1755, Loss 2.9965476989746094\n",
      "\t Params: tensor([  5.2113, -16.4194])\n",
      "\t Grad: tensor([-0.0266,  0.1507])\n",
      "Epoch 1756, Loss 2.9963133335113525\n",
      "\t Params: tensor([  5.2116, -16.4209])\n",
      "\t Grad: tensor([-0.0266,  0.1504])\n",
      "Epoch 1757, Loss 2.9960806369781494\n",
      "\t Params: tensor([  5.2118, -16.4224])\n",
      "\t Grad: tensor([-0.0265,  0.1502])\n",
      "Epoch 1758, Loss 2.995847463607788\n",
      "\t Params: tensor([  5.2121, -16.4239])\n",
      "\t Grad: tensor([-0.0265,  0.1499])\n",
      "Epoch 1759, Loss 2.9956154823303223\n",
      "\t Params: tensor([  5.2124, -16.4254])\n",
      "\t Grad: tensor([-0.0264,  0.1496])\n",
      "Epoch 1760, Loss 2.9953866004943848\n",
      "\t Params: tensor([  5.2126, -16.4269])\n",
      "\t Grad: tensor([-0.0264,  0.1494])\n",
      "Epoch 1761, Loss 2.9951562881469727\n",
      "\t Params: tensor([  5.2129, -16.4283])\n",
      "\t Grad: tensor([-0.0263,  0.1491])\n",
      "Epoch 1762, Loss 2.9949281215667725\n",
      "\t Params: tensor([  5.2132, -16.4298])\n",
      "\t Grad: tensor([-0.0263,  0.1489])\n",
      "Epoch 1763, Loss 2.994699239730835\n",
      "\t Params: tensor([  5.2134, -16.4313])\n",
      "\t Grad: tensor([-0.0263,  0.1486])\n",
      "Epoch 1764, Loss 2.9944710731506348\n",
      "\t Params: tensor([  5.2137, -16.4328])\n",
      "\t Grad: tensor([-0.0262,  0.1484])\n",
      "Epoch 1765, Loss 2.9942452907562256\n",
      "\t Params: tensor([  5.2139, -16.4343])\n",
      "\t Grad: tensor([-0.0262,  0.1481])\n",
      "Epoch 1766, Loss 2.9940185546875\n",
      "\t Params: tensor([  5.2142, -16.4358])\n",
      "\t Grad: tensor([-0.0261,  0.1479])\n",
      "Epoch 1767, Loss 2.9937944412231445\n",
      "\t Params: tensor([  5.2145, -16.4372])\n",
      "\t Grad: tensor([-0.0261,  0.1476])\n",
      "Epoch 1768, Loss 2.9935693740844727\n",
      "\t Params: tensor([  5.2147, -16.4387])\n",
      "\t Grad: tensor([-0.0260,  0.1474])\n",
      "Epoch 1769, Loss 2.993344306945801\n",
      "\t Params: tensor([  5.2150, -16.4402])\n",
      "\t Grad: tensor([-0.0260,  0.1471])\n",
      "Epoch 1770, Loss 2.9931211471557617\n",
      "\t Params: tensor([  5.2152, -16.4417])\n",
      "\t Grad: tensor([-0.0260,  0.1469])\n",
      "Epoch 1771, Loss 2.9929001331329346\n",
      "\t Params: tensor([  5.2155, -16.4431])\n",
      "\t Grad: tensor([-0.0259,  0.1466])\n",
      "Epoch 1772, Loss 2.992677927017212\n",
      "\t Params: tensor([  5.2158, -16.4446])\n",
      "\t Grad: tensor([-0.0259,  0.1464])\n",
      "Epoch 1773, Loss 2.992457389831543\n",
      "\t Params: tensor([  5.2160, -16.4460])\n",
      "\t Grad: tensor([-0.0258,  0.1461])\n",
      "Epoch 1774, Loss 2.9922373294830322\n",
      "\t Params: tensor([  5.2163, -16.4475])\n",
      "\t Grad: tensor([-0.0258,  0.1459])\n",
      "Epoch 1775, Loss 2.9920167922973633\n",
      "\t Params: tensor([  5.2165, -16.4490])\n",
      "\t Grad: tensor([-0.0257,  0.1456])\n",
      "Epoch 1776, Loss 2.991797685623169\n",
      "\t Params: tensor([  5.2168, -16.4504])\n",
      "\t Grad: tensor([-0.0257,  0.1454])\n",
      "Epoch 1777, Loss 2.9915823936462402\n",
      "\t Params: tensor([  5.2170, -16.4519])\n",
      "\t Grad: tensor([-0.0256,  0.1451])\n",
      "Epoch 1778, Loss 2.991365671157837\n",
      "\t Params: tensor([  5.2173, -16.4533])\n",
      "\t Grad: tensor([-0.0256,  0.1449])\n",
      "Epoch 1779, Loss 2.9911460876464844\n",
      "\t Params: tensor([  5.2176, -16.4548])\n",
      "\t Grad: tensor([-0.0256,  0.1446])\n",
      "Epoch 1780, Loss 2.990931510925293\n",
      "\t Params: tensor([  5.2178, -16.4562])\n",
      "\t Grad: tensor([-0.0255,  0.1444])\n",
      "Epoch 1781, Loss 2.9907188415527344\n",
      "\t Params: tensor([  5.2181, -16.4576])\n",
      "\t Grad: tensor([-0.0255,  0.1442])\n",
      "Epoch 1782, Loss 2.9905028343200684\n",
      "\t Params: tensor([  5.2183, -16.4591])\n",
      "\t Grad: tensor([-0.0254,  0.1439])\n",
      "Epoch 1783, Loss 2.990288496017456\n",
      "\t Params: tensor([  5.2186, -16.4605])\n",
      "\t Grad: tensor([-0.0254,  0.1437])\n",
      "Epoch 1784, Loss 2.9900782108306885\n",
      "\t Params: tensor([  5.2188, -16.4620])\n",
      "\t Grad: tensor([-0.0253,  0.1434])\n",
      "Epoch 1785, Loss 2.989866018295288\n",
      "\t Params: tensor([  5.2191, -16.4634])\n",
      "\t Grad: tensor([-0.0253,  0.1432])\n",
      "Epoch 1786, Loss 2.989654541015625\n",
      "\t Params: tensor([  5.2193, -16.4648])\n",
      "\t Grad: tensor([-0.0252,  0.1429])\n",
      "Epoch 1787, Loss 2.989443302154541\n",
      "\t Params: tensor([  5.2196, -16.4662])\n",
      "\t Grad: tensor([-0.0252,  0.1427])\n",
      "Epoch 1788, Loss 2.9892327785491943\n",
      "\t Params: tensor([  5.2198, -16.4677])\n",
      "\t Grad: tensor([-0.0252,  0.1424])\n",
      "Epoch 1789, Loss 2.9890248775482178\n",
      "\t Params: tensor([  5.2201, -16.4691])\n",
      "\t Grad: tensor([-0.0251,  0.1422])\n",
      "Epoch 1790, Loss 2.988816976547241\n",
      "\t Params: tensor([  5.2203, -16.4705])\n",
      "\t Grad: tensor([-0.0251,  0.1420])\n",
      "Epoch 1791, Loss 2.9886085987091064\n",
      "\t Params: tensor([  5.2206, -16.4719])\n",
      "\t Grad: tensor([-0.0250,  0.1417])\n",
      "Epoch 1792, Loss 2.988401174545288\n",
      "\t Params: tensor([  5.2208, -16.4733])\n",
      "\t Grad: tensor([-0.0250,  0.1415])\n",
      "Epoch 1793, Loss 2.9881949424743652\n",
      "\t Params: tensor([  5.2211, -16.4748])\n",
      "\t Grad: tensor([-0.0249,  0.1412])\n",
      "Epoch 1794, Loss 2.9879889488220215\n",
      "\t Params: tensor([  5.2213, -16.4762])\n",
      "\t Grad: tensor([-0.0249,  0.1410])\n",
      "Epoch 1795, Loss 2.9877851009368896\n",
      "\t Params: tensor([  5.2216, -16.4776])\n",
      "\t Grad: tensor([-0.0249,  0.1408])\n",
      "Epoch 1796, Loss 2.987581729888916\n",
      "\t Params: tensor([  5.2218, -16.4790])\n",
      "\t Grad: tensor([-0.0248,  0.1405])\n",
      "Epoch 1797, Loss 2.987377166748047\n",
      "\t Params: tensor([  5.2221, -16.4804])\n",
      "\t Grad: tensor([-0.0248,  0.1403])\n",
      "Epoch 1798, Loss 2.987173557281494\n",
      "\t Params: tensor([  5.2223, -16.4818])\n",
      "\t Grad: tensor([-0.0247,  0.1400])\n",
      "Epoch 1799, Loss 2.986973524093628\n",
      "\t Params: tensor([  5.2226, -16.4832])\n",
      "\t Grad: tensor([-0.0247,  0.1398])\n",
      "Epoch 1800, Loss 2.98677134513855\n",
      "\t Params: tensor([  5.2228, -16.4846])\n",
      "\t Grad: tensor([-0.0246,  0.1396])\n",
      "Epoch 1801, Loss 2.986570119857788\n",
      "\t Params: tensor([  5.2231, -16.4860])\n",
      "\t Grad: tensor([-0.0246,  0.1393])\n",
      "Epoch 1802, Loss 2.9863710403442383\n",
      "\t Params: tensor([  5.2233, -16.4874])\n",
      "\t Grad: tensor([-0.0246,  0.1391])\n",
      "Epoch 1803, Loss 2.986170768737793\n",
      "\t Params: tensor([  5.2236, -16.4888])\n",
      "\t Grad: tensor([-0.0245,  0.1389])\n",
      "Epoch 1804, Loss 2.985971689224243\n",
      "\t Params: tensor([  5.2238, -16.4901])\n",
      "\t Grad: tensor([-0.0245,  0.1386])\n",
      "Epoch 1805, Loss 2.985774040222168\n",
      "\t Params: tensor([  5.2241, -16.4915])\n",
      "\t Grad: tensor([-0.0245,  0.1384])\n",
      "Epoch 1806, Loss 2.9855782985687256\n",
      "\t Params: tensor([  5.2243, -16.4929])\n",
      "\t Grad: tensor([-0.0244,  0.1382])\n",
      "Epoch 1807, Loss 2.9853806495666504\n",
      "\t Params: tensor([  5.2245, -16.4943])\n",
      "\t Grad: tensor([-0.0244,  0.1379])\n",
      "Epoch 1808, Loss 2.9851839542388916\n",
      "\t Params: tensor([  5.2248, -16.4957])\n",
      "\t Grad: tensor([-0.0243,  0.1377])\n",
      "Epoch 1809, Loss 2.9849894046783447\n",
      "\t Params: tensor([  5.2250, -16.4970])\n",
      "\t Grad: tensor([-0.0243,  0.1374])\n",
      "Epoch 1810, Loss 2.984792947769165\n",
      "\t Params: tensor([  5.2253, -16.4984])\n",
      "\t Grad: tensor([-0.0243,  0.1372])\n",
      "Epoch 1811, Loss 2.9846010208129883\n",
      "\t Params: tensor([  5.2255, -16.4998])\n",
      "\t Grad: tensor([-0.0242,  0.1370])\n",
      "Epoch 1812, Loss 2.9844069480895996\n",
      "\t Params: tensor([  5.2258, -16.5011])\n",
      "\t Grad: tensor([-0.0242,  0.1368])\n",
      "Epoch 1813, Loss 2.984215021133423\n",
      "\t Params: tensor([  5.2260, -16.5025])\n",
      "\t Grad: tensor([-0.0241,  0.1365])\n",
      "Epoch 1814, Loss 2.9840216636657715\n",
      "\t Params: tensor([  5.2262, -16.5039])\n",
      "\t Grad: tensor([-0.0241,  0.1363])\n",
      "Epoch 1815, Loss 2.9838309288024902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params: tensor([  5.2265, -16.5052])\n",
      "\t Grad: tensor([-0.0240,  0.1361])\n",
      "Epoch 1816, Loss 2.9836390018463135\n",
      "\t Params: tensor([  5.2267, -16.5066])\n",
      "\t Grad: tensor([-0.0240,  0.1358])\n",
      "Epoch 1817, Loss 2.9834494590759277\n",
      "\t Params: tensor([  5.2270, -16.5079])\n",
      "\t Grad: tensor([-0.0239,  0.1356])\n",
      "Epoch 1818, Loss 2.983259439468384\n",
      "\t Params: tensor([  5.2272, -16.5093])\n",
      "\t Grad: tensor([-0.0239,  0.1354])\n",
      "Epoch 1819, Loss 2.983072519302368\n",
      "\t Params: tensor([  5.2274, -16.5107])\n",
      "\t Grad: tensor([-0.0239,  0.1351])\n",
      "Epoch 1820, Loss 2.9828836917877197\n",
      "\t Params: tensor([  5.2277, -16.5120])\n",
      "\t Grad: tensor([-0.0238,  0.1349])\n",
      "Epoch 1821, Loss 2.9826972484588623\n",
      "\t Params: tensor([  5.2279, -16.5133])\n",
      "\t Grad: tensor([-0.0238,  0.1347])\n",
      "Epoch 1822, Loss 2.9825096130371094\n",
      "\t Params: tensor([  5.2281, -16.5147])\n",
      "\t Grad: tensor([-0.0237,  0.1344])\n",
      "Epoch 1823, Loss 2.9823217391967773\n",
      "\t Params: tensor([  5.2284, -16.5160])\n",
      "\t Grad: tensor([-0.0237,  0.1342])\n",
      "Epoch 1824, Loss 2.982137441635132\n",
      "\t Params: tensor([  5.2286, -16.5174])\n",
      "\t Grad: tensor([-0.0237,  0.1340])\n",
      "Epoch 1825, Loss 2.9819529056549072\n",
      "\t Params: tensor([  5.2289, -16.5187])\n",
      "\t Grad: tensor([-0.0236,  0.1338])\n",
      "Epoch 1826, Loss 2.9817686080932617\n",
      "\t Params: tensor([  5.2291, -16.5200])\n",
      "\t Grad: tensor([-0.0236,  0.1335])\n",
      "Epoch 1827, Loss 2.9815855026245117\n",
      "\t Params: tensor([  5.2293, -16.5214])\n",
      "\t Grad: tensor([-0.0236,  0.1333])\n",
      "Epoch 1828, Loss 2.9814023971557617\n",
      "\t Params: tensor([  5.2296, -16.5227])\n",
      "\t Grad: tensor([-0.0235,  0.1331])\n",
      "Epoch 1829, Loss 2.9812192916870117\n",
      "\t Params: tensor([  5.2298, -16.5240])\n",
      "\t Grad: tensor([-0.0235,  0.1329])\n",
      "Epoch 1830, Loss 2.981037139892578\n",
      "\t Params: tensor([  5.2300, -16.5254])\n",
      "\t Grad: tensor([-0.0235,  0.1326])\n",
      "Epoch 1831, Loss 2.980855703353882\n",
      "\t Params: tensor([  5.2303, -16.5267])\n",
      "\t Grad: tensor([-0.0234,  0.1324])\n",
      "Epoch 1832, Loss 2.980675458908081\n",
      "\t Params: tensor([  5.2305, -16.5280])\n",
      "\t Grad: tensor([-0.0234,  0.1322])\n",
      "Epoch 1833, Loss 2.9804954528808594\n",
      "\t Params: tensor([  5.2307, -16.5293])\n",
      "\t Grad: tensor([-0.0233,  0.1320])\n",
      "Epoch 1834, Loss 2.9803154468536377\n",
      "\t Params: tensor([  5.2310, -16.5306])\n",
      "\t Grad: tensor([-0.0233,  0.1317])\n",
      "Epoch 1835, Loss 2.9801371097564697\n",
      "\t Params: tensor([  5.2312, -16.5320])\n",
      "\t Grad: tensor([-0.0232,  0.1315])\n",
      "Epoch 1836, Loss 2.9799582958221436\n",
      "\t Params: tensor([  5.2314, -16.5333])\n",
      "\t Grad: tensor([-0.0232,  0.1313])\n",
      "Epoch 1837, Loss 2.9797821044921875\n",
      "\t Params: tensor([  5.2317, -16.5346])\n",
      "\t Grad: tensor([-0.0232,  0.1311])\n",
      "Epoch 1838, Loss 2.9796042442321777\n",
      "\t Params: tensor([  5.2319, -16.5359])\n",
      "\t Grad: tensor([-0.0231,  0.1308])\n",
      "Epoch 1839, Loss 2.979428291320801\n",
      "\t Params: tensor([  5.2321, -16.5372])\n",
      "\t Grad: tensor([-0.0231,  0.1306])\n",
      "Epoch 1840, Loss 2.979252576828003\n",
      "\t Params: tensor([  5.2324, -16.5385])\n",
      "\t Grad: tensor([-0.0230,  0.1304])\n",
      "Epoch 1841, Loss 2.9790780544281006\n",
      "\t Params: tensor([  5.2326, -16.5398])\n",
      "\t Grad: tensor([-0.0230,  0.1302])\n",
      "Epoch 1842, Loss 2.9789023399353027\n",
      "\t Params: tensor([  5.2328, -16.5411])\n",
      "\t Grad: tensor([-0.0229,  0.1300])\n",
      "Epoch 1843, Loss 2.9787285327911377\n",
      "\t Params: tensor([  5.2330, -16.5424])\n",
      "\t Grad: tensor([-0.0229,  0.1297])\n",
      "Epoch 1844, Loss 2.978555917739868\n",
      "\t Params: tensor([  5.2333, -16.5437])\n",
      "\t Grad: tensor([-0.0229,  0.1295])\n",
      "Epoch 1845, Loss 2.9783823490142822\n",
      "\t Params: tensor([  5.2335, -16.5450])\n",
      "\t Grad: tensor([-0.0228,  0.1293])\n",
      "Epoch 1846, Loss 2.978210687637329\n",
      "\t Params: tensor([  5.2337, -16.5463])\n",
      "\t Grad: tensor([-0.0228,  0.1291])\n",
      "Epoch 1847, Loss 2.9780385494232178\n",
      "\t Params: tensor([  5.2340, -16.5476])\n",
      "\t Grad: tensor([-0.0228,  0.1288])\n",
      "Epoch 1848, Loss 2.9778671264648438\n",
      "\t Params: tensor([  5.2342, -16.5489])\n",
      "\t Grad: tensor([-0.0227,  0.1286])\n",
      "Epoch 1849, Loss 2.977696180343628\n",
      "\t Params: tensor([  5.2344, -16.5501])\n",
      "\t Grad: tensor([-0.0227,  0.1284])\n",
      "Epoch 1850, Loss 2.977527379989624\n",
      "\t Params: tensor([  5.2346, -16.5514])\n",
      "\t Grad: tensor([-0.0227,  0.1282])\n",
      "Epoch 1851, Loss 2.9773573875427246\n",
      "\t Params: tensor([  5.2349, -16.5527])\n",
      "\t Grad: tensor([-0.0226,  0.1280])\n",
      "Epoch 1852, Loss 2.9771883487701416\n",
      "\t Params: tensor([  5.2351, -16.5540])\n",
      "\t Grad: tensor([-0.0226,  0.1278])\n",
      "Epoch 1853, Loss 2.977020502090454\n",
      "\t Params: tensor([  5.2353, -16.5553])\n",
      "\t Grad: tensor([-0.0225,  0.1275])\n",
      "Epoch 1854, Loss 2.976853370666504\n",
      "\t Params: tensor([  5.2355, -16.5565])\n",
      "\t Grad: tensor([-0.0225,  0.1273])\n",
      "Epoch 1855, Loss 2.976686716079712\n",
      "\t Params: tensor([  5.2358, -16.5578])\n",
      "\t Grad: tensor([-0.0225,  0.1271])\n",
      "Epoch 1856, Loss 2.9765195846557617\n",
      "\t Params: tensor([  5.2360, -16.5591])\n",
      "\t Grad: tensor([-0.0224,  0.1269])\n",
      "Epoch 1857, Loss 2.976353645324707\n",
      "\t Params: tensor([  5.2362, -16.5603])\n",
      "\t Grad: tensor([-0.0224,  0.1267])\n",
      "Epoch 1858, Loss 2.976189374923706\n",
      "\t Params: tensor([  5.2364, -16.5616])\n",
      "\t Grad: tensor([-0.0223,  0.1265])\n",
      "Epoch 1859, Loss 2.9760231971740723\n",
      "\t Params: tensor([  5.2367, -16.5629])\n",
      "\t Grad: tensor([-0.0223,  0.1263])\n",
      "Epoch 1860, Loss 2.9758596420288086\n",
      "\t Params: tensor([  5.2369, -16.5641])\n",
      "\t Grad: tensor([-0.0223,  0.1260])\n",
      "Epoch 1861, Loss 2.975696563720703\n",
      "\t Params: tensor([  5.2371, -16.5654])\n",
      "\t Grad: tensor([-0.0222,  0.1258])\n",
      "Epoch 1862, Loss 2.9755325317382812\n",
      "\t Params: tensor([  5.2373, -16.5666])\n",
      "\t Grad: tensor([-0.0222,  0.1256])\n",
      "Epoch 1863, Loss 2.9753692150115967\n",
      "\t Params: tensor([  5.2375, -16.5679])\n",
      "\t Grad: tensor([-0.0222,  0.1254])\n",
      "Epoch 1864, Loss 2.975208282470703\n",
      "\t Params: tensor([  5.2378, -16.5691])\n",
      "\t Grad: tensor([-0.0221,  0.1252])\n",
      "Epoch 1865, Loss 2.975045919418335\n",
      "\t Params: tensor([  5.2380, -16.5704])\n",
      "\t Grad: tensor([-0.0221,  0.1250])\n",
      "Epoch 1866, Loss 2.9748857021331787\n",
      "\t Params: tensor([  5.2382, -16.5716])\n",
      "\t Grad: tensor([-0.0220,  0.1248])\n",
      "Epoch 1867, Loss 2.974724769592285\n",
      "\t Params: tensor([  5.2384, -16.5729])\n",
      "\t Grad: tensor([-0.0220,  0.1245])\n",
      "Epoch 1868, Loss 2.974565029144287\n",
      "\t Params: tensor([  5.2386, -16.5741])\n",
      "\t Grad: tensor([-0.0220,  0.1243])\n",
      "Epoch 1869, Loss 2.9744057655334473\n",
      "\t Params: tensor([  5.2389, -16.5754])\n",
      "\t Grad: tensor([-0.0219,  0.1241])\n",
      "Epoch 1870, Loss 2.974247694015503\n",
      "\t Params: tensor([  5.2391, -16.5766])\n",
      "\t Grad: tensor([-0.0219,  0.1239])\n",
      "Epoch 1871, Loss 2.974088191986084\n",
      "\t Params: tensor([  5.2393, -16.5778])\n",
      "\t Grad: tensor([-0.0219,  0.1237])\n",
      "Epoch 1872, Loss 2.9739303588867188\n",
      "\t Params: tensor([  5.2395, -16.5791])\n",
      "\t Grad: tensor([-0.0218,  0.1235])\n",
      "Epoch 1873, Loss 2.973775625228882\n",
      "\t Params: tensor([  5.2397, -16.5803])\n",
      "\t Grad: tensor([-0.0218,  0.1233])\n",
      "Epoch 1874, Loss 2.973618268966675\n",
      "\t Params: tensor([  5.2400, -16.5815])\n",
      "\t Grad: tensor([-0.0217,  0.1231])\n",
      "Epoch 1875, Loss 2.9734625816345215\n",
      "\t Params: tensor([  5.2402, -16.5828])\n",
      "\t Grad: tensor([-0.0217,  0.1229])\n",
      "Epoch 1876, Loss 2.973306894302368\n",
      "\t Params: tensor([  5.2404, -16.5840])\n",
      "\t Grad: tensor([-0.0217,  0.1227])\n",
      "Epoch 1877, Loss 2.9731507301330566\n",
      "\t Params: tensor([  5.2406, -16.5852])\n",
      "\t Grad: tensor([-0.0216,  0.1224])\n",
      "Epoch 1878, Loss 2.972996473312378\n",
      "\t Params: tensor([  5.2408, -16.5864])\n",
      "\t Grad: tensor([-0.0216,  0.1222])\n",
      "Epoch 1879, Loss 2.9728434085845947\n",
      "\t Params: tensor([  5.2410, -16.5877])\n",
      "\t Grad: tensor([-0.0215,  0.1220])\n",
      "Epoch 1880, Loss 2.972689628601074\n",
      "\t Params: tensor([  5.2413, -16.5889])\n",
      "\t Grad: tensor([-0.0215,  0.1218])\n",
      "Epoch 1881, Loss 2.9725358486175537\n",
      "\t Params: tensor([  5.2415, -16.5901])\n",
      "\t Grad: tensor([-0.0215,  0.1216])\n",
      "Epoch 1882, Loss 2.9723832607269287\n",
      "\t Params: tensor([  5.2417, -16.5913])\n",
      "\t Grad: tensor([-0.0214,  0.1214])\n",
      "Epoch 1883, Loss 2.9722321033477783\n",
      "\t Params: tensor([  5.2419, -16.5925])\n",
      "\t Grad: tensor([-0.0214,  0.1212])\n",
      "Epoch 1884, Loss 2.972081184387207\n",
      "\t Params: tensor([  5.2421, -16.5937])\n",
      "\t Grad: tensor([-0.0214,  0.1210])\n",
      "Epoch 1885, Loss 2.971931219100952\n",
      "\t Params: tensor([  5.2423, -16.5949])\n",
      "\t Grad: tensor([-0.0213,  0.1208])\n",
      "Epoch 1886, Loss 2.9717798233032227\n",
      "\t Params: tensor([  5.2425, -16.5961])\n",
      "\t Grad: tensor([-0.0213,  0.1206])\n",
      "Epoch 1887, Loss 2.9716298580169678\n",
      "\t Params: tensor([  5.2427, -16.5974])\n",
      "\t Grad: tensor([-0.0213,  0.1204])\n",
      "Epoch 1888, Loss 2.97148060798645\n",
      "\t Params: tensor([  5.2430, -16.5986])\n",
      "\t Grad: tensor([-0.0212,  0.1202])\n",
      "Epoch 1889, Loss 2.971331834793091\n",
      "\t Params: tensor([  5.2432, -16.5998])\n",
      "\t Grad: tensor([-0.0212,  0.1200])\n",
      "Epoch 1890, Loss 2.971184492111206\n",
      "\t Params: tensor([  5.2434, -16.6010])\n",
      "\t Grad: tensor([-0.0212,  0.1198])\n",
      "Epoch 1891, Loss 2.9710352420806885\n",
      "\t Params: tensor([  5.2436, -16.6021])\n",
      "\t Grad: tensor([-0.0211,  0.1196])\n",
      "Epoch 1892, Loss 2.9708876609802246\n",
      "\t Params: tensor([  5.2438, -16.6033])\n",
      "\t Grad: tensor([-0.0211,  0.1194])\n",
      "Epoch 1893, Loss 2.970740556716919\n",
      "\t Params: tensor([  5.2440, -16.6045])\n",
      "\t Grad: tensor([-0.0211,  0.1192])\n",
      "Epoch 1894, Loss 2.9705960750579834\n",
      "\t Params: tensor([  5.2442, -16.6057])\n",
      "\t Grad: tensor([-0.0210,  0.1190])\n",
      "Epoch 1895, Loss 2.9704489707946777\n",
      "\t Params: tensor([  5.2444, -16.6069])\n",
      "\t Grad: tensor([-0.0210,  0.1188])\n",
      "Epoch 1896, Loss 2.970303535461426\n",
      "\t Params: tensor([  5.2446, -16.6081])\n",
      "\t Grad: tensor([-0.0209,  0.1186])\n",
      "Epoch 1897, Loss 2.970158576965332\n",
      "\t Params: tensor([  5.2449, -16.6093])\n",
      "\t Grad: tensor([-0.0209,  0.1183])\n",
      "Epoch 1898, Loss 2.970015525817871\n",
      "\t Params: tensor([  5.2451, -16.6105])\n",
      "\t Grad: tensor([-0.0209,  0.1182])\n",
      "Epoch 1899, Loss 2.9698705673217773\n",
      "\t Params: tensor([  5.2453, -16.6116])\n",
      "\t Grad: tensor([-0.0208,  0.1180])\n",
      "Epoch 1900, Loss 2.9697272777557373\n",
      "\t Params: tensor([  5.2455, -16.6128])\n",
      "\t Grad: tensor([-0.0208,  0.1178])\n",
      "Epoch 1901, Loss 2.969585657119751\n",
      "\t Params: tensor([  5.2457, -16.6140])\n",
      "\t Grad: tensor([-0.0208,  0.1175])\n",
      "Epoch 1902, Loss 2.96944260597229\n",
      "\t Params: tensor([  5.2459, -16.6152])\n",
      "\t Grad: tensor([-0.0207,  0.1173])\n",
      "Epoch 1903, Loss 2.969301700592041\n",
      "\t Params: tensor([  5.2461, -16.6163])\n",
      "\t Grad: tensor([-0.0207,  0.1172])\n",
      "Epoch 1904, Loss 2.9691596031188965\n",
      "\t Params: tensor([  5.2463, -16.6175])\n",
      "\t Grad: tensor([-0.0206,  0.1170])\n",
      "Epoch 1905, Loss 2.969017267227173\n",
      "\t Params: tensor([  5.2465, -16.6187])\n",
      "\t Grad: tensor([-0.0206,  0.1168])\n",
      "Epoch 1906, Loss 2.968878984451294\n",
      "\t Params: tensor([  5.2467, -16.6198])\n",
      "\t Grad: tensor([-0.0206,  0.1166])\n",
      "Epoch 1907, Loss 2.9687387943267822\n",
      "\t Params: tensor([  5.2469, -16.6210])\n",
      "\t Grad: tensor([-0.0205,  0.1164])\n",
      "Epoch 1908, Loss 2.9685988426208496\n",
      "\t Params: tensor([  5.2471, -16.6222])\n",
      "\t Grad: tensor([-0.0205,  0.1162])\n",
      "Epoch 1909, Loss 2.9684598445892334\n",
      "\t Params: tensor([  5.2473, -16.6233])\n",
      "\t Grad: tensor([-0.0205,  0.1160])\n",
      "Epoch 1910, Loss 2.9683213233947754\n",
      "\t Params: tensor([  5.2475, -16.6245])\n",
      "\t Grad: tensor([-0.0204,  0.1158])\n",
      "Epoch 1911, Loss 2.9681832790374756\n",
      "\t Params: tensor([  5.2477, -16.6256])\n",
      "\t Grad: tensor([-0.0204,  0.1156])\n",
      "Epoch 1912, Loss 2.968045711517334\n",
      "\t Params: tensor([  5.2479, -16.6268])\n",
      "\t Grad: tensor([-0.0204,  0.1154])\n",
      "Epoch 1913, Loss 2.9679079055786133\n",
      "\t Params: tensor([  5.2482, -16.6279])\n",
      "\t Grad: tensor([-0.0204,  0.1152])\n",
      "Epoch 1914, Loss 2.9677717685699463\n",
      "\t Params: tensor([  5.2484, -16.6291])\n",
      "\t Grad: tensor([-0.0203,  0.1150])\n",
      "Epoch 1915, Loss 2.9676358699798584\n",
      "\t Params: tensor([  5.2486, -16.6302])\n",
      "\t Grad: tensor([-0.0203,  0.1148])\n",
      "Epoch 1916, Loss 2.967499017715454\n",
      "\t Params: tensor([  5.2488, -16.6314])\n",
      "\t Grad: tensor([-0.0202,  0.1146])\n",
      "Epoch 1917, Loss 2.967364549636841\n",
      "\t Params: tensor([  5.2490, -16.6325])\n",
      "\t Grad: tensor([-0.0202,  0.1144])\n",
      "Epoch 1918, Loss 2.9672296047210693\n",
      "\t Params: tensor([  5.2492, -16.6337])\n",
      "\t Grad: tensor([-0.0202,  0.1142])\n",
      "Epoch 1919, Loss 2.967095375061035\n",
      "\t Params: tensor([  5.2494, -16.6348])\n",
      "\t Grad: tensor([-0.0202,  0.1140])\n",
      "Epoch 1920, Loss 2.966960906982422\n",
      "\t Params: tensor([  5.2496, -16.6360])\n",
      "\t Grad: tensor([-0.0201,  0.1138])\n",
      "Epoch 1921, Loss 2.966827630996704\n",
      "\t Params: tensor([  5.2498, -16.6371])\n",
      "\t Grad: tensor([-0.0201,  0.1136])\n",
      "Epoch 1922, Loss 2.966693162918091\n",
      "\t Params: tensor([  5.2500, -16.6382])\n",
      "\t Grad: tensor([-0.0200,  0.1134])\n",
      "Epoch 1923, Loss 2.9665610790252686\n",
      "\t Params: tensor([  5.2502, -16.6394])\n",
      "\t Grad: tensor([-0.0200,  0.1132])\n",
      "Epoch 1924, Loss 2.9664289951324463\n",
      "\t Params: tensor([  5.2504, -16.6405])\n",
      "\t Grad: tensor([-0.0200,  0.1130])\n",
      "Epoch 1925, Loss 2.9662973880767822\n",
      "\t Params: tensor([  5.2506, -16.6416])\n",
      "\t Grad: tensor([-0.0199,  0.1128])\n",
      "Epoch 1926, Loss 2.966167688369751\n",
      "\t Params: tensor([  5.2508, -16.6427])\n",
      "\t Grad: tensor([-0.0199,  0.1127])\n",
      "Epoch 1927, Loss 2.9660356044769287\n",
      "\t Params: tensor([  5.2510, -16.6439])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad: tensor([-0.0199,  0.1125])\n",
      "Epoch 1928, Loss 2.965904474258423\n",
      "\t Params: tensor([  5.2512, -16.6450])\n",
      "\t Grad: tensor([-0.0198,  0.1123])\n",
      "Epoch 1929, Loss 2.9657766819000244\n",
      "\t Params: tensor([  5.2514, -16.6461])\n",
      "\t Grad: tensor([-0.0198,  0.1121])\n",
      "Epoch 1930, Loss 2.965646505355835\n",
      "\t Params: tensor([  5.2516, -16.6472])\n",
      "\t Grad: tensor([-0.0198,  0.1119])\n",
      "Epoch 1931, Loss 2.9655160903930664\n",
      "\t Params: tensor([  5.2518, -16.6484])\n",
      "\t Grad: tensor([-0.0197,  0.1117])\n",
      "Epoch 1932, Loss 2.9653875827789307\n",
      "\t Params: tensor([  5.2520, -16.6495])\n",
      "\t Grad: tensor([-0.0197,  0.1115])\n",
      "Epoch 1933, Loss 2.9652605056762695\n",
      "\t Params: tensor([  5.2522, -16.6506])\n",
      "\t Grad: tensor([-0.0197,  0.1113])\n",
      "Epoch 1934, Loss 2.9651312828063965\n",
      "\t Params: tensor([  5.2523, -16.6517])\n",
      "\t Grad: tensor([-0.0196,  0.1111])\n",
      "Epoch 1935, Loss 2.96500563621521\n",
      "\t Params: tensor([  5.2525, -16.6528])\n",
      "\t Grad: tensor([-0.0196,  0.1109])\n",
      "Epoch 1936, Loss 2.9648773670196533\n",
      "\t Params: tensor([  5.2527, -16.6539])\n",
      "\t Grad: tensor([-0.0196,  0.1108])\n",
      "Epoch 1937, Loss 2.9647514820098877\n",
      "\t Params: tensor([  5.2529, -16.6550])\n",
      "\t Grad: tensor([-0.0195,  0.1106])\n",
      "Epoch 1938, Loss 2.964625358581543\n",
      "\t Params: tensor([  5.2531, -16.6561])\n",
      "\t Grad: tensor([-0.0195,  0.1104])\n",
      "Epoch 1939, Loss 2.9644997119903564\n",
      "\t Params: tensor([  5.2533, -16.6572])\n",
      "\t Grad: tensor([-0.0195,  0.1102])\n",
      "Epoch 1940, Loss 2.9643752574920654\n",
      "\t Params: tensor([  5.2535, -16.6583])\n",
      "\t Grad: tensor([-0.0195,  0.1100])\n",
      "Epoch 1941, Loss 2.964250326156616\n",
      "\t Params: tensor([  5.2537, -16.6594])\n",
      "\t Grad: tensor([-0.0194,  0.1098])\n",
      "Epoch 1942, Loss 2.964125871658325\n",
      "\t Params: tensor([  5.2539, -16.6605])\n",
      "\t Grad: tensor([-0.0194,  0.1096])\n",
      "Epoch 1943, Loss 2.964000940322876\n",
      "\t Params: tensor([  5.2541, -16.6616])\n",
      "\t Grad: tensor([-0.0194,  0.1094])\n",
      "Epoch 1944, Loss 2.963878870010376\n",
      "\t Params: tensor([  5.2543, -16.6627])\n",
      "\t Grad: tensor([-0.0193,  0.1093])\n",
      "Epoch 1945, Loss 2.9637558460235596\n",
      "\t Params: tensor([  5.2545, -16.6638])\n",
      "\t Grad: tensor([-0.0193,  0.1091])\n",
      "Epoch 1946, Loss 2.963632345199585\n",
      "\t Params: tensor([  5.2547, -16.6649])\n",
      "\t Grad: tensor([-0.0192,  0.1089])\n",
      "Epoch 1947, Loss 2.963510751724243\n",
      "\t Params: tensor([  5.2549, -16.6660])\n",
      "\t Grad: tensor([-0.0192,  0.1087])\n",
      "Epoch 1948, Loss 2.963387966156006\n",
      "\t Params: tensor([  5.2551, -16.6671])\n",
      "\t Grad: tensor([-0.0192,  0.1085])\n",
      "Epoch 1949, Loss 2.963266134262085\n",
      "\t Params: tensor([  5.2553, -16.6681])\n",
      "\t Grad: tensor([-0.0191,  0.1083])\n",
      "Epoch 1950, Loss 2.963148593902588\n",
      "\t Params: tensor([  5.2554, -16.6692])\n",
      "\t Grad: tensor([-0.0191,  0.1081])\n",
      "Epoch 1951, Loss 2.963026285171509\n",
      "\t Params: tensor([  5.2556, -16.6703])\n",
      "\t Grad: tensor([-0.0191,  0.1080])\n",
      "Epoch 1952, Loss 2.9629065990448\n",
      "\t Params: tensor([  5.2558, -16.6714])\n",
      "\t Grad: tensor([-0.0190,  0.1078])\n",
      "Epoch 1953, Loss 2.9627878665924072\n",
      "\t Params: tensor([  5.2560, -16.6725])\n",
      "\t Grad: tensor([-0.0190,  0.1076])\n",
      "Epoch 1954, Loss 2.9626665115356445\n",
      "\t Params: tensor([  5.2562, -16.6735])\n",
      "\t Grad: tensor([-0.0190,  0.1074])\n",
      "Epoch 1955, Loss 2.9625465869903564\n",
      "\t Params: tensor([  5.2564, -16.6746])\n",
      "\t Grad: tensor([-0.0189,  0.1072])\n",
      "Epoch 1956, Loss 2.9624290466308594\n",
      "\t Params: tensor([  5.2566, -16.6757])\n",
      "\t Grad: tensor([-0.0189,  0.1071])\n",
      "Epoch 1957, Loss 2.9623117446899414\n",
      "\t Params: tensor([  5.2568, -16.6767])\n",
      "\t Grad: tensor([-0.0189,  0.1069])\n",
      "Epoch 1958, Loss 2.9621946811676025\n",
      "\t Params: tensor([  5.2570, -16.6778])\n",
      "\t Grad: tensor([-0.0188,  0.1067])\n",
      "Epoch 1959, Loss 2.9620778560638428\n",
      "\t Params: tensor([  5.2572, -16.6789])\n",
      "\t Grad: tensor([-0.0188,  0.1065])\n",
      "Epoch 1960, Loss 2.96195912361145\n",
      "\t Params: tensor([  5.2573, -16.6799])\n",
      "\t Grad: tensor([-0.0188,  0.1063])\n",
      "Epoch 1961, Loss 2.9618430137634277\n",
      "\t Params: tensor([  5.2575, -16.6810])\n",
      "\t Grad: tensor([-0.0187,  0.1062])\n",
      "Epoch 1962, Loss 2.961728096008301\n",
      "\t Params: tensor([  5.2577, -16.6821])\n",
      "\t Grad: tensor([-0.0187,  0.1060])\n",
      "Epoch 1963, Loss 2.961611270904541\n",
      "\t Params: tensor([  5.2579, -16.6831])\n",
      "\t Grad: tensor([-0.0187,  0.1058])\n",
      "Epoch 1964, Loss 2.961495876312256\n",
      "\t Params: tensor([  5.2581, -16.6842])\n",
      "\t Grad: tensor([-0.0187,  0.1056])\n",
      "Epoch 1965, Loss 2.9613821506500244\n",
      "\t Params: tensor([  5.2583, -16.6852])\n",
      "\t Grad: tensor([-0.0186,  0.1054])\n",
      "Epoch 1966, Loss 2.9612672328948975\n",
      "\t Params: tensor([  5.2585, -16.6863])\n",
      "\t Grad: tensor([-0.0186,  0.1052])\n",
      "Epoch 1967, Loss 2.9611525535583496\n",
      "\t Params: tensor([  5.2586, -16.6873])\n",
      "\t Grad: tensor([-0.0186,  0.1051])\n",
      "Epoch 1968, Loss 2.96103835105896\n",
      "\t Params: tensor([  5.2588, -16.6884])\n",
      "\t Grad: tensor([-0.0185,  0.1049])\n",
      "Epoch 1969, Loss 2.9609262943267822\n",
      "\t Params: tensor([  5.2590, -16.6894])\n",
      "\t Grad: tensor([-0.0185,  0.1047])\n",
      "Epoch 1970, Loss 2.960813283920288\n",
      "\t Params: tensor([  5.2592, -16.6905])\n",
      "\t Grad: tensor([-0.0185,  0.1045])\n",
      "Epoch 1971, Loss 2.9606995582580566\n",
      "\t Params: tensor([  5.2594, -16.6915])\n",
      "\t Grad: tensor([-0.0184,  0.1044])\n",
      "Epoch 1972, Loss 2.9605867862701416\n",
      "\t Params: tensor([  5.2596, -16.6926])\n",
      "\t Grad: tensor([-0.0184,  0.1042])\n",
      "Epoch 1973, Loss 2.960475206375122\n",
      "\t Params: tensor([  5.2598, -16.6936])\n",
      "\t Grad: tensor([-0.0184,  0.1040])\n",
      "Epoch 1974, Loss 2.960365056991577\n",
      "\t Params: tensor([  5.2599, -16.6946])\n",
      "\t Grad: tensor([-0.0183,  0.1038])\n",
      "Epoch 1975, Loss 2.960254669189453\n",
      "\t Params: tensor([  5.2601, -16.6957])\n",
      "\t Grad: tensor([-0.0183,  0.1037])\n",
      "Epoch 1976, Loss 2.9601433277130127\n",
      "\t Params: tensor([  5.2603, -16.6967])\n",
      "\t Grad: tensor([-0.0183,  0.1035])\n",
      "Epoch 1977, Loss 2.9600331783294678\n",
      "\t Params: tensor([  5.2605, -16.6977])\n",
      "\t Grad: tensor([-0.0182,  0.1033])\n",
      "Epoch 1978, Loss 2.959923028945923\n",
      "\t Params: tensor([  5.2607, -16.6988])\n",
      "\t Grad: tensor([-0.0182,  0.1031])\n",
      "Epoch 1979, Loss 2.9598119258880615\n",
      "\t Params: tensor([  5.2608, -16.6998])\n",
      "\t Grad: tensor([-0.0182,  0.1029])\n",
      "Epoch 1980, Loss 2.959703207015991\n",
      "\t Params: tensor([  5.2610, -16.7008])\n",
      "\t Grad: tensor([-0.0182,  0.1028])\n",
      "Epoch 1981, Loss 2.9595940113067627\n",
      "\t Params: tensor([  5.2612, -16.7019])\n",
      "\t Grad: tensor([-0.0181,  0.1026])\n",
      "Epoch 1982, Loss 2.9594857692718506\n",
      "\t Params: tensor([  5.2614, -16.7029])\n",
      "\t Grad: tensor([-0.0181,  0.1024])\n",
      "Epoch 1983, Loss 2.959378242492676\n",
      "\t Params: tensor([  5.2616, -16.7039])\n",
      "\t Grad: tensor([-0.0181,  0.1022])\n",
      "Epoch 1984, Loss 2.959270715713501\n",
      "\t Params: tensor([  5.2618, -16.7049])\n",
      "\t Grad: tensor([-0.0180,  0.1021])\n",
      "Epoch 1985, Loss 2.9591622352600098\n",
      "\t Params: tensor([  5.2619, -16.7059])\n",
      "\t Grad: tensor([-0.0180,  0.1019])\n",
      "Epoch 1986, Loss 2.959054708480835\n",
      "\t Params: tensor([  5.2621, -16.7070])\n",
      "\t Grad: tensor([-0.0180,  0.1017])\n",
      "Epoch 1987, Loss 2.9589502811431885\n",
      "\t Params: tensor([  5.2623, -16.7080])\n",
      "\t Grad: tensor([-0.0179,  0.1016])\n",
      "Epoch 1988, Loss 2.9588422775268555\n",
      "\t Params: tensor([  5.2625, -16.7090])\n",
      "\t Grad: tensor([-0.0179,  0.1014])\n",
      "Epoch 1989, Loss 2.958737850189209\n",
      "\t Params: tensor([  5.2626, -16.7100])\n",
      "\t Grad: tensor([-0.0179,  0.1012])\n",
      "Epoch 1990, Loss 2.958631992340088\n",
      "\t Params: tensor([  5.2628, -16.7110])\n",
      "\t Grad: tensor([-0.0179,  0.1010])\n",
      "Epoch 1991, Loss 2.9585258960723877\n",
      "\t Params: tensor([  5.2630, -16.7120])\n",
      "\t Grad: tensor([-0.0178,  0.1009])\n",
      "Epoch 1992, Loss 2.9584219455718994\n",
      "\t Params: tensor([  5.2632, -16.7130])\n",
      "\t Grad: tensor([-0.0178,  0.1007])\n",
      "Epoch 1993, Loss 2.9583168029785156\n",
      "\t Params: tensor([  5.2634, -16.7140])\n",
      "\t Grad: tensor([-0.0178,  0.1005])\n",
      "Epoch 1994, Loss 2.95821213722229\n",
      "\t Params: tensor([  5.2635, -16.7150])\n",
      "\t Grad: tensor([-0.0177,  0.1004])\n",
      "Epoch 1995, Loss 2.958109140396118\n",
      "\t Params: tensor([  5.2637, -16.7160])\n",
      "\t Grad: tensor([-0.0177,  0.1002])\n",
      "Epoch 1996, Loss 2.9580061435699463\n",
      "\t Params: tensor([  5.2639, -16.7170])\n",
      "\t Grad: tensor([-0.0176,  0.1000])\n",
      "Epoch 1997, Loss 2.9579038619995117\n",
      "\t Params: tensor([  5.2641, -16.7180])\n",
      "\t Grad: tensor([-0.0176,  0.0998])\n",
      "Epoch 1998, Loss 2.95780086517334\n",
      "\t Params: tensor([  5.2642, -16.7190])\n",
      "\t Grad: tensor([-0.0176,  0.0997])\n",
      "Epoch 1999, Loss 2.957697868347168\n",
      "\t Params: tensor([  5.2644, -16.7200])\n",
      "\t Grad: tensor([-0.0176,  0.0995])\n",
      "Epoch 2000, Loss 2.9575960636138916\n",
      "\t Params: tensor([  5.2646, -16.7210])\n",
      "\t Grad: tensor([-0.0176,  0.0993])\n",
      "Epoch 2001, Loss 2.9574942588806152\n",
      "\t Params: tensor([  5.2648, -16.7220])\n",
      "\t Grad: tensor([-0.0175,  0.0992])\n",
      "Epoch 2002, Loss 2.957392692565918\n",
      "\t Params: tensor([  5.2649, -16.7230])\n",
      "\t Grad: tensor([-0.0175,  0.0990])\n",
      "Epoch 2003, Loss 2.957292079925537\n",
      "\t Params: tensor([  5.2651, -16.7240])\n",
      "\t Grad: tensor([-0.0174,  0.0988])\n",
      "Epoch 2004, Loss 2.957192897796631\n",
      "\t Params: tensor([  5.2653, -16.7250])\n",
      "\t Grad: tensor([-0.0174,  0.0987])\n",
      "Epoch 2005, Loss 2.9570913314819336\n",
      "\t Params: tensor([  5.2655, -16.7260])\n",
      "\t Grad: tensor([-0.0174,  0.0985])\n",
      "Epoch 2006, Loss 2.956991672515869\n",
      "\t Params: tensor([  5.2656, -16.7269])\n",
      "\t Grad: tensor([-0.0174,  0.0983])\n",
      "Epoch 2007, Loss 2.956892490386963\n",
      "\t Params: tensor([  5.2658, -16.7279])\n",
      "\t Grad: tensor([-0.0173,  0.0982])\n",
      "Epoch 2008, Loss 2.9567923545837402\n",
      "\t Params: tensor([  5.2660, -16.7289])\n",
      "\t Grad: tensor([-0.0173,  0.0980])\n",
      "Epoch 2009, Loss 2.9566941261291504\n",
      "\t Params: tensor([  5.2662, -16.7299])\n",
      "\t Grad: tensor([-0.0173,  0.0978])\n",
      "Epoch 2010, Loss 2.9565954208374023\n",
      "\t Params: tensor([  5.2663, -16.7309])\n",
      "\t Grad: tensor([-0.0172,  0.0977])\n",
      "Epoch 2011, Loss 2.956496238708496\n",
      "\t Params: tensor([  5.2665, -16.7318])\n",
      "\t Grad: tensor([-0.0172,  0.0975])\n",
      "Epoch 2012, Loss 2.95639705657959\n",
      "\t Params: tensor([  5.2667, -16.7328])\n",
      "\t Grad: tensor([-0.0172,  0.0973])\n",
      "Epoch 2013, Loss 2.9563004970550537\n",
      "\t Params: tensor([  5.2668, -16.7338])\n",
      "\t Grad: tensor([-0.0172,  0.0972])\n",
      "Epoch 2014, Loss 2.9562041759490967\n",
      "\t Params: tensor([  5.2670, -16.7348])\n",
      "\t Grad: tensor([-0.0171,  0.0970])\n",
      "Epoch 2015, Loss 2.9561080932617188\n",
      "\t Params: tensor([  5.2672, -16.7357])\n",
      "\t Grad: tensor([-0.0171,  0.0968])\n",
      "Epoch 2016, Loss 2.95600962638855\n",
      "\t Params: tensor([  5.2674, -16.7367])\n",
      "\t Grad: tensor([-0.0171,  0.0967])\n",
      "Epoch 2017, Loss 2.955913782119751\n",
      "\t Params: tensor([  5.2675, -16.7377])\n",
      "\t Grad: tensor([-0.0171,  0.0965])\n",
      "Epoch 2018, Loss 2.955817461013794\n",
      "\t Params: tensor([  5.2677, -16.7386])\n",
      "\t Grad: tensor([-0.0170,  0.0963])\n",
      "Epoch 2019, Loss 2.955721855163574\n",
      "\t Params: tensor([  5.2679, -16.7396])\n",
      "\t Grad: tensor([-0.0170,  0.0962])\n",
      "Epoch 2020, Loss 2.955627202987671\n",
      "\t Params: tensor([  5.2680, -16.7405])\n",
      "\t Grad: tensor([-0.0170,  0.0960])\n",
      "Epoch 2021, Loss 2.955533027648926\n",
      "\t Params: tensor([  5.2682, -16.7415])\n",
      "\t Grad: tensor([-0.0169,  0.0959])\n",
      "Epoch 2022, Loss 2.9554357528686523\n",
      "\t Params: tensor([  5.2684, -16.7425])\n",
      "\t Grad: tensor([-0.0169,  0.0957])\n",
      "Epoch 2023, Loss 2.955343008041382\n",
      "\t Params: tensor([  5.2686, -16.7434])\n",
      "\t Grad: tensor([-0.0169,  0.0955])\n",
      "Epoch 2024, Loss 2.955249786376953\n",
      "\t Params: tensor([  5.2687, -16.7444])\n",
      "\t Grad: tensor([-0.0169,  0.0954])\n",
      "Epoch 2025, Loss 2.9551544189453125\n",
      "\t Params: tensor([  5.2689, -16.7453])\n",
      "\t Grad: tensor([-0.0168,  0.0952])\n",
      "Epoch 2026, Loss 2.9550621509552\n",
      "\t Params: tensor([  5.2691, -16.7463])\n",
      "\t Grad: tensor([-0.0168,  0.0950])\n",
      "Epoch 2027, Loss 2.9549689292907715\n",
      "\t Params: tensor([  5.2692, -16.7472])\n",
      "\t Grad: tensor([-0.0168,  0.0949])\n",
      "Epoch 2028, Loss 2.9548749923706055\n",
      "\t Params: tensor([  5.2694, -16.7482])\n",
      "\t Grad: tensor([-0.0167,  0.0947])\n",
      "Epoch 2029, Loss 2.9547829627990723\n",
      "\t Params: tensor([  5.2696, -16.7491])\n",
      "\t Grad: tensor([-0.0167,  0.0946])\n",
      "Epoch 2030, Loss 2.9546914100646973\n",
      "\t Params: tensor([  5.2697, -16.7501])\n",
      "\t Grad: tensor([-0.0167,  0.0944])\n",
      "Epoch 2031, Loss 2.954599618911743\n",
      "\t Params: tensor([  5.2699, -16.7510])\n",
      "\t Grad: tensor([-0.0167,  0.0942])\n",
      "Epoch 2032, Loss 2.954507350921631\n",
      "\t Params: tensor([  5.2701, -16.7519])\n",
      "\t Grad: tensor([-0.0166,  0.0941])\n",
      "Epoch 2033, Loss 2.9544172286987305\n",
      "\t Params: tensor([  5.2702, -16.7529])\n",
      "\t Grad: tensor([-0.0166,  0.0939])\n",
      "Epoch 2034, Loss 2.9543263912200928\n",
      "\t Params: tensor([  5.2704, -16.7538])\n",
      "\t Grad: tensor([-0.0165,  0.0938])\n",
      "Epoch 2035, Loss 2.954235076904297\n",
      "\t Params: tensor([  5.2706, -16.7547])\n",
      "\t Grad: tensor([-0.0165,  0.0936])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2036, Loss 2.9541451930999756\n",
      "\t Params: tensor([  5.2707, -16.7557])\n",
      "\t Grad: tensor([-0.0165,  0.0934])\n",
      "Epoch 2037, Loss 2.954055070877075\n",
      "\t Params: tensor([  5.2709, -16.7566])\n",
      "\t Grad: tensor([-0.0165,  0.0933])\n",
      "Epoch 2038, Loss 2.953965902328491\n",
      "\t Params: tensor([  5.2710, -16.7575])\n",
      "\t Grad: tensor([-0.0164,  0.0931])\n",
      "Epoch 2039, Loss 2.9538755416870117\n",
      "\t Params: tensor([  5.2712, -16.7585])\n",
      "\t Grad: tensor([-0.0164,  0.0930])\n",
      "Epoch 2040, Loss 2.953787326812744\n",
      "\t Params: tensor([  5.2714, -16.7594])\n",
      "\t Grad: tensor([-0.0164,  0.0928])\n",
      "Epoch 2041, Loss 2.953697919845581\n",
      "\t Params: tensor([  5.2715, -16.7603])\n",
      "\t Grad: tensor([-0.0164,  0.0926])\n",
      "Epoch 2042, Loss 2.9536101818084717\n",
      "\t Params: tensor([  5.2717, -16.7613])\n",
      "\t Grad: tensor([-0.0163,  0.0925])\n",
      "Epoch 2043, Loss 2.9535210132598877\n",
      "\t Params: tensor([  5.2719, -16.7622])\n",
      "\t Grad: tensor([-0.0163,  0.0923])\n",
      "Epoch 2044, Loss 2.9534342288970947\n",
      "\t Params: tensor([  5.2720, -16.7631])\n",
      "\t Grad: tensor([-0.0163,  0.0922])\n",
      "Epoch 2045, Loss 2.9533462524414062\n",
      "\t Params: tensor([  5.2722, -16.7640])\n",
      "\t Grad: tensor([-0.0163,  0.0920])\n",
      "Epoch 2046, Loss 2.953259229660034\n",
      "\t Params: tensor([  5.2724, -16.7649])\n",
      "\t Grad: tensor([-0.0162,  0.0919])\n",
      "Epoch 2047, Loss 2.953171491622925\n",
      "\t Params: tensor([  5.2725, -16.7659])\n",
      "\t Grad: tensor([-0.0162,  0.0917])\n",
      "Epoch 2048, Loss 2.953085422515869\n",
      "\t Params: tensor([  5.2727, -16.7668])\n",
      "\t Grad: tensor([-0.0162,  0.0915])\n",
      "Epoch 2049, Loss 2.9529998302459717\n",
      "\t Params: tensor([  5.2728, -16.7677])\n",
      "\t Grad: tensor([-0.0162,  0.0914])\n",
      "Epoch 2050, Loss 2.9529130458831787\n",
      "\t Params: tensor([  5.2730, -16.7686])\n",
      "\t Grad: tensor([-0.0161,  0.0912])\n",
      "Epoch 2051, Loss 2.9528284072875977\n",
      "\t Params: tensor([  5.2732, -16.7695])\n",
      "\t Grad: tensor([-0.0161,  0.0911])\n",
      "Epoch 2052, Loss 2.952741861343384\n",
      "\t Params: tensor([  5.2733, -16.7704])\n",
      "\t Grad: tensor([-0.0161,  0.0909])\n",
      "Epoch 2053, Loss 2.9526567459106445\n",
      "\t Params: tensor([  5.2735, -16.7713])\n",
      "\t Grad: tensor([-0.0160,  0.0908])\n",
      "Epoch 2054, Loss 2.952570915222168\n",
      "\t Params: tensor([  5.2736, -16.7722])\n",
      "\t Grad: tensor([-0.0160,  0.0906])\n",
      "Epoch 2055, Loss 2.952486991882324\n",
      "\t Params: tensor([  5.2738, -16.7731])\n",
      "\t Grad: tensor([-0.0160,  0.0905])\n",
      "Epoch 2056, Loss 2.9524033069610596\n",
      "\t Params: tensor([  5.2740, -16.7740])\n",
      "\t Grad: tensor([-0.0160,  0.0903])\n",
      "Epoch 2057, Loss 2.952317953109741\n",
      "\t Params: tensor([  5.2741, -16.7749])\n",
      "\t Grad: tensor([-0.0159,  0.0902])\n",
      "Epoch 2058, Loss 2.9522347450256348\n",
      "\t Params: tensor([  5.2743, -16.7758])\n",
      "\t Grad: tensor([-0.0159,  0.0900])\n",
      "Epoch 2059, Loss 2.9521515369415283\n",
      "\t Params: tensor([  5.2744, -16.7767])\n",
      "\t Grad: tensor([-0.0159,  0.0899])\n",
      "Epoch 2060, Loss 2.9520680904388428\n",
      "\t Params: tensor([  5.2746, -16.7776])\n",
      "\t Grad: tensor([-0.0158,  0.0897])\n",
      "Epoch 2061, Loss 2.9519851207733154\n",
      "\t Params: tensor([  5.2748, -16.7785])\n",
      "\t Grad: tensor([-0.0158,  0.0895])\n",
      "Epoch 2062, Loss 2.951901912689209\n",
      "\t Params: tensor([  5.2749, -16.7794])\n",
      "\t Grad: tensor([-0.0158,  0.0894])\n",
      "Epoch 2063, Loss 2.9518203735351562\n",
      "\t Params: tensor([  5.2751, -16.7803])\n",
      "\t Grad: tensor([-0.0158,  0.0892])\n",
      "Epoch 2064, Loss 2.951737880706787\n",
      "\t Params: tensor([  5.2752, -16.7812])\n",
      "\t Grad: tensor([-0.0157,  0.0891])\n",
      "Epoch 2065, Loss 2.9516561031341553\n",
      "\t Params: tensor([  5.2754, -16.7821])\n",
      "\t Grad: tensor([-0.0157,  0.0889])\n",
      "Epoch 2066, Loss 2.951575517654419\n",
      "\t Params: tensor([  5.2755, -16.7830])\n",
      "\t Grad: tensor([-0.0157,  0.0888])\n",
      "Epoch 2067, Loss 2.9514944553375244\n",
      "\t Params: tensor([  5.2757, -16.7839])\n",
      "\t Grad: tensor([-0.0157,  0.0886])\n",
      "Epoch 2068, Loss 2.951413154602051\n",
      "\t Params: tensor([  5.2759, -16.7848])\n",
      "\t Grad: tensor([-0.0157,  0.0885])\n",
      "Epoch 2069, Loss 2.9513330459594727\n",
      "\t Params: tensor([  5.2760, -16.7856])\n",
      "\t Grad: tensor([-0.0156,  0.0883])\n",
      "Epoch 2070, Loss 2.9512522220611572\n",
      "\t Params: tensor([  5.2762, -16.7865])\n",
      "\t Grad: tensor([-0.0156,  0.0882])\n",
      "Epoch 2071, Loss 2.9511711597442627\n",
      "\t Params: tensor([  5.2763, -16.7874])\n",
      "\t Grad: tensor([-0.0155,  0.0880])\n",
      "Epoch 2072, Loss 2.9510927200317383\n",
      "\t Params: tensor([  5.2765, -16.7883])\n",
      "\t Grad: tensor([-0.0155,  0.0879])\n",
      "Epoch 2073, Loss 2.9510116577148438\n",
      "\t Params: tensor([  5.2766, -16.7892])\n",
      "\t Grad: tensor([-0.0155,  0.0877])\n",
      "Epoch 2074, Loss 2.9509315490722656\n",
      "\t Params: tensor([  5.2768, -16.7900])\n",
      "\t Grad: tensor([-0.0155,  0.0876])\n",
      "Epoch 2075, Loss 2.950853109359741\n",
      "\t Params: tensor([  5.2769, -16.7909])\n",
      "\t Grad: tensor([-0.0154,  0.0874])\n",
      "Epoch 2076, Loss 2.9507744312286377\n",
      "\t Params: tensor([  5.2771, -16.7918])\n",
      "\t Grad: tensor([-0.0154,  0.0873])\n",
      "Epoch 2077, Loss 2.950697422027588\n",
      "\t Params: tensor([  5.2772, -16.7927])\n",
      "\t Grad: tensor([-0.0154,  0.0871])\n",
      "Epoch 2078, Loss 2.950618028640747\n",
      "\t Params: tensor([  5.2774, -16.7935])\n",
      "\t Grad: tensor([-0.0154,  0.0870])\n",
      "Epoch 2079, Loss 2.95054030418396\n",
      "\t Params: tensor([  5.2776, -16.7944])\n",
      "\t Grad: tensor([-0.0154,  0.0868])\n",
      "Epoch 2080, Loss 2.950463056564331\n",
      "\t Params: tensor([  5.2777, -16.7953])\n",
      "\t Grad: tensor([-0.0153,  0.0867])\n",
      "Epoch 2081, Loss 2.950385093688965\n",
      "\t Params: tensor([  5.2779, -16.7961])\n",
      "\t Grad: tensor([-0.0153,  0.0866])\n",
      "Epoch 2082, Loss 2.950308322906494\n",
      "\t Params: tensor([  5.2780, -16.7970])\n",
      "\t Grad: tensor([-0.0153,  0.0864])\n",
      "Epoch 2083, Loss 2.9502313137054443\n",
      "\t Params: tensor([  5.2782, -16.7979])\n",
      "\t Grad: tensor([-0.0152,  0.0863])\n",
      "Epoch 2084, Loss 2.9501543045043945\n",
      "\t Params: tensor([  5.2783, -16.7987])\n",
      "\t Grad: tensor([-0.0152,  0.0861])\n",
      "Epoch 2085, Loss 2.9500784873962402\n",
      "\t Params: tensor([  5.2785, -16.7996])\n",
      "\t Grad: tensor([-0.0152,  0.0860])\n",
      "Epoch 2086, Loss 2.950003147125244\n",
      "\t Params: tensor([  5.2786, -16.8004])\n",
      "\t Grad: tensor([-0.0152,  0.0858])\n",
      "Epoch 2087, Loss 2.949925422668457\n",
      "\t Params: tensor([  5.2788, -16.8013])\n",
      "\t Grad: tensor([-0.0152,  0.0857])\n",
      "Epoch 2088, Loss 2.949849843978882\n",
      "\t Params: tensor([  5.2789, -16.8021])\n",
      "\t Grad: tensor([-0.0151,  0.0855])\n",
      "Epoch 2089, Loss 2.9497756958007812\n",
      "\t Params: tensor([  5.2791, -16.8030])\n",
      "\t Grad: tensor([-0.0151,  0.0854])\n",
      "Epoch 2090, Loss 2.9496991634368896\n",
      "\t Params: tensor([  5.2792, -16.8039])\n",
      "\t Grad: tensor([-0.0151,  0.0852])\n",
      "Epoch 2091, Loss 2.9496259689331055\n",
      "\t Params: tensor([  5.2794, -16.8047])\n",
      "\t Grad: tensor([-0.0150,  0.0851])\n",
      "Epoch 2092, Loss 2.9495503902435303\n",
      "\t Params: tensor([  5.2795, -16.8056])\n",
      "\t Grad: tensor([-0.0150,  0.0850])\n",
      "Epoch 2093, Loss 2.9494760036468506\n",
      "\t Params: tensor([  5.2797, -16.8064])\n",
      "\t Grad: tensor([-0.0150,  0.0848])\n",
      "Epoch 2094, Loss 2.9494011402130127\n",
      "\t Params: tensor([  5.2798, -16.8072])\n",
      "\t Grad: tensor([-0.0149,  0.0847])\n",
      "Epoch 2095, Loss 2.9493277072906494\n",
      "\t Params: tensor([  5.2800, -16.8081])\n",
      "\t Grad: tensor([-0.0150,  0.0845])\n",
      "Epoch 2096, Loss 2.949253797531128\n",
      "\t Params: tensor([  5.2801, -16.8089])\n",
      "\t Grad: tensor([-0.0149,  0.0844])\n",
      "Epoch 2097, Loss 2.94918155670166\n",
      "\t Params: tensor([  5.2803, -16.8098])\n",
      "\t Grad: tensor([-0.0149,  0.0842])\n",
      "Epoch 2098, Loss 2.949108123779297\n",
      "\t Params: tensor([  5.2804, -16.8106])\n",
      "\t Grad: tensor([-0.0149,  0.0841])\n",
      "Epoch 2099, Loss 2.949035406112671\n",
      "\t Params: tensor([  5.2806, -16.8115])\n",
      "\t Grad: tensor([-0.0148,  0.0839])\n",
      "Epoch 2100, Loss 2.9489617347717285\n",
      "\t Params: tensor([  5.2807, -16.8123])\n",
      "\t Grad: tensor([-0.0148,  0.0838])\n",
      "Epoch 2101, Loss 2.948889970779419\n",
      "\t Params: tensor([  5.2809, -16.8131])\n",
      "\t Grad: tensor([-0.0148,  0.0837])\n",
      "Epoch 2102, Loss 2.9488184452056885\n",
      "\t Params: tensor([  5.2810, -16.8140])\n",
      "\t Grad: tensor([-0.0148,  0.0835])\n",
      "Epoch 2103, Loss 2.9487454891204834\n",
      "\t Params: tensor([  5.2812, -16.8148])\n",
      "\t Grad: tensor([-0.0148,  0.0834])\n",
      "Epoch 2104, Loss 2.9486746788024902\n",
      "\t Params: tensor([  5.2813, -16.8156])\n",
      "\t Grad: tensor([-0.0147,  0.0832])\n",
      "Epoch 2105, Loss 2.9486021995544434\n",
      "\t Params: tensor([  5.2815, -16.8165])\n",
      "\t Grad: tensor([-0.0147,  0.0831])\n",
      "Epoch 2106, Loss 2.9485321044921875\n",
      "\t Params: tensor([  5.2816, -16.8173])\n",
      "\t Grad: tensor([-0.0146,  0.0830])\n",
      "Epoch 2107, Loss 2.94846248626709\n",
      "\t Params: tensor([  5.2817, -16.8181])\n",
      "\t Grad: tensor([-0.0146,  0.0828])\n",
      "Epoch 2108, Loss 2.9483907222747803\n",
      "\t Params: tensor([  5.2819, -16.8189])\n",
      "\t Grad: tensor([-0.0146,  0.0827])\n",
      "Epoch 2109, Loss 2.9483213424682617\n",
      "\t Params: tensor([  5.2820, -16.8198])\n",
      "\t Grad: tensor([-0.0146,  0.0825])\n",
      "Epoch 2110, Loss 2.9482500553131104\n",
      "\t Params: tensor([  5.2822, -16.8206])\n",
      "\t Grad: tensor([-0.0145,  0.0824])\n",
      "Epoch 2111, Loss 2.948180913925171\n",
      "\t Params: tensor([  5.2823, -16.8214])\n",
      "\t Grad: tensor([-0.0145,  0.0823])\n",
      "Epoch 2112, Loss 2.9481089115142822\n",
      "\t Params: tensor([  5.2825, -16.8222])\n",
      "\t Grad: tensor([-0.0145,  0.0821])\n",
      "Epoch 2113, Loss 2.948040723800659\n",
      "\t Params: tensor([  5.2826, -16.8231])\n",
      "\t Grad: tensor([-0.0145,  0.0820])\n",
      "Epoch 2114, Loss 2.9479713439941406\n",
      "\t Params: tensor([  5.2828, -16.8239])\n",
      "\t Grad: tensor([-0.0144,  0.0818])\n",
      "Epoch 2115, Loss 2.947901725769043\n",
      "\t Params: tensor([  5.2829, -16.8247])\n",
      "\t Grad: tensor([-0.0144,  0.0817])\n",
      "Epoch 2116, Loss 2.9478330612182617\n",
      "\t Params: tensor([  5.2831, -16.8255])\n",
      "\t Grad: tensor([-0.0144,  0.0816])\n",
      "Epoch 2117, Loss 2.9477646350860596\n",
      "\t Params: tensor([  5.2832, -16.8263])\n",
      "\t Grad: tensor([-0.0144,  0.0814])\n",
      "Epoch 2118, Loss 2.9476959705352783\n",
      "\t Params: tensor([  5.2833, -16.8271])\n",
      "\t Grad: tensor([-0.0144,  0.0813])\n",
      "Epoch 2119, Loss 2.9476282596588135\n",
      "\t Params: tensor([  5.2835, -16.8280])\n",
      "\t Grad: tensor([-0.0143,  0.0811])\n",
      "Epoch 2120, Loss 2.9475603103637695\n",
      "\t Params: tensor([  5.2836, -16.8288])\n",
      "\t Grad: tensor([-0.0143,  0.0810])\n",
      "Epoch 2121, Loss 2.9474942684173584\n",
      "\t Params: tensor([  5.2838, -16.8296])\n",
      "\t Grad: tensor([-0.0143,  0.0809])\n",
      "Epoch 2122, Loss 2.9474258422851562\n",
      "\t Params: tensor([  5.2839, -16.8304])\n",
      "\t Grad: tensor([-0.0143,  0.0807])\n",
      "Epoch 2123, Loss 2.947357416152954\n",
      "\t Params: tensor([  5.2841, -16.8312])\n",
      "\t Grad: tensor([-0.0142,  0.0806])\n",
      "Epoch 2124, Loss 2.947293281555176\n",
      "\t Params: tensor([  5.2842, -16.8320])\n",
      "\t Grad: tensor([-0.0142,  0.0805])\n",
      "Epoch 2125, Loss 2.9472250938415527\n",
      "\t Params: tensor([  5.2843, -16.8328])\n",
      "\t Grad: tensor([-0.0142,  0.0803])\n",
      "Epoch 2126, Loss 2.947158098220825\n",
      "\t Params: tensor([  5.2845, -16.8336])\n",
      "\t Grad: tensor([-0.0142,  0.0802])\n",
      "Epoch 2127, Loss 2.947091579437256\n",
      "\t Params: tensor([  5.2846, -16.8344])\n",
      "\t Grad: tensor([-0.0141,  0.0800])\n",
      "Epoch 2128, Loss 2.947026252746582\n",
      "\t Params: tensor([  5.2848, -16.8352])\n",
      "\t Grad: tensor([-0.0141,  0.0799])\n",
      "Epoch 2129, Loss 2.94696044921875\n",
      "\t Params: tensor([  5.2849, -16.8360])\n",
      "\t Grad: tensor([-0.0141,  0.0798])\n",
      "Epoch 2130, Loss 2.946894645690918\n",
      "\t Params: tensor([  5.2850, -16.8368])\n",
      "\t Grad: tensor([-0.0141,  0.0796])\n",
      "Epoch 2131, Loss 2.9468297958374023\n",
      "\t Params: tensor([  5.2852, -16.8376])\n",
      "\t Grad: tensor([-0.0141,  0.0795])\n",
      "Epoch 2132, Loss 2.9467644691467285\n",
      "\t Params: tensor([  5.2853, -16.8384])\n",
      "\t Grad: tensor([-0.0140,  0.0794])\n",
      "Epoch 2133, Loss 2.946699619293213\n",
      "\t Params: tensor([  5.2855, -16.8392])\n",
      "\t Grad: tensor([-0.0140,  0.0792])\n",
      "Epoch 2134, Loss 2.9466352462768555\n",
      "\t Params: tensor([  5.2856, -16.8400])\n",
      "\t Grad: tensor([-0.0140,  0.0791])\n",
      "Epoch 2135, Loss 2.946570634841919\n",
      "\t Params: tensor([  5.2857, -16.8407])\n",
      "\t Grad: tensor([-0.0139,  0.0790])\n",
      "Epoch 2136, Loss 2.9465065002441406\n",
      "\t Params: tensor([  5.2859, -16.8415])\n",
      "\t Grad: tensor([-0.0139,  0.0788])\n",
      "Epoch 2137, Loss 2.946441888809204\n",
      "\t Params: tensor([  5.2860, -16.8423])\n",
      "\t Grad: tensor([-0.0139,  0.0787])\n",
      "Epoch 2138, Loss 2.946377754211426\n",
      "\t Params: tensor([  5.2862, -16.8431])\n",
      "\t Grad: tensor([-0.0139,  0.0786])\n",
      "Epoch 2139, Loss 2.9463140964508057\n",
      "\t Params: tensor([  5.2863, -16.8439])\n",
      "\t Grad: tensor([-0.0138,  0.0784])\n",
      "Epoch 2140, Loss 2.946251153945923\n",
      "\t Params: tensor([  5.2864, -16.8447])\n",
      "\t Grad: tensor([-0.0138,  0.0783])\n",
      "Epoch 2141, Loss 2.9461889266967773\n",
      "\t Params: tensor([  5.2866, -16.8455])\n",
      "\t Grad: tensor([-0.0138,  0.0782])\n",
      "Epoch 2142, Loss 2.9461257457733154\n",
      "\t Params: tensor([  5.2867, -16.8462])\n",
      "\t Grad: tensor([-0.0138,  0.0780])\n",
      "Epoch 2143, Loss 2.9460630416870117\n",
      "\t Params: tensor([  5.2869, -16.8470])\n",
      "\t Grad: tensor([-0.0138,  0.0779])\n",
      "Epoch 2144, Loss 2.946000814437866\n",
      "\t Params: tensor([  5.2870, -16.8478])\n",
      "\t Grad: tensor([-0.0137,  0.0778])\n",
      "Epoch 2145, Loss 2.945937395095825\n",
      "\t Params: tensor([  5.2871, -16.8486])\n",
      "\t Grad: tensor([-0.0137,  0.0776])\n",
      "Epoch 2146, Loss 2.945876121520996\n",
      "\t Params: tensor([  5.2873, -16.8493])\n",
      "\t Grad: tensor([-0.0137,  0.0775])\n",
      "Epoch 2147, Loss 2.945814609527588\n",
      "\t Params: tensor([  5.2874, -16.8501])\n",
      "\t Grad: tensor([-0.0137,  0.0774])\n",
      "Epoch 2148, Loss 2.9457528591156006\n",
      "\t Params: tensor([  5.2875, -16.8509])\n",
      "\t Grad: tensor([-0.0136,  0.0772])\n",
      "Epoch 2149, Loss 2.945690155029297\n",
      "\t Params: tensor([  5.2877, -16.8517])\n",
      "\t Grad: tensor([-0.0136,  0.0771])\n",
      "Epoch 2150, Loss 2.9456300735473633\n",
      "\t Params: tensor([  5.2878, -16.8524])\n",
      "\t Grad: tensor([-0.0136,  0.0770])\n",
      "Epoch 2151, Loss 2.9455673694610596\n",
      "\t Params: tensor([  5.2879, -16.8532])\n",
      "\t Grad: tensor([-0.0136,  0.0768])\n",
      "Epoch 2152, Loss 2.9455084800720215\n",
      "\t Params: tensor([  5.2881, -16.8540])\n",
      "\t Grad: tensor([-0.0135,  0.0767])\n",
      "Epoch 2153, Loss 2.9454474449157715\n",
      "\t Params: tensor([  5.2882, -16.8547])\n",
      "\t Grad: tensor([-0.0135,  0.0766])\n",
      "Epoch 2154, Loss 2.945385217666626\n",
      "\t Params: tensor([  5.2884, -16.8555])\n",
      "\t Grad: tensor([-0.0135,  0.0765])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2155, Loss 2.9453253746032715\n",
      "\t Params: tensor([  5.2885, -16.8563])\n",
      "\t Grad: tensor([-0.0135,  0.0763])\n",
      "Epoch 2156, Loss 2.9452669620513916\n",
      "\t Params: tensor([  5.2886, -16.8570])\n",
      "\t Grad: tensor([-0.0135,  0.0762])\n",
      "Epoch 2157, Loss 2.9452064037323\n",
      "\t Params: tensor([  5.2888, -16.8578])\n",
      "\t Grad: tensor([-0.0134,  0.0761])\n",
      "Epoch 2158, Loss 2.945146083831787\n",
      "\t Params: tensor([  5.2889, -16.8585])\n",
      "\t Grad: tensor([-0.0134,  0.0759])\n",
      "Epoch 2159, Loss 2.9450876712799072\n",
      "\t Params: tensor([  5.2890, -16.8593])\n",
      "\t Grad: tensor([-0.0134,  0.0758])\n",
      "Epoch 2160, Loss 2.9450278282165527\n",
      "\t Params: tensor([  5.2892, -16.8601])\n",
      "\t Grad: tensor([-0.0134,  0.0757])\n",
      "Epoch 2161, Loss 2.944969415664673\n",
      "\t Params: tensor([  5.2893, -16.8608])\n",
      "\t Grad: tensor([-0.0133,  0.0755])\n",
      "Epoch 2162, Loss 2.944911003112793\n",
      "\t Params: tensor([  5.2894, -16.8616])\n",
      "\t Grad: tensor([-0.0133,  0.0754])\n",
      "Epoch 2163, Loss 2.944852352142334\n",
      "\t Params: tensor([  5.2896, -16.8623])\n",
      "\t Grad: tensor([-0.0133,  0.0753])\n",
      "Epoch 2164, Loss 2.9447922706604004\n",
      "\t Params: tensor([  5.2897, -16.8631])\n",
      "\t Grad: tensor([-0.0133,  0.0752])\n",
      "Epoch 2165, Loss 2.9447362422943115\n",
      "\t Params: tensor([  5.2898, -16.8638])\n",
      "\t Grad: tensor([-0.0133,  0.0750])\n",
      "Epoch 2166, Loss 2.9446775913238525\n",
      "\t Params: tensor([  5.2900, -16.8646])\n",
      "\t Grad: tensor([-0.0132,  0.0749])\n",
      "Epoch 2167, Loss 2.9446191787719727\n",
      "\t Params: tensor([  5.2901, -16.8653])\n",
      "\t Grad: tensor([-0.0132,  0.0748])\n",
      "Epoch 2168, Loss 2.944561719894409\n",
      "\t Params: tensor([  5.2902, -16.8661])\n",
      "\t Grad: tensor([-0.0132,  0.0747])\n",
      "Epoch 2169, Loss 2.9445037841796875\n",
      "\t Params: tensor([  5.2903, -16.8668])\n",
      "\t Grad: tensor([-0.0132,  0.0745])\n",
      "Epoch 2170, Loss 2.9444468021392822\n",
      "\t Params: tensor([  5.2905, -16.8676])\n",
      "\t Grad: tensor([-0.0132,  0.0744])\n",
      "Epoch 2171, Loss 2.9443907737731934\n",
      "\t Params: tensor([  5.2906, -16.8683])\n",
      "\t Grad: tensor([-0.0131,  0.0743])\n",
      "Epoch 2172, Loss 2.9443323612213135\n",
      "\t Params: tensor([  5.2907, -16.8690])\n",
      "\t Grad: tensor([-0.0131,  0.0742])\n",
      "Epoch 2173, Loss 2.9442763328552246\n",
      "\t Params: tensor([  5.2909, -16.8698])\n",
      "\t Grad: tensor([-0.0131,  0.0740])\n",
      "Epoch 2174, Loss 2.9442200660705566\n",
      "\t Params: tensor([  5.2910, -16.8705])\n",
      "\t Grad: tensor([-0.0131,  0.0739])\n",
      "Epoch 2175, Loss 2.9441640377044678\n",
      "\t Params: tensor([  5.2911, -16.8713])\n",
      "\t Grad: tensor([-0.0130,  0.0738])\n",
      "Epoch 2176, Loss 2.9441077709198\n",
      "\t Params: tensor([  5.2913, -16.8720])\n",
      "\t Grad: tensor([-0.0130,  0.0736])\n",
      "Epoch 2177, Loss 2.9440526962280273\n",
      "\t Params: tensor([  5.2914, -16.8727])\n",
      "\t Grad: tensor([-0.0130,  0.0735])\n",
      "Epoch 2178, Loss 2.9439961910247803\n",
      "\t Params: tensor([  5.2915, -16.8735])\n",
      "\t Grad: tensor([-0.0130,  0.0734])\n",
      "Epoch 2179, Loss 2.9439408779144287\n",
      "\t Params: tensor([  5.2917, -16.8742])\n",
      "\t Grad: tensor([-0.0129,  0.0733])\n",
      "Epoch 2180, Loss 2.9438865184783936\n",
      "\t Params: tensor([  5.2918, -16.8749])\n",
      "\t Grad: tensor([-0.0129,  0.0731])\n",
      "Epoch 2181, Loss 2.943830728530884\n",
      "\t Params: tensor([  5.2919, -16.8757])\n",
      "\t Grad: tensor([-0.0129,  0.0730])\n",
      "Epoch 2182, Loss 2.9437758922576904\n",
      "\t Params: tensor([  5.2920, -16.8764])\n",
      "\t Grad: tensor([-0.0129,  0.0729])\n",
      "Epoch 2183, Loss 2.943721055984497\n",
      "\t Params: tensor([  5.2922, -16.8771])\n",
      "\t Grad: tensor([-0.0129,  0.0728])\n",
      "Epoch 2184, Loss 2.9436662197113037\n",
      "\t Params: tensor([  5.2923, -16.8778])\n",
      "\t Grad: tensor([-0.0128,  0.0727])\n",
      "Epoch 2185, Loss 2.943612575531006\n",
      "\t Params: tensor([  5.2924, -16.8786])\n",
      "\t Grad: tensor([-0.0128,  0.0725])\n",
      "Epoch 2186, Loss 2.9435575008392334\n",
      "\t Params: tensor([  5.2926, -16.8793])\n",
      "\t Grad: tensor([-0.0128,  0.0724])\n",
      "Epoch 2187, Loss 2.9435033798217773\n",
      "\t Params: tensor([  5.2927, -16.8800])\n",
      "\t Grad: tensor([-0.0128,  0.0723])\n",
      "Epoch 2188, Loss 2.943450689315796\n",
      "\t Params: tensor([  5.2928, -16.8807])\n",
      "\t Grad: tensor([-0.0127,  0.0722])\n",
      "Epoch 2189, Loss 2.9433951377868652\n",
      "\t Params: tensor([  5.2929, -16.8815])\n",
      "\t Grad: tensor([-0.0127,  0.0720])\n",
      "Epoch 2190, Loss 2.943342685699463\n",
      "\t Params: tensor([  5.2931, -16.8822])\n",
      "\t Grad: tensor([-0.0127,  0.0719])\n",
      "Epoch 2191, Loss 2.9432904720306396\n",
      "\t Params: tensor([  5.2932, -16.8829])\n",
      "\t Grad: tensor([-0.0127,  0.0718])\n",
      "Epoch 2192, Loss 2.943234920501709\n",
      "\t Params: tensor([  5.2933, -16.8836])\n",
      "\t Grad: tensor([-0.0127,  0.0717])\n",
      "Epoch 2193, Loss 2.943183183670044\n",
      "\t Params: tensor([  5.2934, -16.8843])\n",
      "\t Grad: tensor([-0.0126,  0.0715])\n",
      "Epoch 2194, Loss 2.9431302547454834\n",
      "\t Params: tensor([  5.2936, -16.8850])\n",
      "\t Grad: tensor([-0.0126,  0.0714])\n",
      "Epoch 2195, Loss 2.9430794715881348\n",
      "\t Params: tensor([  5.2937, -16.8857])\n",
      "\t Grad: tensor([-0.0126,  0.0713])\n",
      "Epoch 2196, Loss 2.943026542663574\n",
      "\t Params: tensor([  5.2938, -16.8865])\n",
      "\t Grad: tensor([-0.0126,  0.0712])\n",
      "Epoch 2197, Loss 2.9429733753204346\n",
      "\t Params: tensor([  5.2939, -16.8872])\n",
      "\t Grad: tensor([-0.0126,  0.0711])\n",
      "Epoch 2198, Loss 2.9429221153259277\n",
      "\t Params: tensor([  5.2941, -16.8879])\n",
      "\t Grad: tensor([-0.0125,  0.0709])\n",
      "Epoch 2199, Loss 2.9428696632385254\n",
      "\t Params: tensor([  5.2942, -16.8886])\n",
      "\t Grad: tensor([-0.0125,  0.0708])\n",
      "Epoch 2200, Loss 2.9428184032440186\n",
      "\t Params: tensor([  5.2943, -16.8893])\n",
      "\t Grad: tensor([-0.0125,  0.0707])\n",
      "Epoch 2201, Loss 2.942765712738037\n",
      "\t Params: tensor([  5.2944, -16.8900])\n",
      "\t Grad: tensor([-0.0125,  0.0706])\n",
      "Epoch 2202, Loss 2.9427144527435303\n",
      "\t Params: tensor([  5.2946, -16.8907])\n",
      "\t Grad: tensor([-0.0124,  0.0705])\n",
      "Epoch 2203, Loss 2.942664861679077\n",
      "\t Params: tensor([  5.2947, -16.8914])\n",
      "\t Grad: tensor([-0.0124,  0.0703])\n",
      "Epoch 2204, Loss 2.942612409591675\n",
      "\t Params: tensor([  5.2948, -16.8921])\n",
      "\t Grad: tensor([-0.0124,  0.0702])\n",
      "Epoch 2205, Loss 2.942563533782959\n",
      "\t Params: tensor([  5.2949, -16.8928])\n",
      "\t Grad: tensor([-0.0124,  0.0701])\n",
      "Epoch 2206, Loss 2.9425103664398193\n",
      "\t Params: tensor([  5.2951, -16.8935])\n",
      "\t Grad: tensor([-0.0124,  0.0700])\n",
      "Epoch 2207, Loss 2.9424614906311035\n",
      "\t Params: tensor([  5.2952, -16.8942])\n",
      "\t Grad: tensor([-0.0123,  0.0699])\n",
      "Epoch 2208, Loss 2.942411184310913\n",
      "\t Params: tensor([  5.2953, -16.8949])\n",
      "\t Grad: tensor([-0.0123,  0.0697])\n",
      "Epoch 2209, Loss 2.9423606395721436\n",
      "\t Params: tensor([  5.2954, -16.8956])\n",
      "\t Grad: tensor([-0.0123,  0.0696])\n",
      "Epoch 2210, Loss 2.942310333251953\n",
      "\t Params: tensor([  5.2956, -16.8963])\n",
      "\t Grad: tensor([-0.0123,  0.0695])\n",
      "Epoch 2211, Loss 2.942260503768921\n",
      "\t Params: tensor([  5.2957, -16.8970])\n",
      "\t Grad: tensor([-0.0122,  0.0694])\n",
      "Epoch 2212, Loss 2.942211389541626\n",
      "\t Params: tensor([  5.2958, -16.8977])\n",
      "\t Grad: tensor([-0.0122,  0.0693])\n",
      "Epoch 2213, Loss 2.9421615600585938\n",
      "\t Params: tensor([  5.2959, -16.8984])\n",
      "\t Grad: tensor([-0.0122,  0.0692])\n",
      "Epoch 2214, Loss 2.9421119689941406\n",
      "\t Params: tensor([  5.2960, -16.8991])\n",
      "\t Grad: tensor([-0.0122,  0.0690])\n",
      "Epoch 2215, Loss 2.9420621395111084\n",
      "\t Params: tensor([  5.2962, -16.8998])\n",
      "\t Grad: tensor([-0.0122,  0.0689])\n",
      "Epoch 2216, Loss 2.942014217376709\n",
      "\t Params: tensor([  5.2963, -16.9004])\n",
      "\t Grad: tensor([-0.0122,  0.0688])\n",
      "Epoch 2217, Loss 2.941964626312256\n",
      "\t Params: tensor([  5.2964, -16.9011])\n",
      "\t Grad: tensor([-0.0121,  0.0687])\n",
      "Epoch 2218, Loss 2.941917657852173\n",
      "\t Params: tensor([  5.2965, -16.9018])\n",
      "\t Grad: tensor([-0.0121,  0.0686])\n",
      "Epoch 2219, Loss 2.9418678283691406\n",
      "\t Params: tensor([  5.2967, -16.9025])\n",
      "\t Grad: tensor([-0.0121,  0.0685])\n",
      "Epoch 2220, Loss 2.9418210983276367\n",
      "\t Params: tensor([  5.2968, -16.9032])\n",
      "\t Grad: tensor([-0.0121,  0.0683])\n",
      "Epoch 2221, Loss 2.9417734146118164\n",
      "\t Params: tensor([  5.2969, -16.9039])\n",
      "\t Grad: tensor([-0.0120,  0.0682])\n",
      "Epoch 2222, Loss 2.9417243003845215\n",
      "\t Params: tensor([  5.2970, -16.9046])\n",
      "\t Grad: tensor([-0.0120,  0.0681])\n",
      "Epoch 2223, Loss 2.9416770935058594\n",
      "\t Params: tensor([  5.2971, -16.9052])\n",
      "\t Grad: tensor([-0.0120,  0.0680])\n",
      "Epoch 2224, Loss 2.941628932952881\n",
      "\t Params: tensor([  5.2973, -16.9059])\n",
      "\t Grad: tensor([-0.0120,  0.0679])\n",
      "Epoch 2225, Loss 2.9415817260742188\n",
      "\t Params: tensor([  5.2974, -16.9066])\n",
      "\t Grad: tensor([-0.0120,  0.0678])\n",
      "Epoch 2226, Loss 2.9415342807769775\n",
      "\t Params: tensor([  5.2975, -16.9073])\n",
      "\t Grad: tensor([-0.0119,  0.0676])\n",
      "Epoch 2227, Loss 2.9414877891540527\n",
      "\t Params: tensor([  5.2976, -16.9079])\n",
      "\t Grad: tensor([-0.0119,  0.0675])\n",
      "Epoch 2228, Loss 2.9414398670196533\n",
      "\t Params: tensor([  5.2977, -16.9086])\n",
      "\t Grad: tensor([-0.0119,  0.0674])\n",
      "Epoch 2229, Loss 2.941392660140991\n",
      "\t Params: tensor([  5.2979, -16.9093])\n",
      "\t Grad: tensor([-0.0119,  0.0673])\n",
      "Epoch 2230, Loss 2.9413461685180664\n",
      "\t Params: tensor([  5.2980, -16.9100])\n",
      "\t Grad: tensor([-0.0119,  0.0672])\n",
      "Epoch 2231, Loss 2.941298723220825\n",
      "\t Params: tensor([  5.2981, -16.9106])\n",
      "\t Grad: tensor([-0.0118,  0.0671])\n",
      "Epoch 2232, Loss 2.9412527084350586\n",
      "\t Params: tensor([  5.2982, -16.9113])\n",
      "\t Grad: tensor([-0.0118,  0.0670])\n",
      "Epoch 2233, Loss 2.941206455230713\n",
      "\t Params: tensor([  5.2983, -16.9120])\n",
      "\t Grad: tensor([-0.0118,  0.0668])\n",
      "Epoch 2234, Loss 2.9411628246307373\n",
      "\t Params: tensor([  5.2984, -16.9126])\n",
      "\t Grad: tensor([-0.0118,  0.0667])\n",
      "Epoch 2235, Loss 2.941115617752075\n",
      "\t Params: tensor([  5.2986, -16.9133])\n",
      "\t Grad: tensor([-0.0118,  0.0666])\n",
      "Epoch 2236, Loss 2.9410698413848877\n",
      "\t Params: tensor([  5.2987, -16.9140])\n",
      "\t Grad: tensor([-0.0117,  0.0665])\n",
      "Epoch 2237, Loss 2.9410247802734375\n",
      "\t Params: tensor([  5.2988, -16.9146])\n",
      "\t Grad: tensor([-0.0117,  0.0664])\n",
      "Epoch 2238, Loss 2.940978765487671\n",
      "\t Params: tensor([  5.2989, -16.9153])\n",
      "\t Grad: tensor([-0.0117,  0.0663])\n",
      "Epoch 2239, Loss 2.9409332275390625\n",
      "\t Params: tensor([  5.2990, -16.9160])\n",
      "\t Grad: tensor([-0.0117,  0.0662])\n",
      "Epoch 2240, Loss 2.940889596939087\n",
      "\t Params: tensor([  5.2991, -16.9166])\n",
      "\t Grad: tensor([-0.0117,  0.0661])\n",
      "Epoch 2241, Loss 2.9408440589904785\n",
      "\t Params: tensor([  5.2993, -16.9173])\n",
      "\t Grad: tensor([-0.0117,  0.0659])\n",
      "Epoch 2242, Loss 2.940798282623291\n",
      "\t Params: tensor([  5.2994, -16.9179])\n",
      "\t Grad: tensor([-0.0116,  0.0658])\n",
      "Epoch 2243, Loss 2.940753221511841\n",
      "\t Params: tensor([  5.2995, -16.9186])\n",
      "\t Grad: tensor([-0.0116,  0.0657])\n",
      "Epoch 2244, Loss 2.9407105445861816\n",
      "\t Params: tensor([  5.2996, -16.9192])\n",
      "\t Grad: tensor([-0.0116,  0.0656])\n",
      "Epoch 2245, Loss 2.9406661987304688\n",
      "\t Params: tensor([  5.2997, -16.9199])\n",
      "\t Grad: tensor([-0.0116,  0.0655])\n",
      "Epoch 2246, Loss 2.9406211376190186\n",
      "\t Params: tensor([  5.2998, -16.9206])\n",
      "\t Grad: tensor([-0.0115,  0.0654])\n",
      "Epoch 2247, Loss 2.9405760765075684\n",
      "\t Params: tensor([  5.3000, -16.9212])\n",
      "\t Grad: tensor([-0.0115,  0.0653])\n",
      "Epoch 2248, Loss 2.940532922744751\n",
      "\t Params: tensor([  5.3001, -16.9219])\n",
      "\t Grad: tensor([-0.0115,  0.0652])\n",
      "Epoch 2249, Loss 2.9404892921447754\n",
      "\t Params: tensor([  5.3002, -16.9225])\n",
      "\t Grad: tensor([-0.0115,  0.0650])\n",
      "Epoch 2250, Loss 2.9404456615448\n",
      "\t Params: tensor([  5.3003, -16.9232])\n",
      "\t Grad: tensor([-0.0115,  0.0649])\n",
      "Epoch 2251, Loss 2.9404027462005615\n",
      "\t Params: tensor([  5.3004, -16.9238])\n",
      "\t Grad: tensor([-0.0114,  0.0648])\n",
      "Epoch 2252, Loss 2.9403581619262695\n",
      "\t Params: tensor([  5.3005, -16.9245])\n",
      "\t Grad: tensor([-0.0114,  0.0647])\n",
      "Epoch 2253, Loss 2.9403159618377686\n",
      "\t Params: tensor([  5.3006, -16.9251])\n",
      "\t Grad: tensor([-0.0114,  0.0646])\n",
      "Epoch 2254, Loss 2.940274238586426\n",
      "\t Params: tensor([  5.3008, -16.9257])\n",
      "\t Grad: tensor([-0.0114,  0.0645])\n",
      "Epoch 2255, Loss 2.9402289390563965\n",
      "\t Params: tensor([  5.3009, -16.9264])\n",
      "\t Grad: tensor([-0.0114,  0.0644])\n",
      "Epoch 2256, Loss 2.940187692642212\n",
      "\t Params: tensor([  5.3010, -16.9270])\n",
      "\t Grad: tensor([-0.0114,  0.0643])\n",
      "Epoch 2257, Loss 2.940143585205078\n",
      "\t Params: tensor([  5.3011, -16.9277])\n",
      "\t Grad: tensor([-0.0114,  0.0642])\n",
      "Epoch 2258, Loss 2.9401016235351562\n",
      "\t Params: tensor([  5.3012, -16.9283])\n",
      "\t Grad: tensor([-0.0113,  0.0641])\n",
      "Epoch 2259, Loss 2.9400599002838135\n",
      "\t Params: tensor([  5.3013, -16.9290])\n",
      "\t Grad: tensor([-0.0113,  0.0640])\n",
      "Epoch 2260, Loss 2.94001841545105\n",
      "\t Params: tensor([  5.3014, -16.9296])\n",
      "\t Grad: tensor([-0.0113,  0.0638])\n",
      "Epoch 2261, Loss 2.9399771690368652\n",
      "\t Params: tensor([  5.3016, -16.9302])\n",
      "\t Grad: tensor([-0.0113,  0.0637])\n",
      "Epoch 2262, Loss 2.9399335384368896\n",
      "\t Params: tensor([  5.3017, -16.9309])\n",
      "\t Grad: tensor([-0.0112,  0.0636])\n",
      "Epoch 2263, Loss 2.9398910999298096\n",
      "\t Params: tensor([  5.3018, -16.9315])\n",
      "\t Grad: tensor([-0.0112,  0.0635])\n",
      "Epoch 2264, Loss 2.9398512840270996\n",
      "\t Params: tensor([  5.3019, -16.9321])\n",
      "\t Grad: tensor([-0.0112,  0.0634])\n",
      "Epoch 2265, Loss 2.9398093223571777\n",
      "\t Params: tensor([  5.3020, -16.9328])\n",
      "\t Grad: tensor([-0.0112,  0.0633])\n",
      "Epoch 2266, Loss 2.9397695064544678\n",
      "\t Params: tensor([  5.3021, -16.9334])\n",
      "\t Grad: tensor([-0.0112,  0.0632])\n",
      "Epoch 2267, Loss 2.9397268295288086\n",
      "\t Params: tensor([  5.3022, -16.9340])\n",
      "\t Grad: tensor([-0.0111,  0.0631])\n",
      "Epoch 2268, Loss 2.939685583114624\n",
      "\t Params: tensor([  5.3023, -16.9347])\n",
      "\t Grad: tensor([-0.0111,  0.0630])\n",
      "Epoch 2269, Loss 2.939645528793335\n",
      "\t Params: tensor([  5.3024, -16.9353])\n",
      "\t Grad: tensor([-0.0111,  0.0629])\n",
      "Epoch 2270, Loss 2.9396049976348877\n",
      "\t Params: tensor([  5.3026, -16.9359])\n",
      "\t Grad: tensor([-0.0111,  0.0628])\n",
      "Epoch 2271, Loss 2.939565896987915\n",
      "\t Params: tensor([  5.3027, -16.9365])\n",
      "\t Grad: tensor([-0.0111,  0.0627])\n",
      "Epoch 2272, Loss 2.9395220279693604\n",
      "\t Params: tensor([  5.3028, -16.9372])\n",
      "\t Grad: tensor([-0.0111,  0.0626])\n",
      "Epoch 2273, Loss 2.9394829273223877\n",
      "\t Params: tensor([  5.3029, -16.9378])\n",
      "\t Grad: tensor([-0.0110,  0.0624])\n",
      "Epoch 2274, Loss 2.9394431114196777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params: tensor([  5.3030, -16.9384])\n",
      "\t Grad: tensor([-0.0110,  0.0623])\n",
      "Epoch 2275, Loss 2.9394028186798096\n",
      "\t Params: tensor([  5.3031, -16.9390])\n",
      "\t Grad: tensor([-0.0110,  0.0622])\n",
      "Epoch 2276, Loss 2.939361333847046\n",
      "\t Params: tensor([  5.3032, -16.9397])\n",
      "\t Grad: tensor([-0.0110,  0.0621])\n",
      "Epoch 2277, Loss 2.9393234252929688\n",
      "\t Params: tensor([  5.3033, -16.9403])\n",
      "\t Grad: tensor([-0.0110,  0.0620])\n",
      "Epoch 2278, Loss 2.939282178878784\n",
      "\t Params: tensor([  5.3034, -16.9409])\n",
      "\t Grad: tensor([-0.0109,  0.0619])\n",
      "Epoch 2279, Loss 2.9392430782318115\n",
      "\t Params: tensor([  5.3035, -16.9415])\n",
      "\t Grad: tensor([-0.0109,  0.0618])\n",
      "Epoch 2280, Loss 2.9392054080963135\n",
      "\t Params: tensor([  5.3037, -16.9421])\n",
      "\t Grad: tensor([-0.0109,  0.0617])\n",
      "Epoch 2281, Loss 2.939164638519287\n",
      "\t Params: tensor([  5.3038, -16.9428])\n",
      "\t Grad: tensor([-0.0109,  0.0616])\n",
      "Epoch 2282, Loss 2.93912672996521\n",
      "\t Params: tensor([  5.3039, -16.9434])\n",
      "\t Grad: tensor([-0.0109,  0.0615])\n",
      "Epoch 2283, Loss 2.9390869140625\n",
      "\t Params: tensor([  5.3040, -16.9440])\n",
      "\t Grad: tensor([-0.0108,  0.0614])\n",
      "Epoch 2284, Loss 2.939049005508423\n",
      "\t Params: tensor([  5.3041, -16.9446])\n",
      "\t Grad: tensor([-0.0108,  0.0613])\n",
      "Epoch 2285, Loss 2.939011335372925\n",
      "\t Params: tensor([  5.3042, -16.9452])\n",
      "\t Grad: tensor([-0.0108,  0.0612])\n",
      "Epoch 2286, Loss 2.9389710426330566\n",
      "\t Params: tensor([  5.3043, -16.9458])\n",
      "\t Grad: tensor([-0.0108,  0.0611])\n",
      "Epoch 2287, Loss 2.9389328956604004\n",
      "\t Params: tensor([  5.3044, -16.9464])\n",
      "\t Grad: tensor([-0.0108,  0.0610])\n",
      "Epoch 2288, Loss 2.9388930797576904\n",
      "\t Params: tensor([  5.3045, -16.9470])\n",
      "\t Grad: tensor([-0.0108,  0.0609])\n",
      "Epoch 2289, Loss 2.938857078552246\n",
      "\t Params: tensor([  5.3046, -16.9476])\n",
      "\t Grad: tensor([-0.0107,  0.0608])\n",
      "Epoch 2290, Loss 2.9388198852539062\n",
      "\t Params: tensor([  5.3047, -16.9482])\n",
      "\t Grad: tensor([-0.0107,  0.0607])\n",
      "Epoch 2291, Loss 2.938779354095459\n",
      "\t Params: tensor([  5.3048, -16.9489])\n",
      "\t Grad: tensor([-0.0107,  0.0606])\n",
      "Epoch 2292, Loss 2.9387433528900146\n",
      "\t Params: tensor([  5.3049, -16.9495])\n",
      "\t Grad: tensor([-0.0107,  0.0605])\n",
      "Epoch 2293, Loss 2.9387052059173584\n",
      "\t Params: tensor([  5.3051, -16.9501])\n",
      "\t Grad: tensor([-0.0107,  0.0604])\n",
      "Epoch 2294, Loss 2.9386672973632812\n",
      "\t Params: tensor([  5.3052, -16.9507])\n",
      "\t Grad: tensor([-0.0106,  0.0603])\n",
      "Epoch 2295, Loss 2.938628911972046\n",
      "\t Params: tensor([  5.3053, -16.9513])\n",
      "\t Grad: tensor([-0.0106,  0.0602])\n",
      "Epoch 2296, Loss 2.9385931491851807\n",
      "\t Params: tensor([  5.3054, -16.9519])\n",
      "\t Grad: tensor([-0.0106,  0.0601])\n",
      "Epoch 2297, Loss 2.9385550022125244\n",
      "\t Params: tensor([  5.3055, -16.9525])\n",
      "\t Grad: tensor([-0.0106,  0.0600])\n",
      "Epoch 2298, Loss 2.938519239425659\n",
      "\t Params: tensor([  5.3056, -16.9531])\n",
      "\t Grad: tensor([-0.0106,  0.0598])\n",
      "Epoch 2299, Loss 2.9384806156158447\n",
      "\t Params: tensor([  5.3057, -16.9537])\n",
      "\t Grad: tensor([-0.0106,  0.0597])\n",
      "Epoch 2300, Loss 2.938443899154663\n",
      "\t Params: tensor([  5.3058, -16.9543])\n",
      "\t Grad: tensor([-0.0105,  0.0596])\n",
      "Epoch 2301, Loss 2.938408136367798\n",
      "\t Params: tensor([  5.3059, -16.9549])\n",
      "\t Grad: tensor([-0.0105,  0.0595])\n",
      "Epoch 2302, Loss 2.938371419906616\n",
      "\t Params: tensor([  5.3060, -16.9554])\n",
      "\t Grad: tensor([-0.0105,  0.0594])\n",
      "Epoch 2303, Loss 2.9383347034454346\n",
      "\t Params: tensor([  5.3061, -16.9560])\n",
      "\t Grad: tensor([-0.0105,  0.0593])\n",
      "Epoch 2304, Loss 2.9382987022399902\n",
      "\t Params: tensor([  5.3062, -16.9566])\n",
      "\t Grad: tensor([-0.0105,  0.0592])\n",
      "Epoch 2305, Loss 2.938262701034546\n",
      "\t Params: tensor([  5.3063, -16.9572])\n",
      "\t Grad: tensor([-0.0105,  0.0591])\n",
      "Epoch 2306, Loss 2.9382269382476807\n",
      "\t Params: tensor([  5.3064, -16.9578])\n",
      "\t Grad: tensor([-0.0104,  0.0590])\n",
      "Epoch 2307, Loss 2.938190460205078\n",
      "\t Params: tensor([  5.3065, -16.9584])\n",
      "\t Grad: tensor([-0.0104,  0.0589])\n",
      "Epoch 2308, Loss 2.938155174255371\n",
      "\t Params: tensor([  5.3066, -16.9590])\n",
      "\t Grad: tensor([-0.0104,  0.0588])\n",
      "Epoch 2309, Loss 2.9381182193756104\n",
      "\t Params: tensor([  5.3067, -16.9596])\n",
      "\t Grad: tensor([-0.0104,  0.0587])\n",
      "Epoch 2310, Loss 2.938084363937378\n",
      "\t Params: tensor([  5.3068, -16.9602])\n",
      "\t Grad: tensor([-0.0104,  0.0586])\n",
      "Epoch 2311, Loss 2.9380486011505127\n",
      "\t Params: tensor([  5.3069, -16.9608])\n",
      "\t Grad: tensor([-0.0103,  0.0585])\n",
      "Epoch 2312, Loss 2.9380135536193848\n",
      "\t Params: tensor([  5.3070, -16.9613])\n",
      "\t Grad: tensor([-0.0103,  0.0584])\n",
      "Epoch 2313, Loss 2.937976837158203\n",
      "\t Params: tensor([  5.3072, -16.9619])\n",
      "\t Grad: tensor([-0.0103,  0.0583])\n",
      "Epoch 2314, Loss 2.937943458557129\n",
      "\t Params: tensor([  5.3073, -16.9625])\n",
      "\t Grad: tensor([-0.0103,  0.0582])\n",
      "Epoch 2315, Loss 2.937908411026001\n",
      "\t Params: tensor([  5.3074, -16.9631])\n",
      "\t Grad: tensor([-0.0103,  0.0581])\n",
      "Epoch 2316, Loss 2.9378724098205566\n",
      "\t Params: tensor([  5.3075, -16.9637])\n",
      "\t Grad: tensor([-0.0103,  0.0580])\n",
      "Epoch 2317, Loss 2.937838554382324\n",
      "\t Params: tensor([  5.3076, -16.9642])\n",
      "\t Grad: tensor([-0.0102,  0.0580])\n",
      "Epoch 2318, Loss 2.9378042221069336\n",
      "\t Params: tensor([  5.3077, -16.9648])\n",
      "\t Grad: tensor([-0.0102,  0.0578])\n",
      "Epoch 2319, Loss 2.9377686977386475\n",
      "\t Params: tensor([  5.3078, -16.9654])\n",
      "\t Grad: tensor([-0.0102,  0.0578])\n",
      "Epoch 2320, Loss 2.937734365463257\n",
      "\t Params: tensor([  5.3079, -16.9660])\n",
      "\t Grad: tensor([-0.0102,  0.0577])\n",
      "Epoch 2321, Loss 2.937699794769287\n",
      "\t Params: tensor([  5.3080, -16.9666])\n",
      "\t Grad: tensor([-0.0102,  0.0576])\n",
      "Epoch 2322, Loss 2.9376649856567383\n",
      "\t Params: tensor([  5.3081, -16.9671])\n",
      "\t Grad: tensor([-0.0102,  0.0575])\n",
      "Epoch 2323, Loss 2.9376320838928223\n",
      "\t Params: tensor([  5.3082, -16.9677])\n",
      "\t Grad: tensor([-0.0101,  0.0574])\n",
      "Epoch 2324, Loss 2.9375975131988525\n",
      "\t Params: tensor([  5.3083, -16.9683])\n",
      "\t Grad: tensor([-0.0101,  0.0573])\n",
      "Epoch 2325, Loss 2.9375646114349365\n",
      "\t Params: tensor([  5.3084, -16.9688])\n",
      "\t Grad: tensor([-0.0101,  0.0572])\n",
      "Epoch 2326, Loss 2.937530755996704\n",
      "\t Params: tensor([  5.3085, -16.9694])\n",
      "\t Grad: tensor([-0.0101,  0.0571])\n",
      "Epoch 2327, Loss 2.9374985694885254\n",
      "\t Params: tensor([  5.3086, -16.9700])\n",
      "\t Grad: tensor([-0.0101,  0.0570])\n",
      "Epoch 2328, Loss 2.937464714050293\n",
      "\t Params: tensor([  5.3087, -16.9706])\n",
      "\t Grad: tensor([-0.0101,  0.0569])\n",
      "Epoch 2329, Loss 2.937429666519165\n",
      "\t Params: tensor([  5.3088, -16.9711])\n",
      "\t Grad: tensor([-0.0100,  0.0568])\n",
      "Epoch 2330, Loss 2.9373979568481445\n",
      "\t Params: tensor([  5.3089, -16.9717])\n",
      "\t Grad: tensor([-0.0100,  0.0567])\n",
      "Epoch 2331, Loss 2.937364339828491\n",
      "\t Params: tensor([  5.3090, -16.9723])\n",
      "\t Grad: tensor([-0.0100,  0.0566])\n",
      "Epoch 2332, Loss 2.9373323917388916\n",
      "\t Params: tensor([  5.3091, -16.9728])\n",
      "\t Grad: tensor([-0.0100,  0.0565])\n",
      "Epoch 2333, Loss 2.937298536300659\n",
      "\t Params: tensor([  5.3092, -16.9734])\n",
      "\t Grad: tensor([-0.0100,  0.0564])\n",
      "Epoch 2334, Loss 2.937265157699585\n",
      "\t Params: tensor([  5.3093, -16.9739])\n",
      "\t Grad: tensor([-0.0100,  0.0563])\n",
      "Epoch 2335, Loss 2.937232255935669\n",
      "\t Params: tensor([  5.3094, -16.9745])\n",
      "\t Grad: tensor([-0.0099,  0.0562])\n",
      "Epoch 2336, Loss 2.9372007846832275\n",
      "\t Params: tensor([  5.3095, -16.9751])\n",
      "\t Grad: tensor([-0.0099,  0.0561])\n",
      "Epoch 2337, Loss 2.9371674060821533\n",
      "\t Params: tensor([  5.3096, -16.9756])\n",
      "\t Grad: tensor([-0.0099,  0.0560])\n",
      "Epoch 2338, Loss 2.937134027481079\n",
      "\t Params: tensor([  5.3097, -16.9762])\n",
      "\t Grad: tensor([-0.0099,  0.0559])\n",
      "Epoch 2339, Loss 2.9371042251586914\n",
      "\t Params: tensor([  5.3098, -16.9767])\n",
      "\t Grad: tensor([-0.0099,  0.0558])\n",
      "Epoch 2340, Loss 2.9370713233947754\n",
      "\t Params: tensor([  5.3099, -16.9773])\n",
      "\t Grad: tensor([-0.0098,  0.0557])\n",
      "Epoch 2341, Loss 2.937039375305176\n",
      "\t Params: tensor([  5.3100, -16.9779])\n",
      "\t Grad: tensor([-0.0098,  0.0556])\n",
      "Epoch 2342, Loss 2.9370079040527344\n",
      "\t Params: tensor([  5.3101, -16.9784])\n",
      "\t Grad: tensor([-0.0098,  0.0555])\n",
      "Epoch 2343, Loss 2.9369757175445557\n",
      "\t Params: tensor([  5.3102, -16.9790])\n",
      "\t Grad: tensor([-0.0098,  0.0554])\n",
      "Epoch 2344, Loss 2.9369447231292725\n",
      "\t Params: tensor([  5.3103, -16.9795])\n",
      "\t Grad: tensor([-0.0098,  0.0553])\n",
      "Epoch 2345, Loss 2.9369115829467773\n",
      "\t Params: tensor([  5.3104, -16.9801])\n",
      "\t Grad: tensor([-0.0098,  0.0553])\n",
      "Epoch 2346, Loss 2.936882734298706\n",
      "\t Params: tensor([  5.3105, -16.9806])\n",
      "\t Grad: tensor([-0.0097,  0.0552])\n",
      "Epoch 2347, Loss 2.9368505477905273\n",
      "\t Params: tensor([  5.3106, -16.9812])\n",
      "\t Grad: tensor([-0.0097,  0.0551])\n",
      "Epoch 2348, Loss 2.936819314956665\n",
      "\t Params: tensor([  5.3107, -16.9817])\n",
      "\t Grad: tensor([-0.0097,  0.0550])\n",
      "Epoch 2349, Loss 2.936788320541382\n",
      "\t Params: tensor([  5.3107, -16.9823])\n",
      "\t Grad: tensor([-0.0097,  0.0549])\n",
      "Epoch 2350, Loss 2.9367570877075195\n",
      "\t Params: tensor([  5.3108, -16.9828])\n",
      "\t Grad: tensor([-0.0097,  0.0548])\n",
      "Epoch 2351, Loss 2.936724901199341\n",
      "\t Params: tensor([  5.3109, -16.9834])\n",
      "\t Grad: tensor([-0.0097,  0.0547])\n",
      "Epoch 2352, Loss 2.9366941452026367\n",
      "\t Params: tensor([  5.3110, -16.9839])\n",
      "\t Grad: tensor([-0.0096,  0.0546])\n",
      "Epoch 2353, Loss 2.936664581298828\n",
      "\t Params: tensor([  5.3111, -16.9845])\n",
      "\t Grad: tensor([-0.0096,  0.0545])\n",
      "Epoch 2354, Loss 2.936633348464966\n",
      "\t Params: tensor([  5.3112, -16.9850])\n",
      "\t Grad: tensor([-0.0096,  0.0544])\n",
      "Epoch 2355, Loss 2.9366021156311035\n",
      "\t Params: tensor([  5.3113, -16.9856])\n",
      "\t Grad: tensor([-0.0096,  0.0543])\n",
      "Epoch 2356, Loss 2.9365720748901367\n",
      "\t Params: tensor([  5.3114, -16.9861])\n",
      "\t Grad: tensor([-0.0096,  0.0542])\n",
      "Epoch 2357, Loss 2.9365415573120117\n",
      "\t Params: tensor([  5.3115, -16.9866])\n",
      "\t Grad: tensor([-0.0095,  0.0541])\n",
      "Epoch 2358, Loss 2.936511278152466\n",
      "\t Params: tensor([  5.3116, -16.9872])\n",
      "\t Grad: tensor([-0.0096,  0.0540])\n",
      "Epoch 2359, Loss 2.936481475830078\n",
      "\t Params: tensor([  5.3117, -16.9877])\n",
      "\t Grad: tensor([-0.0095,  0.0540])\n",
      "Epoch 2360, Loss 2.936450719833374\n",
      "\t Params: tensor([  5.3118, -16.9883])\n",
      "\t Grad: tensor([-0.0095,  0.0539])\n",
      "Epoch 2361, Loss 2.9364211559295654\n",
      "\t Params: tensor([  5.3119, -16.9888])\n",
      "\t Grad: tensor([-0.0095,  0.0538])\n",
      "Epoch 2362, Loss 2.936392307281494\n",
      "\t Params: tensor([  5.3120, -16.9893])\n",
      "\t Grad: tensor([-0.0095,  0.0537])\n",
      "Epoch 2363, Loss 2.93636155128479\n",
      "\t Params: tensor([  5.3121, -16.9899])\n",
      "\t Grad: tensor([-0.0094,  0.0536])\n",
      "Epoch 2364, Loss 2.9363324642181396\n",
      "\t Params: tensor([  5.3122, -16.9904])\n",
      "\t Grad: tensor([-0.0094,  0.0535])\n",
      "Epoch 2365, Loss 2.9363043308258057\n",
      "\t Params: tensor([  5.3123, -16.9909])\n",
      "\t Grad: tensor([-0.0094,  0.0534])\n",
      "Epoch 2366, Loss 2.9362738132476807\n",
      "\t Params: tensor([  5.3124, -16.9915])\n",
      "\t Grad: tensor([-0.0094,  0.0533])\n",
      "Epoch 2367, Loss 2.9362435340881348\n",
      "\t Params: tensor([  5.3125, -16.9920])\n",
      "\t Grad: tensor([-0.0094,  0.0532])\n",
      "Epoch 2368, Loss 2.936215877532959\n",
      "\t Params: tensor([  5.3126, -16.9925])\n",
      "\t Grad: tensor([-0.0094,  0.0531])\n",
      "Epoch 2369, Loss 2.936187505722046\n",
      "\t Params: tensor([  5.3127, -16.9931])\n",
      "\t Grad: tensor([-0.0094,  0.0530])\n",
      "Epoch 2370, Loss 2.9361562728881836\n",
      "\t Params: tensor([  5.3127, -16.9936])\n",
      "\t Grad: tensor([-0.0094,  0.0530])\n",
      "Epoch 2371, Loss 2.9361283779144287\n",
      "\t Params: tensor([  5.3128, -16.9941])\n",
      "\t Grad: tensor([-0.0093,  0.0529])\n",
      "Epoch 2372, Loss 2.9360997676849365\n",
      "\t Params: tensor([  5.3129, -16.9946])\n",
      "\t Grad: tensor([-0.0093,  0.0528])\n",
      "Epoch 2373, Loss 2.9360716342926025\n",
      "\t Params: tensor([  5.3130, -16.9952])\n",
      "\t Grad: tensor([-0.0093,  0.0527])\n",
      "Epoch 2374, Loss 2.936042070388794\n",
      "\t Params: tensor([  5.3131, -16.9957])\n",
      "\t Grad: tensor([-0.0093,  0.0526])\n",
      "Epoch 2375, Loss 2.936014413833618\n",
      "\t Params: tensor([  5.3132, -16.9962])\n",
      "\t Grad: tensor([-0.0093,  0.0525])\n",
      "Epoch 2376, Loss 2.935985803604126\n",
      "\t Params: tensor([  5.3133, -16.9967])\n",
      "\t Grad: tensor([-0.0093,  0.0524])\n",
      "Epoch 2377, Loss 2.935957193374634\n",
      "\t Params: tensor([  5.3134, -16.9973])\n",
      "\t Grad: tensor([-0.0093,  0.0523])\n",
      "Epoch 2378, Loss 2.9359281063079834\n",
      "\t Params: tensor([  5.3135, -16.9978])\n",
      "\t Grad: tensor([-0.0092,  0.0522])\n",
      "Epoch 2379, Loss 2.9359006881713867\n",
      "\t Params: tensor([  5.3136, -16.9983])\n",
      "\t Grad: tensor([-0.0092,  0.0522])\n",
      "Epoch 2380, Loss 2.935872793197632\n",
      "\t Params: tensor([  5.3137, -16.9988])\n",
      "\t Grad: tensor([-0.0092,  0.0521])\n",
      "Epoch 2381, Loss 2.935845136642456\n",
      "\t Params: tensor([  5.3138, -16.9994])\n",
      "\t Grad: tensor([-0.0092,  0.0520])\n",
      "Epoch 2382, Loss 2.935817003250122\n",
      "\t Params: tensor([  5.3139, -16.9999])\n",
      "\t Grad: tensor([-0.0092,  0.0519])\n",
      "Epoch 2383, Loss 2.935788869857788\n",
      "\t Params: tensor([  5.3139, -17.0004])\n",
      "\t Grad: tensor([-0.0092,  0.0518])\n",
      "Epoch 2384, Loss 2.9357619285583496\n",
      "\t Params: tensor([  5.3140, -17.0009])\n",
      "\t Grad: tensor([-0.0092,  0.0517])\n",
      "Epoch 2385, Loss 2.9357335567474365\n",
      "\t Params: tensor([  5.3141, -17.0014])\n",
      "\t Grad: tensor([-0.0091,  0.0516])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2386, Loss 2.9357070922851562\n",
      "\t Params: tensor([  5.3142, -17.0019])\n",
      "\t Grad: tensor([-0.0091,  0.0515])\n",
      "Epoch 2387, Loss 2.935678720474243\n",
      "\t Params: tensor([  5.3143, -17.0025])\n",
      "\t Grad: tensor([-0.0091,  0.0514])\n",
      "Epoch 2388, Loss 2.935650110244751\n",
      "\t Params: tensor([  5.3144, -17.0030])\n",
      "\t Grad: tensor([-0.0091,  0.0514])\n",
      "Epoch 2389, Loss 2.9356260299682617\n",
      "\t Params: tensor([  5.3145, -17.0035])\n",
      "\t Grad: tensor([-0.0090,  0.0513])\n",
      "Epoch 2390, Loss 2.935596466064453\n",
      "\t Params: tensor([  5.3146, -17.0040])\n",
      "\t Grad: tensor([-0.0090,  0.0512])\n",
      "Epoch 2391, Loss 2.9355711936950684\n",
      "\t Params: tensor([  5.3147, -17.0045])\n",
      "\t Grad: tensor([-0.0090,  0.0511])\n",
      "Epoch 2392, Loss 2.935544490814209\n",
      "\t Params: tensor([  5.3148, -17.0050])\n",
      "\t Grad: tensor([-0.0090,  0.0510])\n",
      "Epoch 2393, Loss 2.935516357421875\n",
      "\t Params: tensor([  5.3149, -17.0055])\n",
      "\t Grad: tensor([-0.0090,  0.0509])\n",
      "Epoch 2394, Loss 2.9354889392852783\n",
      "\t Params: tensor([  5.3149, -17.0060])\n",
      "\t Grad: tensor([-0.0090,  0.0508])\n",
      "Epoch 2395, Loss 2.93546462059021\n",
      "\t Params: tensor([  5.3150, -17.0065])\n",
      "\t Grad: tensor([-0.0090,  0.0507])\n",
      "Epoch 2396, Loss 2.935436487197876\n",
      "\t Params: tensor([  5.3151, -17.0070])\n",
      "\t Grad: tensor([-0.0090,  0.0507])\n",
      "Epoch 2397, Loss 2.935411214828491\n",
      "\t Params: tensor([  5.3152, -17.0076])\n",
      "\t Grad: tensor([-0.0089,  0.0506])\n",
      "Epoch 2398, Loss 2.935385227203369\n",
      "\t Params: tensor([  5.3153, -17.0081])\n",
      "\t Grad: tensor([-0.0089,  0.0505])\n",
      "Epoch 2399, Loss 2.935356378555298\n",
      "\t Params: tensor([  5.3154, -17.0086])\n",
      "\t Grad: tensor([-0.0089,  0.0504])\n",
      "Epoch 2400, Loss 2.9353315830230713\n",
      "\t Params: tensor([  5.3155, -17.0091])\n",
      "\t Grad: tensor([-0.0089,  0.0503])\n",
      "Epoch 2401, Loss 2.9353041648864746\n",
      "\t Params: tensor([  5.3156, -17.0096])\n",
      "\t Grad: tensor([-0.0089,  0.0502])\n",
      "Epoch 2402, Loss 2.9352805614471436\n",
      "\t Params: tensor([  5.3157, -17.0101])\n",
      "\t Grad: tensor([-0.0088,  0.0502])\n",
      "Epoch 2403, Loss 2.9352524280548096\n",
      "\t Params: tensor([  5.3157, -17.0106])\n",
      "\t Grad: tensor([-0.0088,  0.0501])\n",
      "Epoch 2404, Loss 2.935228109359741\n",
      "\t Params: tensor([  5.3158, -17.0111])\n",
      "\t Grad: tensor([-0.0088,  0.0500])\n",
      "Epoch 2405, Loss 2.9352033138275146\n",
      "\t Params: tensor([  5.3159, -17.0116])\n",
      "\t Grad: tensor([-0.0088,  0.0499])\n",
      "Epoch 2406, Loss 2.9351766109466553\n",
      "\t Params: tensor([  5.3160, -17.0121])\n",
      "\t Grad: tensor([-0.0088,  0.0498])\n",
      "Epoch 2407, Loss 2.9351515769958496\n",
      "\t Params: tensor([  5.3161, -17.0126])\n",
      "\t Grad: tensor([-0.0088,  0.0497])\n",
      "Epoch 2408, Loss 2.9351258277893066\n",
      "\t Params: tensor([  5.3162, -17.0131])\n",
      "\t Grad: tensor([-0.0088,  0.0496])\n",
      "Epoch 2409, Loss 2.9350996017456055\n",
      "\t Params: tensor([  5.3163, -17.0136])\n",
      "\t Grad: tensor([-0.0088,  0.0496])\n",
      "Epoch 2410, Loss 2.9350745677948\n",
      "\t Params: tensor([  5.3164, -17.0140])\n",
      "\t Grad: tensor([-0.0087,  0.0495])\n",
      "Epoch 2411, Loss 2.9350485801696777\n",
      "\t Params: tensor([  5.3164, -17.0145])\n",
      "\t Grad: tensor([-0.0087,  0.0494])\n",
      "Epoch 2412, Loss 2.9350240230560303\n",
      "\t Params: tensor([  5.3165, -17.0150])\n",
      "\t Grad: tensor([-0.0087,  0.0493])\n",
      "Epoch 2413, Loss 2.9350011348724365\n",
      "\t Params: tensor([  5.3166, -17.0155])\n",
      "\t Grad: tensor([-0.0087,  0.0492])\n",
      "Epoch 2414, Loss 2.9349730014801025\n",
      "\t Params: tensor([  5.3167, -17.0160])\n",
      "\t Grad: tensor([-0.0087,  0.0491])\n",
      "Epoch 2415, Loss 2.934948682785034\n",
      "\t Params: tensor([  5.3168, -17.0165])\n",
      "\t Grad: tensor([-0.0087,  0.0491])\n",
      "Epoch 2416, Loss 2.9349253177642822\n",
      "\t Params: tensor([  5.3169, -17.0170])\n",
      "\t Grad: tensor([-0.0086,  0.0490])\n",
      "Epoch 2417, Loss 2.934899091720581\n",
      "\t Params: tensor([  5.3170, -17.0175])\n",
      "\t Grad: tensor([-0.0086,  0.0489])\n",
      "Epoch 2418, Loss 2.934875726699829\n",
      "\t Params: tensor([  5.3171, -17.0180])\n",
      "\t Grad: tensor([-0.0086,  0.0488])\n",
      "Epoch 2419, Loss 2.9348526000976562\n",
      "\t Params: tensor([  5.3171, -17.0185])\n",
      "\t Grad: tensor([-0.0086,  0.0487])\n",
      "Epoch 2420, Loss 2.934826135635376\n",
      "\t Params: tensor([  5.3172, -17.0189])\n",
      "\t Grad: tensor([-0.0086,  0.0486])\n",
      "Epoch 2421, Loss 2.9348020553588867\n",
      "\t Params: tensor([  5.3173, -17.0194])\n",
      "\t Grad: tensor([-0.0086,  0.0486])\n",
      "Epoch 2422, Loss 2.93477725982666\n",
      "\t Params: tensor([  5.3174, -17.0199])\n",
      "\t Grad: tensor([-0.0086,  0.0485])\n",
      "Epoch 2423, Loss 2.934753179550171\n",
      "\t Params: tensor([  5.3175, -17.0204])\n",
      "\t Grad: tensor([-0.0086,  0.0484])\n",
      "Epoch 2424, Loss 2.93472957611084\n",
      "\t Params: tensor([  5.3176, -17.0209])\n",
      "\t Grad: tensor([-0.0086,  0.0483])\n",
      "Epoch 2425, Loss 2.9347054958343506\n",
      "\t Params: tensor([  5.3177, -17.0214])\n",
      "\t Grad: tensor([-0.0085,  0.0482])\n",
      "Epoch 2426, Loss 2.934680938720703\n",
      "\t Params: tensor([  5.3177, -17.0219])\n",
      "\t Grad: tensor([-0.0085,  0.0481])\n",
      "Epoch 2427, Loss 2.9346578121185303\n",
      "\t Params: tensor([  5.3178, -17.0223])\n",
      "\t Grad: tensor([-0.0085,  0.0481])\n",
      "Epoch 2428, Loss 2.9346354007720947\n",
      "\t Params: tensor([  5.3179, -17.0228])\n",
      "\t Grad: tensor([-0.0085,  0.0480])\n",
      "Epoch 2429, Loss 2.9346091747283936\n",
      "\t Params: tensor([  5.3180, -17.0233])\n",
      "\t Grad: tensor([-0.0085,  0.0479])\n",
      "Epoch 2430, Loss 2.934584856033325\n",
      "\t Params: tensor([  5.3181, -17.0238])\n",
      "\t Grad: tensor([-0.0084,  0.0478])\n",
      "Epoch 2431, Loss 2.934563398361206\n",
      "\t Params: tensor([  5.3182, -17.0242])\n",
      "\t Grad: tensor([-0.0084,  0.0477])\n",
      "Epoch 2432, Loss 2.9345412254333496\n",
      "\t Params: tensor([  5.3182, -17.0247])\n",
      "\t Grad: tensor([-0.0084,  0.0477])\n",
      "Epoch 2433, Loss 2.934516191482544\n",
      "\t Params: tensor([  5.3183, -17.0252])\n",
      "\t Grad: tensor([-0.0084,  0.0476])\n",
      "Epoch 2434, Loss 2.934492588043213\n",
      "\t Params: tensor([  5.3184, -17.0257])\n",
      "\t Grad: tensor([-0.0084,  0.0475])\n",
      "Epoch 2435, Loss 2.9344687461853027\n",
      "\t Params: tensor([  5.3185, -17.0261])\n",
      "\t Grad: tensor([-0.0084,  0.0474])\n",
      "Epoch 2436, Loss 2.934446096420288\n",
      "\t Params: tensor([  5.3186, -17.0266])\n",
      "\t Grad: tensor([-0.0084,  0.0473])\n",
      "Epoch 2437, Loss 2.9344232082366943\n",
      "\t Params: tensor([  5.3187, -17.0271])\n",
      "\t Grad: tensor([-0.0083,  0.0473])\n",
      "Epoch 2438, Loss 2.9344000816345215\n",
      "\t Params: tensor([  5.3187, -17.0276])\n",
      "\t Grad: tensor([-0.0083,  0.0472])\n",
      "Epoch 2439, Loss 2.9343767166137695\n",
      "\t Params: tensor([  5.3188, -17.0280])\n",
      "\t Grad: tensor([-0.0083,  0.0471])\n",
      "Epoch 2440, Loss 2.934354543685913\n",
      "\t Params: tensor([  5.3189, -17.0285])\n",
      "\t Grad: tensor([-0.0083,  0.0470])\n",
      "Epoch 2441, Loss 2.9343314170837402\n",
      "\t Params: tensor([  5.3190, -17.0290])\n",
      "\t Grad: tensor([-0.0083,  0.0469])\n",
      "Epoch 2442, Loss 2.9343085289001465\n",
      "\t Params: tensor([  5.3191, -17.0294])\n",
      "\t Grad: tensor([-0.0083,  0.0469])\n",
      "Epoch 2443, Loss 2.9342870712280273\n",
      "\t Params: tensor([  5.3192, -17.0299])\n",
      "\t Grad: tensor([-0.0083,  0.0468])\n",
      "Epoch 2444, Loss 2.9342641830444336\n",
      "\t Params: tensor([  5.3192, -17.0304])\n",
      "\t Grad: tensor([-0.0083,  0.0467])\n",
      "Epoch 2445, Loss 2.9342422485351562\n",
      "\t Params: tensor([  5.3193, -17.0308])\n",
      "\t Grad: tensor([-0.0083,  0.0466])\n",
      "Epoch 2446, Loss 2.9342191219329834\n",
      "\t Params: tensor([  5.3194, -17.0313])\n",
      "\t Grad: tensor([-0.0082,  0.0465])\n",
      "Epoch 2447, Loss 2.9341979026794434\n",
      "\t Params: tensor([  5.3195, -17.0318])\n",
      "\t Grad: tensor([-0.0082,  0.0465])\n",
      "Epoch 2448, Loss 2.9341750144958496\n",
      "\t Params: tensor([  5.3196, -17.0322])\n",
      "\t Grad: tensor([-0.0082,  0.0464])\n",
      "Epoch 2449, Loss 2.9341514110565186\n",
      "\t Params: tensor([  5.3197, -17.0327])\n",
      "\t Grad: tensor([-0.0082,  0.0463])\n",
      "Epoch 2450, Loss 2.934129476547241\n",
      "\t Params: tensor([  5.3197, -17.0332])\n",
      "\t Grad: tensor([-0.0082,  0.0462])\n",
      "Epoch 2451, Loss 2.934108018875122\n",
      "\t Params: tensor([  5.3198, -17.0336])\n",
      "\t Grad: tensor([-0.0082,  0.0461])\n",
      "Epoch 2452, Loss 2.934084415435791\n",
      "\t Params: tensor([  5.3199, -17.0341])\n",
      "\t Grad: tensor([-0.0081,  0.0461])\n",
      "Epoch 2453, Loss 2.9340646266937256\n",
      "\t Params: tensor([  5.3200, -17.0345])\n",
      "\t Grad: tensor([-0.0081,  0.0460])\n",
      "Epoch 2454, Loss 2.9340431690216064\n",
      "\t Params: tensor([  5.3201, -17.0350])\n",
      "\t Grad: tensor([-0.0081,  0.0459])\n",
      "Epoch 2455, Loss 2.9340200424194336\n",
      "\t Params: tensor([  5.3201, -17.0355])\n",
      "\t Grad: tensor([-0.0081,  0.0458])\n",
      "Epoch 2456, Loss 2.93399977684021\n",
      "\t Params: tensor([  5.3202, -17.0359])\n",
      "\t Grad: tensor([-0.0081,  0.0457])\n",
      "Epoch 2457, Loss 2.933978319168091\n",
      "\t Params: tensor([  5.3203, -17.0364])\n",
      "\t Grad: tensor([-0.0081,  0.0457])\n",
      "Epoch 2458, Loss 2.9339563846588135\n",
      "\t Params: tensor([  5.3204, -17.0368])\n",
      "\t Grad: tensor([-0.0080,  0.0456])\n",
      "Epoch 2459, Loss 2.9339349269866943\n",
      "\t Params: tensor([  5.3205, -17.0373])\n",
      "\t Grad: tensor([-0.0080,  0.0455])\n",
      "Epoch 2460, Loss 2.9339139461517334\n",
      "\t Params: tensor([  5.3205, -17.0377])\n",
      "\t Grad: tensor([-0.0080,  0.0454])\n",
      "Epoch 2461, Loss 2.9338927268981934\n",
      "\t Params: tensor([  5.3206, -17.0382])\n",
      "\t Grad: tensor([-0.0080,  0.0454])\n",
      "Epoch 2462, Loss 2.933870553970337\n",
      "\t Params: tensor([  5.3207, -17.0386])\n",
      "\t Grad: tensor([-0.0080,  0.0453])\n",
      "Epoch 2463, Loss 2.9338486194610596\n",
      "\t Params: tensor([  5.3208, -17.0391])\n",
      "\t Grad: tensor([-0.0080,  0.0452])\n",
      "Epoch 2464, Loss 2.9338278770446777\n",
      "\t Params: tensor([  5.3209, -17.0396])\n",
      "\t Grad: tensor([-0.0080,  0.0451])\n",
      "Epoch 2465, Loss 2.9338066577911377\n",
      "\t Params: tensor([  5.3209, -17.0400])\n",
      "\t Grad: tensor([-0.0080,  0.0451])\n",
      "Epoch 2466, Loss 2.9337873458862305\n",
      "\t Params: tensor([  5.3210, -17.0405])\n",
      "\t Grad: tensor([-0.0079,  0.0450])\n",
      "Epoch 2467, Loss 2.9337668418884277\n",
      "\t Params: tensor([  5.3211, -17.0409])\n",
      "\t Grad: tensor([-0.0079,  0.0449])\n",
      "Epoch 2468, Loss 2.9337456226348877\n",
      "\t Params: tensor([  5.3212, -17.0413])\n",
      "\t Grad: tensor([-0.0079,  0.0448])\n",
      "Epoch 2469, Loss 2.933722734451294\n",
      "\t Params: tensor([  5.3213, -17.0418])\n",
      "\t Grad: tensor([-0.0079,  0.0448])\n",
      "Epoch 2470, Loss 2.933703660964966\n",
      "\t Params: tensor([  5.3213, -17.0422])\n",
      "\t Grad: tensor([-0.0079,  0.0447])\n",
      "Epoch 2471, Loss 2.933682441711426\n",
      "\t Params: tensor([  5.3214, -17.0427])\n",
      "\t Grad: tensor([-0.0079,  0.0446])\n",
      "Epoch 2472, Loss 2.9336624145507812\n",
      "\t Params: tensor([  5.3215, -17.0431])\n",
      "\t Grad: tensor([-0.0079,  0.0445])\n",
      "Epoch 2473, Loss 2.933642625808716\n",
      "\t Params: tensor([  5.3216, -17.0436])\n",
      "\t Grad: tensor([-0.0079,  0.0444])\n",
      "Epoch 2474, Loss 2.933622121810913\n",
      "\t Params: tensor([  5.3217, -17.0440])\n",
      "\t Grad: tensor([-0.0078,  0.0444])\n",
      "Epoch 2475, Loss 2.9336020946502686\n",
      "\t Params: tensor([  5.3217, -17.0445])\n",
      "\t Grad: tensor([-0.0078,  0.0443])\n",
      "Epoch 2476, Loss 2.9335830211639404\n",
      "\t Params: tensor([  5.3218, -17.0449])\n",
      "\t Grad: tensor([-0.0078,  0.0442])\n",
      "Epoch 2477, Loss 2.933560848236084\n",
      "\t Params: tensor([  5.3219, -17.0453])\n",
      "\t Grad: tensor([-0.0078,  0.0441])\n",
      "Epoch 2478, Loss 2.9335405826568604\n",
      "\t Params: tensor([  5.3220, -17.0458])\n",
      "\t Grad: tensor([-0.0078,  0.0441])\n",
      "Epoch 2479, Loss 2.933521270751953\n",
      "\t Params: tensor([  5.3220, -17.0462])\n",
      "\t Grad: tensor([-0.0078,  0.0440])\n",
      "Epoch 2480, Loss 2.9335012435913086\n",
      "\t Params: tensor([  5.3221, -17.0467])\n",
      "\t Grad: tensor([-0.0078,  0.0439])\n",
      "Epoch 2481, Loss 2.9334802627563477\n",
      "\t Params: tensor([  5.3222, -17.0471])\n",
      "\t Grad: tensor([-0.0077,  0.0438])\n",
      "Epoch 2482, Loss 2.9334633350372314\n",
      "\t Params: tensor([  5.3223, -17.0475])\n",
      "\t Grad: tensor([-0.0077,  0.0438])\n",
      "Epoch 2483, Loss 2.9334418773651123\n",
      "\t Params: tensor([  5.3224, -17.0480])\n",
      "\t Grad: tensor([-0.0077,  0.0437])\n",
      "Epoch 2484, Loss 2.933422327041626\n",
      "\t Params: tensor([  5.3224, -17.0484])\n",
      "\t Grad: tensor([-0.0077,  0.0436])\n",
      "Epoch 2485, Loss 2.933403253555298\n",
      "\t Params: tensor([  5.3225, -17.0489])\n",
      "\t Grad: tensor([-0.0077,  0.0436])\n",
      "Epoch 2486, Loss 2.9333817958831787\n",
      "\t Params: tensor([  5.3226, -17.0493])\n",
      "\t Grad: tensor([-0.0077,  0.0435])\n",
      "Epoch 2487, Loss 2.9333646297454834\n",
      "\t Params: tensor([  5.3227, -17.0497])\n",
      "\t Grad: tensor([-0.0077,  0.0434])\n",
      "Epoch 2488, Loss 2.933344841003418\n",
      "\t Params: tensor([  5.3227, -17.0502])\n",
      "\t Grad: tensor([-0.0077,  0.0433])\n",
      "Epoch 2489, Loss 2.9333252906799316\n",
      "\t Params: tensor([  5.3228, -17.0506])\n",
      "\t Grad: tensor([-0.0076,  0.0433])\n",
      "Epoch 2490, Loss 2.9333059787750244\n",
      "\t Params: tensor([  5.3229, -17.0510])\n",
      "\t Grad: tensor([-0.0076,  0.0432])\n",
      "Epoch 2491, Loss 2.9332869052886963\n",
      "\t Params: tensor([  5.3230, -17.0515])\n",
      "\t Grad: tensor([-0.0076,  0.0431])\n",
      "Epoch 2492, Loss 2.9332656860351562\n",
      "\t Params: tensor([  5.3230, -17.0519])\n",
      "\t Grad: tensor([-0.0076,  0.0430])\n",
      "Epoch 2493, Loss 2.93324875831604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params: tensor([  5.3231, -17.0523])\n",
      "\t Grad: tensor([-0.0076,  0.0430])\n",
      "Epoch 2494, Loss 2.9332289695739746\n",
      "\t Params: tensor([  5.3232, -17.0527])\n",
      "\t Grad: tensor([-0.0076,  0.0429])\n",
      "Epoch 2495, Loss 2.9332094192504883\n",
      "\t Params: tensor([  5.3233, -17.0532])\n",
      "\t Grad: tensor([-0.0076,  0.0428])\n",
      "Epoch 2496, Loss 2.93319034576416\n",
      "\t Params: tensor([  5.3233, -17.0536])\n",
      "\t Grad: tensor([-0.0075,  0.0427])\n",
      "Epoch 2497, Loss 2.9331719875335693\n",
      "\t Params: tensor([  5.3234, -17.0540])\n",
      "\t Grad: tensor([-0.0075,  0.0427])\n",
      "Epoch 2498, Loss 2.9331536293029785\n",
      "\t Params: tensor([  5.3235, -17.0544])\n",
      "\t Grad: tensor([-0.0075,  0.0426])\n",
      "Epoch 2499, Loss 2.933133840560913\n",
      "\t Params: tensor([  5.3236, -17.0549])\n",
      "\t Grad: tensor([-0.0075,  0.0425])\n",
      "Epoch 2500, Loss 2.9331159591674805\n",
      "\t Params: tensor([  5.3236, -17.0553])\n",
      "\t Grad: tensor([-0.0075,  0.0425])\n",
      "Epoch 2501, Loss 2.9330968856811523\n",
      "\t Params: tensor([  5.3237, -17.0557])\n",
      "\t Grad: tensor([-0.0075,  0.0424])\n",
      "Epoch 2502, Loss 2.9330787658691406\n",
      "\t Params: tensor([  5.3238, -17.0561])\n",
      "\t Grad: tensor([-0.0075,  0.0423])\n",
      "Epoch 2503, Loss 2.9330599308013916\n",
      "\t Params: tensor([  5.3239, -17.0566])\n",
      "\t Grad: tensor([-0.0075,  0.0422])\n",
      "Epoch 2504, Loss 2.9330427646636963\n",
      "\t Params: tensor([  5.3239, -17.0570])\n",
      "\t Grad: tensor([-0.0074,  0.0422])\n",
      "Epoch 2505, Loss 2.933025360107422\n",
      "\t Params: tensor([  5.3240, -17.0574])\n",
      "\t Grad: tensor([-0.0074,  0.0421])\n",
      "Epoch 2506, Loss 2.93300724029541\n",
      "\t Params: tensor([  5.3241, -17.0578])\n",
      "\t Grad: tensor([-0.0074,  0.0420])\n",
      "Epoch 2507, Loss 2.932988166809082\n",
      "\t Params: tensor([  5.3242, -17.0582])\n",
      "\t Grad: tensor([-0.0074,  0.0420])\n",
      "Epoch 2508, Loss 2.9329702854156494\n",
      "\t Params: tensor([  5.3242, -17.0587])\n",
      "\t Grad: tensor([-0.0074,  0.0419])\n",
      "Epoch 2509, Loss 2.932952880859375\n",
      "\t Params: tensor([  5.3243, -17.0591])\n",
      "\t Grad: tensor([-0.0074,  0.0418])\n",
      "Epoch 2510, Loss 2.932932138442993\n",
      "\t Params: tensor([  5.3244, -17.0595])\n",
      "\t Grad: tensor([-0.0074,  0.0417])\n",
      "Epoch 2511, Loss 2.932915449142456\n",
      "\t Params: tensor([  5.3245, -17.0599])\n",
      "\t Grad: tensor([-0.0073,  0.0417])\n",
      "Epoch 2512, Loss 2.9328980445861816\n",
      "\t Params: tensor([  5.3245, -17.0603])\n",
      "\t Grad: tensor([-0.0073,  0.0416])\n",
      "Epoch 2513, Loss 2.932880163192749\n",
      "\t Params: tensor([  5.3246, -17.0608])\n",
      "\t Grad: tensor([-0.0073,  0.0415])\n",
      "Epoch 2514, Loss 2.932861566543579\n",
      "\t Params: tensor([  5.3247, -17.0612])\n",
      "\t Grad: tensor([-0.0073,  0.0415])\n",
      "Epoch 2515, Loss 2.9328458309173584\n",
      "\t Params: tensor([  5.3248, -17.0616])\n",
      "\t Grad: tensor([-0.0073,  0.0414])\n",
      "Epoch 2516, Loss 2.932826280593872\n",
      "\t Params: tensor([  5.3248, -17.0620])\n",
      "\t Grad: tensor([-0.0073,  0.0413])\n",
      "Epoch 2517, Loss 2.932810068130493\n",
      "\t Params: tensor([  5.3249, -17.0624])\n",
      "\t Grad: tensor([-0.0073,  0.0412])\n",
      "Epoch 2518, Loss 2.9327902793884277\n",
      "\t Params: tensor([  5.3250, -17.0628])\n",
      "\t Grad: tensor([-0.0073,  0.0412])\n",
      "Epoch 2519, Loss 2.9327738285064697\n",
      "\t Params: tensor([  5.3250, -17.0632])\n",
      "\t Grad: tensor([-0.0073,  0.0411])\n",
      "Epoch 2520, Loss 2.932758092880249\n",
      "\t Params: tensor([  5.3251, -17.0636])\n",
      "\t Grad: tensor([-0.0073,  0.0410])\n",
      "Epoch 2521, Loss 2.9327392578125\n",
      "\t Params: tensor([  5.3252, -17.0640])\n",
      "\t Grad: tensor([-0.0073,  0.0410])\n",
      "Epoch 2522, Loss 2.932722568511963\n",
      "\t Params: tensor([  5.3253, -17.0645])\n",
      "\t Grad: tensor([-0.0072,  0.0409])\n",
      "Epoch 2523, Loss 2.9327056407928467\n",
      "\t Params: tensor([  5.3253, -17.0649])\n",
      "\t Grad: tensor([-0.0072,  0.0408])\n",
      "Epoch 2524, Loss 2.9326887130737305\n",
      "\t Params: tensor([  5.3254, -17.0653])\n",
      "\t Grad: tensor([-0.0072,  0.0408])\n",
      "Epoch 2525, Loss 2.932671308517456\n",
      "\t Params: tensor([  5.3255, -17.0657])\n",
      "\t Grad: tensor([-0.0072,  0.0407])\n",
      "Epoch 2526, Loss 2.9326536655426025\n",
      "\t Params: tensor([  5.3256, -17.0661])\n",
      "\t Grad: tensor([-0.0072,  0.0406])\n",
      "Epoch 2527, Loss 2.9326372146606445\n",
      "\t Params: tensor([  5.3256, -17.0665])\n",
      "\t Grad: tensor([-0.0072,  0.0405])\n",
      "Epoch 2528, Loss 2.9326186180114746\n",
      "\t Params: tensor([  5.3257, -17.0669])\n",
      "\t Grad: tensor([-0.0072,  0.0405])\n",
      "Epoch 2529, Loss 2.932602882385254\n",
      "\t Params: tensor([  5.3258, -17.0673])\n",
      "\t Grad: tensor([-0.0071,  0.0404])\n",
      "Epoch 2530, Loss 2.9325852394104004\n",
      "\t Params: tensor([  5.3258, -17.0677])\n",
      "\t Grad: tensor([-0.0071,  0.0403])\n",
      "Epoch 2531, Loss 2.9325685501098633\n",
      "\t Params: tensor([  5.3259, -17.0681])\n",
      "\t Grad: tensor([-0.0071,  0.0403])\n",
      "Epoch 2532, Loss 2.932553291320801\n",
      "\t Params: tensor([  5.3260, -17.0685])\n",
      "\t Grad: tensor([-0.0071,  0.0402])\n",
      "Epoch 2533, Loss 2.93253493309021\n",
      "\t Params: tensor([  5.3261, -17.0689])\n",
      "\t Grad: tensor([-0.0071,  0.0401])\n",
      "Epoch 2534, Loss 2.9325196743011475\n",
      "\t Params: tensor([  5.3261, -17.0693])\n",
      "\t Grad: tensor([-0.0071,  0.0401])\n",
      "Epoch 2535, Loss 2.932502031326294\n",
      "\t Params: tensor([  5.3262, -17.0697])\n",
      "\t Grad: tensor([-0.0071,  0.0400])\n",
      "Epoch 2536, Loss 2.9324867725372314\n",
      "\t Params: tensor([  5.3263, -17.0701])\n",
      "\t Grad: tensor([-0.0071,  0.0399])\n",
      "Epoch 2537, Loss 2.9324686527252197\n",
      "\t Params: tensor([  5.3263, -17.0705])\n",
      "\t Grad: tensor([-0.0070,  0.0399])\n",
      "Epoch 2538, Loss 2.93245530128479\n",
      "\t Params: tensor([  5.3264, -17.0709])\n",
      "\t Grad: tensor([-0.0070,  0.0398])\n",
      "Epoch 2539, Loss 2.9324381351470947\n",
      "\t Params: tensor([  5.3265, -17.0713])\n",
      "\t Grad: tensor([-0.0070,  0.0397])\n",
      "Epoch 2540, Loss 2.9324212074279785\n",
      "\t Params: tensor([  5.3265, -17.0717])\n",
      "\t Grad: tensor([-0.0070,  0.0397])\n",
      "Epoch 2541, Loss 2.932403802871704\n",
      "\t Params: tensor([  5.3266, -17.0721])\n",
      "\t Grad: tensor([-0.0070,  0.0396])\n",
      "Epoch 2542, Loss 2.932386875152588\n",
      "\t Params: tensor([  5.3267, -17.0725])\n",
      "\t Grad: tensor([-0.0070,  0.0395])\n",
      "Epoch 2543, Loss 2.932370662689209\n",
      "\t Params: tensor([  5.3268, -17.0729])\n",
      "\t Grad: tensor([-0.0070,  0.0395])\n",
      "Epoch 2544, Loss 2.9323575496673584\n",
      "\t Params: tensor([  5.3268, -17.0733])\n",
      "\t Grad: tensor([-0.0070,  0.0394])\n",
      "Epoch 2545, Loss 2.932340383529663\n",
      "\t Params: tensor([  5.3269, -17.0737])\n",
      "\t Grad: tensor([-0.0069,  0.0393])\n",
      "Epoch 2546, Loss 2.9323244094848633\n",
      "\t Params: tensor([  5.3270, -17.0741])\n",
      "\t Grad: tensor([-0.0069,  0.0393])\n",
      "Epoch 2547, Loss 2.932309627532959\n",
      "\t Params: tensor([  5.3270, -17.0745])\n",
      "\t Grad: tensor([-0.0069,  0.0392])\n",
      "Epoch 2548, Loss 2.9322926998138428\n",
      "\t Params: tensor([  5.3271, -17.0749])\n",
      "\t Grad: tensor([-0.0069,  0.0391])\n",
      "Epoch 2549, Loss 2.9322774410247803\n",
      "\t Params: tensor([  5.3272, -17.0752])\n",
      "\t Grad: tensor([-0.0069,  0.0391])\n",
      "Epoch 2550, Loss 2.9322614669799805\n",
      "\t Params: tensor([  5.3272, -17.0756])\n",
      "\t Grad: tensor([-0.0069,  0.0390])\n",
      "Epoch 2551, Loss 2.9322457313537598\n",
      "\t Params: tensor([  5.3273, -17.0760])\n",
      "\t Grad: tensor([-0.0069,  0.0389])\n",
      "Epoch 2552, Loss 2.9322290420532227\n",
      "\t Params: tensor([  5.3274, -17.0764])\n",
      "\t Grad: tensor([-0.0069,  0.0389])\n",
      "Epoch 2553, Loss 2.9322149753570557\n",
      "\t Params: tensor([  5.3274, -17.0768])\n",
      "\t Grad: tensor([-0.0069,  0.0388])\n",
      "Epoch 2554, Loss 2.9321978092193604\n",
      "\t Params: tensor([  5.3275, -17.0772])\n",
      "\t Grad: tensor([-0.0068,  0.0387])\n",
      "Epoch 2555, Loss 2.932183265686035\n",
      "\t Params: tensor([  5.3276, -17.0776])\n",
      "\t Grad: tensor([-0.0068,  0.0387])\n",
      "Epoch 2556, Loss 2.9321672916412354\n",
      "\t Params: tensor([  5.3276, -17.0780])\n",
      "\t Grad: tensor([-0.0068,  0.0386])\n",
      "Epoch 2557, Loss 2.9321532249450684\n",
      "\t Params: tensor([  5.3277, -17.0783])\n",
      "\t Grad: tensor([-0.0068,  0.0385])\n",
      "Epoch 2558, Loss 2.9321374893188477\n",
      "\t Params: tensor([  5.3278, -17.0787])\n",
      "\t Grad: tensor([-0.0068,  0.0385])\n",
      "Epoch 2559, Loss 2.932121515274048\n",
      "\t Params: tensor([  5.3279, -17.0791])\n",
      "\t Grad: tensor([-0.0068,  0.0384])\n",
      "Epoch 2560, Loss 2.932107448577881\n",
      "\t Params: tensor([  5.3279, -17.0795])\n",
      "\t Grad: tensor([-0.0068,  0.0383])\n",
      "Epoch 2561, Loss 2.9320924282073975\n",
      "\t Params: tensor([  5.3280, -17.0799])\n",
      "\t Grad: tensor([-0.0068,  0.0383])\n",
      "Epoch 2562, Loss 2.9320764541625977\n",
      "\t Params: tensor([  5.3281, -17.0803])\n",
      "\t Grad: tensor([-0.0067,  0.0382])\n",
      "Epoch 2563, Loss 2.932061195373535\n",
      "\t Params: tensor([  5.3281, -17.0806])\n",
      "\t Grad: tensor([-0.0067,  0.0381])\n",
      "Epoch 2564, Loss 2.9320473670959473\n",
      "\t Params: tensor([  5.3282, -17.0810])\n",
      "\t Grad: tensor([-0.0067,  0.0381])\n",
      "Epoch 2565, Loss 2.9320311546325684\n",
      "\t Params: tensor([  5.3283, -17.0814])\n",
      "\t Grad: tensor([-0.0067,  0.0380])\n",
      "Epoch 2566, Loss 2.9320173263549805\n",
      "\t Params: tensor([  5.3283, -17.0818])\n",
      "\t Grad: tensor([-0.0067,  0.0379])\n",
      "Epoch 2567, Loss 2.932002305984497\n",
      "\t Params: tensor([  5.3284, -17.0822])\n",
      "\t Grad: tensor([-0.0067,  0.0379])\n",
      "Epoch 2568, Loss 2.931986093521118\n",
      "\t Params: tensor([  5.3285, -17.0825])\n",
      "\t Grad: tensor([-0.0067,  0.0378])\n",
      "Epoch 2569, Loss 2.9319722652435303\n",
      "\t Params: tensor([  5.3285, -17.0829])\n",
      "\t Grad: tensor([-0.0067,  0.0378])\n",
      "Epoch 2570, Loss 2.931957483291626\n",
      "\t Params: tensor([  5.3286, -17.0833])\n",
      "\t Grad: tensor([-0.0067,  0.0377])\n",
      "Epoch 2571, Loss 2.931941270828247\n",
      "\t Params: tensor([  5.3287, -17.0837])\n",
      "\t Grad: tensor([-0.0067,  0.0376])\n",
      "Epoch 2572, Loss 2.931929111480713\n",
      "\t Params: tensor([  5.3287, -17.0840])\n",
      "\t Grad: tensor([-0.0066,  0.0376])\n",
      "Epoch 2573, Loss 2.9319143295288086\n",
      "\t Params: tensor([  5.3288, -17.0844])\n",
      "\t Grad: tensor([-0.0066,  0.0375])\n",
      "Epoch 2574, Loss 2.9318997859954834\n",
      "\t Params: tensor([  5.3289, -17.0848])\n",
      "\t Grad: tensor([-0.0066,  0.0374])\n",
      "Epoch 2575, Loss 2.931885004043579\n",
      "\t Params: tensor([  5.3289, -17.0852])\n",
      "\t Grad: tensor([-0.0066,  0.0374])\n",
      "Epoch 2576, Loss 2.931870222091675\n",
      "\t Params: tensor([  5.3290, -17.0855])\n",
      "\t Grad: tensor([-0.0066,  0.0373])\n",
      "Epoch 2577, Loss 2.9318552017211914\n",
      "\t Params: tensor([  5.3291, -17.0859])\n",
      "\t Grad: tensor([-0.0066,  0.0372])\n",
      "Epoch 2578, Loss 2.931842088699341\n",
      "\t Params: tensor([  5.3291, -17.0863])\n",
      "\t Grad: tensor([-0.0066,  0.0372])\n",
      "Epoch 2579, Loss 2.9318277835845947\n",
      "\t Params: tensor([  5.3292, -17.0867])\n",
      "\t Grad: tensor([-0.0066,  0.0371])\n",
      "Epoch 2580, Loss 2.9318132400512695\n",
      "\t Params: tensor([  5.3293, -17.0870])\n",
      "\t Grad: tensor([-0.0065,  0.0371])\n",
      "Epoch 2581, Loss 2.9317986965179443\n",
      "\t Params: tensor([  5.3293, -17.0874])\n",
      "\t Grad: tensor([-0.0065,  0.0370])\n",
      "Epoch 2582, Loss 2.931785821914673\n",
      "\t Params: tensor([  5.3294, -17.0878])\n",
      "\t Grad: tensor([-0.0065,  0.0369])\n",
      "Epoch 2583, Loss 2.9317710399627686\n",
      "\t Params: tensor([  5.3294, -17.0881])\n",
      "\t Grad: tensor([-0.0065,  0.0369])\n",
      "Epoch 2584, Loss 2.9317588806152344\n",
      "\t Params: tensor([  5.3295, -17.0885])\n",
      "\t Grad: tensor([-0.0065,  0.0368])\n",
      "Epoch 2585, Loss 2.9317421913146973\n",
      "\t Params: tensor([  5.3296, -17.0889])\n",
      "\t Grad: tensor([-0.0065,  0.0367])\n",
      "Epoch 2586, Loss 2.931729316711426\n",
      "\t Params: tensor([  5.3296, -17.0892])\n",
      "\t Grad: tensor([-0.0065,  0.0367])\n",
      "Epoch 2587, Loss 2.9317169189453125\n",
      "\t Params: tensor([  5.3297, -17.0896])\n",
      "\t Grad: tensor([-0.0065,  0.0366])\n",
      "Epoch 2588, Loss 2.9317009449005127\n",
      "\t Params: tensor([  5.3298, -17.0900])\n",
      "\t Grad: tensor([-0.0065,  0.0366])\n",
      "Epoch 2589, Loss 2.9316866397857666\n",
      "\t Params: tensor([  5.3298, -17.0903])\n",
      "\t Grad: tensor([-0.0065,  0.0365])\n",
      "Epoch 2590, Loss 2.931673526763916\n",
      "\t Params: tensor([  5.3299, -17.0907])\n",
      "\t Grad: tensor([-0.0064,  0.0364])\n",
      "Epoch 2591, Loss 2.9316599369049072\n",
      "\t Params: tensor([  5.3300, -17.0911])\n",
      "\t Grad: tensor([-0.0064,  0.0364])\n",
      "Epoch 2592, Loss 2.931647539138794\n",
      "\t Params: tensor([  5.3300, -17.0914])\n",
      "\t Grad: tensor([-0.0064,  0.0363])\n",
      "Epoch 2593, Loss 2.9316320419311523\n",
      "\t Params: tensor([  5.3301, -17.0918])\n",
      "\t Grad: tensor([-0.0064,  0.0362])\n",
      "Epoch 2594, Loss 2.9316186904907227\n",
      "\t Params: tensor([  5.3302, -17.0921])\n",
      "\t Grad: tensor([-0.0064,  0.0362])\n",
      "Epoch 2595, Loss 2.9316062927246094\n",
      "\t Params: tensor([  5.3302, -17.0925])\n",
      "\t Grad: tensor([-0.0064,  0.0361])\n",
      "Epoch 2596, Loss 2.931593418121338\n",
      "\t Params: tensor([  5.3303, -17.0929])\n",
      "\t Grad: tensor([-0.0064,  0.0361])\n",
      "Epoch 2597, Loss 2.931579828262329\n",
      "\t Params: tensor([  5.3303, -17.0932])\n",
      "\t Grad: tensor([-0.0064,  0.0360])\n",
      "Epoch 2598, Loss 2.9315664768218994\n",
      "\t Params: tensor([  5.3304, -17.0936])\n",
      "\t Grad: tensor([-0.0064,  0.0359])\n",
      "Epoch 2599, Loss 2.931553840637207\n",
      "\t Params: tensor([  5.3305, -17.0939])\n",
      "\t Grad: tensor([-0.0064,  0.0359])\n",
      "Epoch 2600, Loss 2.9315383434295654\n",
      "\t Params: tensor([  5.3305, -17.0943])\n",
      "\t Grad: tensor([-0.0063,  0.0358])\n",
      "Epoch 2601, Loss 2.9315261840820312\n",
      "\t Params: tensor([  5.3306, -17.0947])\n",
      "\t Grad: tensor([-0.0063,  0.0358])\n",
      "Epoch 2602, Loss 2.9315123558044434\n",
      "\t Params: tensor([  5.3307, -17.0950])\n",
      "\t Grad: tensor([-0.0063,  0.0357])\n",
      "Epoch 2603, Loss 2.931499481201172\n",
      "\t Params: tensor([  5.3307, -17.0954])\n",
      "\t Grad: tensor([-0.0063,  0.0356])\n",
      "Epoch 2604, Loss 2.931488275527954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params: tensor([  5.3308, -17.0957])\n",
      "\t Grad: tensor([-0.0063,  0.0356])\n",
      "Epoch 2605, Loss 2.931473731994629\n",
      "\t Params: tensor([  5.3309, -17.0961])\n",
      "\t Grad: tensor([-0.0063,  0.0355])\n",
      "Epoch 2606, Loss 2.931462049484253\n",
      "\t Params: tensor([  5.3309, -17.0964])\n",
      "\t Grad: tensor([-0.0062,  0.0355])\n",
      "Epoch 2607, Loss 2.931448459625244\n",
      "\t Params: tensor([  5.3310, -17.0968])\n",
      "\t Grad: tensor([-0.0062,  0.0354])\n",
      "Epoch 2608, Loss 2.9314355850219727\n",
      "\t Params: tensor([  5.3310, -17.0971])\n",
      "\t Grad: tensor([-0.0062,  0.0353])\n",
      "Epoch 2609, Loss 2.9314229488372803\n",
      "\t Params: tensor([  5.3311, -17.0975])\n",
      "\t Grad: tensor([-0.0062,  0.0353])\n",
      "Epoch 2610, Loss 2.931410789489746\n",
      "\t Params: tensor([  5.3312, -17.0979])\n",
      "\t Grad: tensor([-0.0062,  0.0352])\n",
      "Epoch 2611, Loss 2.9313974380493164\n",
      "\t Params: tensor([  5.3312, -17.0982])\n",
      "\t Grad: tensor([-0.0062,  0.0352])\n",
      "Epoch 2612, Loss 2.9313840866088867\n",
      "\t Params: tensor([  5.3313, -17.0986])\n",
      "\t Grad: tensor([-0.0062,  0.0351])\n",
      "Epoch 2613, Loss 2.931370735168457\n",
      "\t Params: tensor([  5.3313, -17.0989])\n",
      "\t Grad: tensor([-0.0062,  0.0350])\n",
      "Epoch 2614, Loss 2.9313583374023438\n",
      "\t Params: tensor([  5.3314, -17.0993])\n",
      "\t Grad: tensor([-0.0062,  0.0350])\n",
      "Epoch 2615, Loss 2.9313459396362305\n",
      "\t Params: tensor([  5.3315, -17.0996])\n",
      "\t Grad: tensor([-0.0062,  0.0349])\n",
      "Epoch 2616, Loss 2.9313347339630127\n",
      "\t Params: tensor([  5.3315, -17.1000])\n",
      "\t Grad: tensor([-0.0062,  0.0349])\n",
      "Epoch 2617, Loss 2.931321620941162\n",
      "\t Params: tensor([  5.3316, -17.1003])\n",
      "\t Grad: tensor([-0.0062,  0.0348])\n",
      "Epoch 2618, Loss 2.9313080310821533\n",
      "\t Params: tensor([  5.3317, -17.1006])\n",
      "\t Grad: tensor([-0.0061,  0.0347])\n",
      "Epoch 2619, Loss 2.931295871734619\n",
      "\t Params: tensor([  5.3317, -17.1010])\n",
      "\t Grad: tensor([-0.0061,  0.0347])\n",
      "Epoch 2620, Loss 2.9312820434570312\n",
      "\t Params: tensor([  5.3318, -17.1013])\n",
      "\t Grad: tensor([-0.0061,  0.0346])\n",
      "Epoch 2621, Loss 2.931271553039551\n",
      "\t Params: tensor([  5.3318, -17.1017])\n",
      "\t Grad: tensor([-0.0061,  0.0346])\n",
      "Epoch 2622, Loss 2.9312584400177\n",
      "\t Params: tensor([  5.3319, -17.1020])\n",
      "\t Grad: tensor([-0.0061,  0.0345])\n",
      "Epoch 2623, Loss 2.9312446117401123\n",
      "\t Params: tensor([  5.3320, -17.1024])\n",
      "\t Grad: tensor([-0.0061,  0.0344])\n",
      "Epoch 2624, Loss 2.931234121322632\n",
      "\t Params: tensor([  5.3320, -17.1027])\n",
      "\t Grad: tensor([-0.0061,  0.0344])\n",
      "Epoch 2625, Loss 2.931222438812256\n",
      "\t Params: tensor([  5.3321, -17.1031])\n",
      "\t Grad: tensor([-0.0061,  0.0343])\n",
      "Epoch 2626, Loss 2.931211233139038\n",
      "\t Params: tensor([  5.3321, -17.1034])\n",
      "\t Grad: tensor([-0.0060,  0.0343])\n",
      "Epoch 2627, Loss 2.9311959743499756\n",
      "\t Params: tensor([  5.3322, -17.1038])\n",
      "\t Grad: tensor([-0.0060,  0.0342])\n",
      "Epoch 2628, Loss 2.931185007095337\n",
      "\t Params: tensor([  5.3323, -17.1041])\n",
      "\t Grad: tensor([-0.0060,  0.0342])\n",
      "Epoch 2629, Loss 2.931173086166382\n",
      "\t Params: tensor([  5.3323, -17.1044])\n",
      "\t Grad: tensor([-0.0060,  0.0341])\n",
      "Epoch 2630, Loss 2.931162118911743\n",
      "\t Params: tensor([  5.3324, -17.1048])\n",
      "\t Grad: tensor([-0.0060,  0.0340])\n",
      "Epoch 2631, Loss 2.9311487674713135\n",
      "\t Params: tensor([  5.3324, -17.1051])\n",
      "\t Grad: tensor([-0.0060,  0.0340])\n",
      "Epoch 2632, Loss 2.931138038635254\n",
      "\t Params: tensor([  5.3325, -17.1055])\n",
      "\t Grad: tensor([-0.0060,  0.0339])\n",
      "Epoch 2633, Loss 2.931126356124878\n",
      "\t Params: tensor([  5.3326, -17.1058])\n",
      "\t Grad: tensor([-0.0060,  0.0339])\n",
      "Epoch 2634, Loss 2.9311141967773438\n",
      "\t Params: tensor([  5.3326, -17.1061])\n",
      "\t Grad: tensor([-0.0060,  0.0338])\n",
      "Epoch 2635, Loss 2.9311013221740723\n",
      "\t Params: tensor([  5.3327, -17.1065])\n",
      "\t Grad: tensor([-0.0060,  0.0337])\n",
      "Epoch 2636, Loss 2.9310896396636963\n",
      "\t Params: tensor([  5.3327, -17.1068])\n",
      "\t Grad: tensor([-0.0059,  0.0337])\n",
      "Epoch 2637, Loss 2.9310789108276367\n",
      "\t Params: tensor([  5.3328, -17.1071])\n",
      "\t Grad: tensor([-0.0059,  0.0336])\n",
      "Epoch 2638, Loss 2.9310669898986816\n",
      "\t Params: tensor([  5.3329, -17.1075])\n",
      "\t Grad: tensor([-0.0059,  0.0336])\n",
      "Epoch 2639, Loss 2.93105411529541\n",
      "\t Params: tensor([  5.3329, -17.1078])\n",
      "\t Grad: tensor([-0.0059,  0.0335])\n",
      "Epoch 2640, Loss 2.931044101715088\n",
      "\t Params: tensor([  5.3330, -17.1081])\n",
      "\t Grad: tensor([-0.0059,  0.0335])\n",
      "Epoch 2641, Loss 2.9310340881347656\n",
      "\t Params: tensor([  5.3330, -17.1085])\n",
      "\t Grad: tensor([-0.0059,  0.0334])\n",
      "Epoch 2642, Loss 2.931021213531494\n",
      "\t Params: tensor([  5.3331, -17.1088])\n",
      "\t Grad: tensor([-0.0059,  0.0333])\n",
      "Epoch 2643, Loss 2.9310102462768555\n",
      "\t Params: tensor([  5.3332, -17.1091])\n",
      "\t Grad: tensor([-0.0059,  0.0333])\n",
      "Epoch 2644, Loss 2.9309990406036377\n",
      "\t Params: tensor([  5.3332, -17.1095])\n",
      "\t Grad: tensor([-0.0059,  0.0332])\n",
      "Epoch 2645, Loss 2.9309868812561035\n",
      "\t Params: tensor([  5.3333, -17.1098])\n",
      "\t Grad: tensor([-0.0059,  0.0332])\n",
      "Epoch 2646, Loss 2.930976152420044\n",
      "\t Params: tensor([  5.3333, -17.1101])\n",
      "\t Grad: tensor([-0.0059,  0.0331])\n",
      "Epoch 2647, Loss 2.930964469909668\n",
      "\t Params: tensor([  5.3334, -17.1105])\n",
      "\t Grad: tensor([-0.0059,  0.0331])\n",
      "Epoch 2648, Loss 2.93095326423645\n",
      "\t Params: tensor([  5.3335, -17.1108])\n",
      "\t Grad: tensor([-0.0058,  0.0330])\n",
      "Epoch 2649, Loss 2.930940866470337\n",
      "\t Params: tensor([  5.3335, -17.1111])\n",
      "\t Grad: tensor([-0.0058,  0.0330])\n",
      "Epoch 2650, Loss 2.930931806564331\n",
      "\t Params: tensor([  5.3336, -17.1115])\n",
      "\t Grad: tensor([-0.0058,  0.0329])\n",
      "Epoch 2651, Loss 2.9309206008911133\n",
      "\t Params: tensor([  5.3336, -17.1118])\n",
      "\t Grad: tensor([-0.0058,  0.0328])\n",
      "Epoch 2652, Loss 2.930907964706421\n",
      "\t Params: tensor([  5.3337, -17.1121])\n",
      "\t Grad: tensor([-0.0058,  0.0328])\n",
      "Epoch 2653, Loss 2.930899143218994\n",
      "\t Params: tensor([  5.3337, -17.1124])\n",
      "\t Grad: tensor([-0.0058,  0.0327])\n",
      "Epoch 2654, Loss 2.930885076522827\n",
      "\t Params: tensor([  5.3338, -17.1128])\n",
      "\t Grad: tensor([-0.0058,  0.0327])\n",
      "Epoch 2655, Loss 2.9308760166168213\n",
      "\t Params: tensor([  5.3339, -17.1131])\n",
      "\t Grad: tensor([-0.0058,  0.0326])\n",
      "Epoch 2656, Loss 2.930863380432129\n",
      "\t Params: tensor([  5.3339, -17.1134])\n",
      "\t Grad: tensor([-0.0057,  0.0326])\n",
      "Epoch 2657, Loss 2.930853843688965\n",
      "\t Params: tensor([  5.3340, -17.1137])\n",
      "\t Grad: tensor([-0.0057,  0.0325])\n",
      "Epoch 2658, Loss 2.9308412075042725\n",
      "\t Params: tensor([  5.3340, -17.1141])\n",
      "\t Grad: tensor([-0.0057,  0.0325])\n",
      "Epoch 2659, Loss 2.930832624435425\n",
      "\t Params: tensor([  5.3341, -17.1144])\n",
      "\t Grad: tensor([-0.0057,  0.0324])\n",
      "Epoch 2660, Loss 2.930821418762207\n",
      "\t Params: tensor([  5.3341, -17.1147])\n",
      "\t Grad: tensor([-0.0057,  0.0323])\n",
      "Epoch 2661, Loss 2.9308106899261475\n",
      "\t Params: tensor([  5.3342, -17.1150])\n",
      "\t Grad: tensor([-0.0057,  0.0323])\n",
      "Epoch 2662, Loss 2.9308013916015625\n",
      "\t Params: tensor([  5.3343, -17.1154])\n",
      "\t Grad: tensor([-0.0057,  0.0322])\n",
      "Epoch 2663, Loss 2.930788278579712\n",
      "\t Params: tensor([  5.3343, -17.1157])\n",
      "\t Grad: tensor([-0.0057,  0.0322])\n",
      "Epoch 2664, Loss 2.9307777881622314\n",
      "\t Params: tensor([  5.3344, -17.1160])\n",
      "\t Grad: tensor([-0.0057,  0.0321])\n",
      "Epoch 2665, Loss 2.9307668209075928\n",
      "\t Params: tensor([  5.3344, -17.1163])\n",
      "\t Grad: tensor([-0.0057,  0.0321])\n",
      "Epoch 2666, Loss 2.9307572841644287\n",
      "\t Params: tensor([  5.3345, -17.1166])\n",
      "\t Grad: tensor([-0.0057,  0.0320])\n",
      "Epoch 2667, Loss 2.930745840072632\n",
      "\t Params: tensor([  5.3345, -17.1170])\n",
      "\t Grad: tensor([-0.0056,  0.0320])\n",
      "Epoch 2668, Loss 2.9307355880737305\n",
      "\t Params: tensor([  5.3346, -17.1173])\n",
      "\t Grad: tensor([-0.0056,  0.0319])\n",
      "Epoch 2669, Loss 2.9307243824005127\n",
      "\t Params: tensor([  5.3347, -17.1176])\n",
      "\t Grad: tensor([-0.0056,  0.0319])\n",
      "Epoch 2670, Loss 2.9307150840759277\n",
      "\t Params: tensor([  5.3347, -17.1179])\n",
      "\t Grad: tensor([-0.0056,  0.0318])\n",
      "Epoch 2671, Loss 2.930703639984131\n",
      "\t Params: tensor([  5.3348, -17.1182])\n",
      "\t Grad: tensor([-0.0056,  0.0317])\n",
      "Epoch 2672, Loss 2.9306938648223877\n",
      "\t Params: tensor([  5.3348, -17.1186])\n",
      "\t Grad: tensor([-0.0056,  0.0317])\n",
      "Epoch 2673, Loss 2.93068528175354\n",
      "\t Params: tensor([  5.3349, -17.1189])\n",
      "\t Grad: tensor([-0.0056,  0.0316])\n",
      "Epoch 2674, Loss 2.930673837661743\n",
      "\t Params: tensor([  5.3349, -17.1192])\n",
      "\t Grad: tensor([-0.0056,  0.0316])\n",
      "Epoch 2675, Loss 2.9306631088256836\n",
      "\t Params: tensor([  5.3350, -17.1195])\n",
      "\t Grad: tensor([-0.0056,  0.0315])\n",
      "Epoch 2676, Loss 2.930654287338257\n",
      "\t Params: tensor([  5.3350, -17.1198])\n",
      "\t Grad: tensor([-0.0056,  0.0315])\n",
      "Epoch 2677, Loss 2.9306440353393555\n",
      "\t Params: tensor([  5.3351, -17.1201])\n",
      "\t Grad: tensor([-0.0055,  0.0314])\n",
      "Epoch 2678, Loss 2.930631160736084\n",
      "\t Params: tensor([  5.3352, -17.1204])\n",
      "\t Grad: tensor([-0.0055,  0.0314])\n",
      "Epoch 2679, Loss 2.930621385574341\n",
      "\t Params: tensor([  5.3352, -17.1208])\n",
      "\t Grad: tensor([-0.0055,  0.0313])\n",
      "Epoch 2680, Loss 2.9306130409240723\n",
      "\t Params: tensor([  5.3353, -17.1211])\n",
      "\t Grad: tensor([-0.0055,  0.0313])\n",
      "Epoch 2681, Loss 2.930602788925171\n",
      "\t Params: tensor([  5.3353, -17.1214])\n",
      "\t Grad: tensor([-0.0055,  0.0312])\n",
      "Epoch 2682, Loss 2.9305925369262695\n",
      "\t Params: tensor([  5.3354, -17.1217])\n",
      "\t Grad: tensor([-0.0055,  0.0312])\n",
      "Epoch 2683, Loss 2.930582284927368\n",
      "\t Params: tensor([  5.3354, -17.1220])\n",
      "\t Grad: tensor([-0.0055,  0.0311])\n",
      "Epoch 2684, Loss 2.9305710792541504\n",
      "\t Params: tensor([  5.3355, -17.1223])\n",
      "\t Grad: tensor([-0.0055,  0.0310])\n",
      "Epoch 2685, Loss 2.9305620193481445\n",
      "\t Params: tensor([  5.3355, -17.1226])\n",
      "\t Grad: tensor([-0.0055,  0.0310])\n",
      "Epoch 2686, Loss 2.9305520057678223\n",
      "\t Params: tensor([  5.3356, -17.1229])\n",
      "\t Grad: tensor([-0.0055,  0.0309])\n",
      "Epoch 2687, Loss 2.9305427074432373\n",
      "\t Params: tensor([  5.3356, -17.1232])\n",
      "\t Grad: tensor([-0.0055,  0.0309])\n",
      "Epoch 2688, Loss 2.9305336475372314\n",
      "\t Params: tensor([  5.3357, -17.1236])\n",
      "\t Grad: tensor([-0.0055,  0.0308])\n",
      "Epoch 2689, Loss 2.930523157119751\n",
      "\t Params: tensor([  5.3358, -17.1239])\n",
      "\t Grad: tensor([-0.0054,  0.0308])\n",
      "Epoch 2690, Loss 2.930513620376587\n",
      "\t Params: tensor([  5.3358, -17.1242])\n",
      "\t Grad: tensor([-0.0054,  0.0307])\n",
      "Epoch 2691, Loss 2.930502414703369\n",
      "\t Params: tensor([  5.3359, -17.1245])\n",
      "\t Grad: tensor([-0.0054,  0.0307])\n",
      "Epoch 2692, Loss 2.9304933547973633\n",
      "\t Params: tensor([  5.3359, -17.1248])\n",
      "\t Grad: tensor([-0.0054,  0.0306])\n",
      "Epoch 2693, Loss 2.9304823875427246\n",
      "\t Params: tensor([  5.3360, -17.1251])\n",
      "\t Grad: tensor([-0.0054,  0.0306])\n",
      "Epoch 2694, Loss 2.930474281311035\n",
      "\t Params: tensor([  5.3360, -17.1254])\n",
      "\t Grad: tensor([-0.0054,  0.0305])\n",
      "Epoch 2695, Loss 2.930464267730713\n",
      "\t Params: tensor([  5.3361, -17.1257])\n",
      "\t Grad: tensor([-0.0054,  0.0305])\n",
      "Epoch 2696, Loss 2.9304540157318115\n",
      "\t Params: tensor([  5.3361, -17.1260])\n",
      "\t Grad: tensor([-0.0054,  0.0304])\n",
      "Epoch 2697, Loss 2.9304451942443848\n",
      "\t Params: tensor([  5.3362, -17.1263])\n",
      "\t Grad: tensor([-0.0054,  0.0304])\n",
      "Epoch 2698, Loss 2.930436134338379\n",
      "\t Params: tensor([  5.3362, -17.1266])\n",
      "\t Grad: tensor([-0.0054,  0.0303])\n",
      "Epoch 2699, Loss 2.9304261207580566\n",
      "\t Params: tensor([  5.3363, -17.1269])\n",
      "\t Grad: tensor([-0.0054,  0.0303])\n",
      "Epoch 2700, Loss 2.9304163455963135\n",
      "\t Params: tensor([  5.3364, -17.1272])\n",
      "\t Grad: tensor([-0.0054,  0.0302])\n",
      "Epoch 2701, Loss 2.930407762527466\n",
      "\t Params: tensor([  5.3364, -17.1275])\n",
      "\t Grad: tensor([-0.0053,  0.0302])\n",
      "Epoch 2702, Loss 2.9303979873657227\n",
      "\t Params: tensor([  5.3365, -17.1278])\n",
      "\t Grad: tensor([-0.0053,  0.0301])\n",
      "Epoch 2703, Loss 2.9303877353668213\n",
      "\t Params: tensor([  5.3365, -17.1281])\n",
      "\t Grad: tensor([-0.0053,  0.0301])\n",
      "Epoch 2704, Loss 2.93038010597229\n",
      "\t Params: tensor([  5.3366, -17.1284])\n",
      "\t Grad: tensor([-0.0053,  0.0300])\n",
      "Epoch 2705, Loss 2.930370330810547\n",
      "\t Params: tensor([  5.3366, -17.1287])\n",
      "\t Grad: tensor([-0.0053,  0.0300])\n",
      "Epoch 2706, Loss 2.9303598403930664\n",
      "\t Params: tensor([  5.3367, -17.1290])\n",
      "\t Grad: tensor([-0.0053,  0.0299])\n",
      "Epoch 2707, Loss 2.9303526878356934\n",
      "\t Params: tensor([  5.3367, -17.1293])\n",
      "\t Grad: tensor([-0.0053,  0.0299])\n",
      "Epoch 2708, Loss 2.930342197418213\n",
      "\t Params: tensor([  5.3368, -17.1296])\n",
      "\t Grad: tensor([-0.0053,  0.0298])\n",
      "Epoch 2709, Loss 2.9303345680236816\n",
      "\t Params: tensor([  5.3368, -17.1299])\n",
      "\t Grad: tensor([-0.0053,  0.0298])\n",
      "Epoch 2710, Loss 2.9303247928619385\n",
      "\t Params: tensor([  5.3369, -17.1302])\n",
      "\t Grad: tensor([-0.0053,  0.0297])\n",
      "Epoch 2711, Loss 2.930314540863037\n",
      "\t Params: tensor([  5.3369, -17.1305])\n",
      "\t Grad: tensor([-0.0053,  0.0297])\n",
      "Epoch 2712, Loss 2.9303061962127686\n",
      "\t Params: tensor([  5.3370, -17.1308])\n",
      "\t Grad: tensor([-0.0052,  0.0296])\n",
      "Epoch 2713, Loss 2.930297613143921\n",
      "\t Params: tensor([  5.3370, -17.1311])\n",
      "\t Grad: tensor([-0.0052,  0.0296])\n",
      "Epoch 2714, Loss 2.930288076400757\n",
      "\t Params: tensor([  5.3371, -17.1314])\n",
      "\t Grad: tensor([-0.0052,  0.0295])\n",
      "Epoch 2715, Loss 2.930278778076172\n",
      "\t Params: tensor([  5.3371, -17.1317])\n",
      "\t Grad: tensor([-0.0052,  0.0295])\n",
      "Epoch 2716, Loss 2.930270195007324\n",
      "\t Params: tensor([  5.3372, -17.1320])\n",
      "\t Grad: tensor([-0.0052,  0.0294])\n",
      "Epoch 2717, Loss 2.9302618503570557\n",
      "\t Params: tensor([  5.3372, -17.1323])\n",
      "\t Grad: tensor([-0.0052,  0.0294])\n",
      "Epoch 2718, Loss 2.930253505706787\n",
      "\t Params: tensor([  5.3373, -17.1326])\n",
      "\t Grad: tensor([-0.0052,  0.0293])\n",
      "Epoch 2719, Loss 2.930243730545044\n",
      "\t Params: tensor([  5.3373, -17.1329])\n",
      "\t Grad: tensor([-0.0052,  0.0293])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2720, Loss 2.9302353858947754\n",
      "\t Params: tensor([  5.3374, -17.1332])\n",
      "\t Grad: tensor([-0.0052,  0.0292])\n",
      "Epoch 2721, Loss 2.9302256107330322\n",
      "\t Params: tensor([  5.3375, -17.1334])\n",
      "\t Grad: tensor([-0.0052,  0.0292])\n",
      "Epoch 2722, Loss 2.9302175045013428\n",
      "\t Params: tensor([  5.3375, -17.1337])\n",
      "\t Grad: tensor([-0.0051,  0.0291])\n",
      "Epoch 2723, Loss 2.930209159851074\n",
      "\t Params: tensor([  5.3376, -17.1340])\n",
      "\t Grad: tensor([-0.0051,  0.0291])\n",
      "Epoch 2724, Loss 2.9302010536193848\n",
      "\t Params: tensor([  5.3376, -17.1343])\n",
      "\t Grad: tensor([-0.0051,  0.0290])\n",
      "Epoch 2725, Loss 2.930190324783325\n",
      "\t Params: tensor([  5.3377, -17.1346])\n",
      "\t Grad: tensor([-0.0051,  0.0290])\n",
      "Epoch 2726, Loss 2.930182695388794\n",
      "\t Params: tensor([  5.3377, -17.1349])\n",
      "\t Grad: tensor([-0.0051,  0.0289])\n",
      "Epoch 2727, Loss 2.930172920227051\n",
      "\t Params: tensor([  5.3378, -17.1352])\n",
      "\t Grad: tensor([-0.0051,  0.0289])\n",
      "Epoch 2728, Loss 2.930166482925415\n",
      "\t Params: tensor([  5.3378, -17.1355])\n",
      "\t Grad: tensor([-0.0051,  0.0288])\n",
      "Epoch 2729, Loss 2.9301564693450928\n",
      "\t Params: tensor([  5.3379, -17.1358])\n",
      "\t Grad: tensor([-0.0051,  0.0288])\n",
      "Epoch 2730, Loss 2.9301488399505615\n",
      "\t Params: tensor([  5.3379, -17.1360])\n",
      "\t Grad: tensor([-0.0051,  0.0287])\n",
      "Epoch 2731, Loss 2.9301393032073975\n",
      "\t Params: tensor([  5.3380, -17.1363])\n",
      "\t Grad: tensor([-0.0051,  0.0287])\n",
      "Epoch 2732, Loss 2.930131435394287\n",
      "\t Params: tensor([  5.3380, -17.1366])\n",
      "\t Grad: tensor([-0.0050,  0.0286])\n",
      "Epoch 2733, Loss 2.9301233291625977\n",
      "\t Params: tensor([  5.3381, -17.1369])\n",
      "\t Grad: tensor([-0.0050,  0.0286])\n",
      "Epoch 2734, Loss 2.9301130771636963\n",
      "\t Params: tensor([  5.3381, -17.1372])\n",
      "\t Grad: tensor([-0.0050,  0.0285])\n",
      "Epoch 2735, Loss 2.930107355117798\n",
      "\t Params: tensor([  5.3382, -17.1375])\n",
      "\t Grad: tensor([-0.0051,  0.0285])\n",
      "Epoch 2736, Loss 2.93009877204895\n",
      "\t Params: tensor([  5.3382, -17.1378])\n",
      "\t Grad: tensor([-0.0050,  0.0284])\n",
      "Epoch 2737, Loss 2.9300901889801025\n",
      "\t Params: tensor([  5.3383, -17.1380])\n",
      "\t Grad: tensor([-0.0050,  0.0284])\n",
      "Epoch 2738, Loss 2.9300811290740967\n",
      "\t Params: tensor([  5.3383, -17.1383])\n",
      "\t Grad: tensor([-0.0050,  0.0283])\n",
      "Epoch 2739, Loss 2.9300730228424072\n",
      "\t Params: tensor([  5.3384, -17.1386])\n",
      "\t Grad: tensor([-0.0050,  0.0283])\n",
      "Epoch 2740, Loss 2.9300642013549805\n",
      "\t Params: tensor([  5.3384, -17.1389])\n",
      "\t Grad: tensor([-0.0050,  0.0282])\n",
      "Epoch 2741, Loss 2.930056095123291\n",
      "\t Params: tensor([  5.3385, -17.1392])\n",
      "\t Grad: tensor([-0.0050,  0.0282])\n",
      "Epoch 2742, Loss 2.9300484657287598\n",
      "\t Params: tensor([  5.3385, -17.1395])\n",
      "\t Grad: tensor([-0.0050,  0.0281])\n",
      "Epoch 2743, Loss 2.9300413131713867\n",
      "\t Params: tensor([  5.3386, -17.1397])\n",
      "\t Grad: tensor([-0.0050,  0.0281])\n",
      "Epoch 2744, Loss 2.9300315380096436\n",
      "\t Params: tensor([  5.3386, -17.1400])\n",
      "\t Grad: tensor([-0.0050,  0.0280])\n",
      "Epoch 2745, Loss 2.9300217628479004\n",
      "\t Params: tensor([  5.3387, -17.1403])\n",
      "\t Grad: tensor([-0.0050,  0.0280])\n",
      "Epoch 2746, Loss 2.930016279220581\n",
      "\t Params: tensor([  5.3387, -17.1406])\n",
      "\t Grad: tensor([-0.0049,  0.0279])\n",
      "Epoch 2747, Loss 2.9300081729888916\n",
      "\t Params: tensor([  5.3388, -17.1409])\n",
      "\t Grad: tensor([-0.0049,  0.0279])\n",
      "Epoch 2748, Loss 2.9300003051757812\n",
      "\t Params: tensor([  5.3388, -17.1411])\n",
      "\t Grad: tensor([-0.0049,  0.0279])\n",
      "Epoch 2749, Loss 2.9299917221069336\n",
      "\t Params: tensor([  5.3389, -17.1414])\n",
      "\t Grad: tensor([-0.0049,  0.0278])\n",
      "Epoch 2750, Loss 2.929983377456665\n",
      "\t Params: tensor([  5.3389, -17.1417])\n",
      "\t Grad: tensor([-0.0049,  0.0278])\n",
      "Epoch 2751, Loss 2.9299752712249756\n",
      "\t Params: tensor([  5.3390, -17.1420])\n",
      "\t Grad: tensor([-0.0049,  0.0277])\n",
      "Epoch 2752, Loss 2.9299681186676025\n",
      "\t Params: tensor([  5.3390, -17.1422])\n",
      "\t Grad: tensor([-0.0049,  0.0277])\n",
      "Epoch 2753, Loss 2.9299604892730713\n",
      "\t Params: tensor([  5.3391, -17.1425])\n",
      "\t Grad: tensor([-0.0049,  0.0276])\n",
      "Epoch 2754, Loss 2.929953098297119\n",
      "\t Params: tensor([  5.3391, -17.1428])\n",
      "\t Grad: tensor([-0.0049,  0.0276])\n",
      "Epoch 2755, Loss 2.929945230484009\n",
      "\t Params: tensor([  5.3392, -17.1431])\n",
      "\t Grad: tensor([-0.0049,  0.0275])\n",
      "Epoch 2756, Loss 2.929936408996582\n",
      "\t Params: tensor([  5.3392, -17.1433])\n",
      "\t Grad: tensor([-0.0049,  0.0275])\n",
      "Epoch 2757, Loss 2.929928779602051\n",
      "\t Params: tensor([  5.3392, -17.1436])\n",
      "\t Grad: tensor([-0.0049,  0.0274])\n",
      "Epoch 2758, Loss 2.9299211502075195\n",
      "\t Params: tensor([  5.3393, -17.1439])\n",
      "\t Grad: tensor([-0.0048,  0.0274])\n",
      "Epoch 2759, Loss 2.9299135208129883\n",
      "\t Params: tensor([  5.3393, -17.1442])\n",
      "\t Grad: tensor([-0.0049,  0.0273])\n",
      "Epoch 2760, Loss 2.9299049377441406\n",
      "\t Params: tensor([  5.3394, -17.1444])\n",
      "\t Grad: tensor([-0.0048,  0.0273])\n",
      "Epoch 2761, Loss 2.9298958778381348\n",
      "\t Params: tensor([  5.3394, -17.1447])\n",
      "\t Grad: tensor([-0.0048,  0.0272])\n",
      "Epoch 2762, Loss 2.9298911094665527\n",
      "\t Params: tensor([  5.3395, -17.1450])\n",
      "\t Grad: tensor([-0.0048,  0.0272])\n",
      "Epoch 2763, Loss 2.9298818111419678\n",
      "\t Params: tensor([  5.3395, -17.1453])\n",
      "\t Grad: tensor([-0.0048,  0.0271])\n",
      "Epoch 2764, Loss 2.9298746585845947\n",
      "\t Params: tensor([  5.3396, -17.1455])\n",
      "\t Grad: tensor([-0.0048,  0.0271])\n",
      "Epoch 2765, Loss 2.929868459701538\n",
      "\t Params: tensor([  5.3396, -17.1458])\n",
      "\t Grad: tensor([-0.0048,  0.0271])\n",
      "Epoch 2766, Loss 2.9298593997955322\n",
      "\t Params: tensor([  5.3397, -17.1461])\n",
      "\t Grad: tensor([-0.0048,  0.0270])\n",
      "Epoch 2767, Loss 2.929851770401001\n",
      "\t Params: tensor([  5.3397, -17.1463])\n",
      "\t Grad: tensor([-0.0048,  0.0270])\n",
      "Epoch 2768, Loss 2.929844856262207\n",
      "\t Params: tensor([  5.3398, -17.1466])\n",
      "\t Grad: tensor([-0.0048,  0.0269])\n",
      "Epoch 2769, Loss 2.929837942123413\n",
      "\t Params: tensor([  5.3398, -17.1469])\n",
      "\t Grad: tensor([-0.0047,  0.0269])\n",
      "Epoch 2770, Loss 2.9298300743103027\n",
      "\t Params: tensor([  5.3399, -17.1471])\n",
      "\t Grad: tensor([-0.0047,  0.0268])\n",
      "Epoch 2771, Loss 2.9298219680786133\n",
      "\t Params: tensor([  5.3399, -17.1474])\n",
      "\t Grad: tensor([-0.0047,  0.0268])\n",
      "Epoch 2772, Loss 2.929816246032715\n",
      "\t Params: tensor([  5.3400, -17.1477])\n",
      "\t Grad: tensor([-0.0047,  0.0267])\n",
      "Epoch 2773, Loss 2.929806709289551\n",
      "\t Params: tensor([  5.3400, -17.1479])\n",
      "\t Grad: tensor([-0.0047,  0.0267])\n",
      "Epoch 2774, Loss 2.9297995567321777\n",
      "\t Params: tensor([  5.3401, -17.1482])\n",
      "\t Grad: tensor([-0.0047,  0.0266])\n",
      "Epoch 2775, Loss 2.9297935962677\n",
      "\t Params: tensor([  5.3401, -17.1485])\n",
      "\t Grad: tensor([-0.0047,  0.0266])\n",
      "Epoch 2776, Loss 2.929786443710327\n",
      "\t Params: tensor([  5.3402, -17.1487])\n",
      "\t Grad: tensor([-0.0047,  0.0266])\n",
      "Epoch 2777, Loss 2.9297780990600586\n",
      "\t Params: tensor([  5.3402, -17.1490])\n",
      "\t Grad: tensor([-0.0047,  0.0265])\n",
      "Epoch 2778, Loss 2.9297711849212646\n",
      "\t Params: tensor([  5.3402, -17.1493])\n",
      "\t Grad: tensor([-0.0047,  0.0265])\n",
      "Epoch 2779, Loss 2.929764747619629\n",
      "\t Params: tensor([  5.3403, -17.1495])\n",
      "\t Grad: tensor([-0.0047,  0.0264])\n",
      "Epoch 2780, Loss 2.9297568798065186\n",
      "\t Params: tensor([  5.3403, -17.1498])\n",
      "\t Grad: tensor([-0.0047,  0.0264])\n",
      "Epoch 2781, Loss 2.9297499656677246\n",
      "\t Params: tensor([  5.3404, -17.1501])\n",
      "\t Grad: tensor([-0.0046,  0.0263])\n",
      "Epoch 2782, Loss 2.9297430515289307\n",
      "\t Params: tensor([  5.3404, -17.1503])\n",
      "\t Grad: tensor([-0.0046,  0.0263])\n",
      "Epoch 2783, Loss 2.929734945297241\n",
      "\t Params: tensor([  5.3405, -17.1506])\n",
      "\t Grad: tensor([-0.0046,  0.0262])\n",
      "Epoch 2784, Loss 2.9297292232513428\n",
      "\t Params: tensor([  5.3405, -17.1508])\n",
      "\t Grad: tensor([-0.0047,  0.0262])\n",
      "Epoch 2785, Loss 2.9297220706939697\n",
      "\t Params: tensor([  5.3406, -17.1511])\n",
      "\t Grad: tensor([-0.0046,  0.0262])\n",
      "Epoch 2786, Loss 2.9297142028808594\n",
      "\t Params: tensor([  5.3406, -17.1514])\n",
      "\t Grad: tensor([-0.0046,  0.0261])\n",
      "Epoch 2787, Loss 2.929706573486328\n",
      "\t Params: tensor([  5.3407, -17.1516])\n",
      "\t Grad: tensor([-0.0046,  0.0261])\n",
      "Epoch 2788, Loss 2.9297006130218506\n",
      "\t Params: tensor([  5.3407, -17.1519])\n",
      "\t Grad: tensor([-0.0046,  0.0260])\n",
      "Epoch 2789, Loss 2.929692029953003\n",
      "\t Params: tensor([  5.3408, -17.1522])\n",
      "\t Grad: tensor([-0.0046,  0.0260])\n",
      "Epoch 2790, Loss 2.929685115814209\n",
      "\t Params: tensor([  5.3408, -17.1524])\n",
      "\t Grad: tensor([-0.0046,  0.0259])\n",
      "Epoch 2791, Loss 2.929680585861206\n",
      "\t Params: tensor([  5.3408, -17.1527])\n",
      "\t Grad: tensor([-0.0046,  0.0259])\n",
      "Epoch 2792, Loss 2.9296722412109375\n",
      "\t Params: tensor([  5.3409, -17.1529])\n",
      "\t Grad: tensor([-0.0046,  0.0258])\n",
      "Epoch 2793, Loss 2.929666042327881\n",
      "\t Params: tensor([  5.3409, -17.1532])\n",
      "\t Grad: tensor([-0.0046,  0.0258])\n",
      "Epoch 2794, Loss 2.9296586513519287\n",
      "\t Params: tensor([  5.3410, -17.1534])\n",
      "\t Grad: tensor([-0.0045,  0.0258])\n",
      "Epoch 2795, Loss 2.9296529293060303\n",
      "\t Params: tensor([  5.3410, -17.1537])\n",
      "\t Grad: tensor([-0.0045,  0.0257])\n",
      "Epoch 2796, Loss 2.9296462535858154\n",
      "\t Params: tensor([  5.3411, -17.1540])\n",
      "\t Grad: tensor([-0.0045,  0.0257])\n",
      "Epoch 2797, Loss 2.929638147354126\n",
      "\t Params: tensor([  5.3411, -17.1542])\n",
      "\t Grad: tensor([-0.0045,  0.0256])\n",
      "Epoch 2798, Loss 2.9296319484710693\n",
      "\t Params: tensor([  5.3412, -17.1545])\n",
      "\t Grad: tensor([-0.0045,  0.0256])\n",
      "Epoch 2799, Loss 2.929626226425171\n",
      "\t Params: tensor([  5.3412, -17.1547])\n",
      "\t Grad: tensor([-0.0045,  0.0255])\n",
      "Epoch 2800, Loss 2.929619789123535\n",
      "\t Params: tensor([  5.3413, -17.1550])\n",
      "\t Grad: tensor([-0.0045,  0.0255])\n",
      "Epoch 2801, Loss 2.9296114444732666\n",
      "\t Params: tensor([  5.3413, -17.1552])\n",
      "\t Grad: tensor([-0.0045,  0.0254])\n",
      "Epoch 2802, Loss 2.9296045303344727\n",
      "\t Params: tensor([  5.3413, -17.1555])\n",
      "\t Grad: tensor([-0.0045,  0.0254])\n",
      "Epoch 2803, Loss 2.9295997619628906\n",
      "\t Params: tensor([  5.3414, -17.1557])\n",
      "\t Grad: tensor([-0.0045,  0.0254])\n",
      "Epoch 2804, Loss 2.9295923709869385\n",
      "\t Params: tensor([  5.3414, -17.1560])\n",
      "\t Grad: tensor([-0.0045,  0.0253])\n",
      "Epoch 2805, Loss 2.929586172103882\n",
      "\t Params: tensor([  5.3415, -17.1562])\n",
      "\t Grad: tensor([-0.0045,  0.0253])\n",
      "Epoch 2806, Loss 2.9295785427093506\n",
      "\t Params: tensor([  5.3415, -17.1565])\n",
      "\t Grad: tensor([-0.0045,  0.0252])\n",
      "Epoch 2807, Loss 2.929572343826294\n",
      "\t Params: tensor([  5.3416, -17.1568])\n",
      "\t Grad: tensor([-0.0044,  0.0252])\n",
      "Epoch 2808, Loss 2.929565668106079\n",
      "\t Params: tensor([  5.3416, -17.1570])\n",
      "\t Grad: tensor([-0.0044,  0.0251])\n",
      "Epoch 2809, Loss 2.9295592308044434\n",
      "\t Params: tensor([  5.3417, -17.1573])\n",
      "\t Grad: tensor([-0.0044,  0.0251])\n",
      "Epoch 2810, Loss 2.929551124572754\n",
      "\t Params: tensor([  5.3417, -17.1575])\n",
      "\t Grad: tensor([-0.0044,  0.0251])\n",
      "Epoch 2811, Loss 2.9295454025268555\n",
      "\t Params: tensor([  5.3417, -17.1578])\n",
      "\t Grad: tensor([-0.0044,  0.0250])\n",
      "Epoch 2812, Loss 2.9295403957366943\n",
      "\t Params: tensor([  5.3418, -17.1580])\n",
      "\t Grad: tensor([-0.0044,  0.0250])\n",
      "Epoch 2813, Loss 2.9295334815979004\n",
      "\t Params: tensor([  5.3418, -17.1583])\n",
      "\t Grad: tensor([-0.0044,  0.0249])\n",
      "Epoch 2814, Loss 2.929527521133423\n",
      "\t Params: tensor([  5.3419, -17.1585])\n",
      "\t Grad: tensor([-0.0044,  0.0249])\n",
      "Epoch 2815, Loss 2.929520606994629\n",
      "\t Params: tensor([  5.3419, -17.1588])\n",
      "\t Grad: tensor([-0.0044,  0.0249])\n",
      "Epoch 2816, Loss 2.929513454437256\n",
      "\t Params: tensor([  5.3420, -17.1590])\n",
      "\t Grad: tensor([-0.0044,  0.0248])\n",
      "Epoch 2817, Loss 2.929507255554199\n",
      "\t Params: tensor([  5.3420, -17.1592])\n",
      "\t Grad: tensor([-0.0043,  0.0248])\n",
      "Epoch 2818, Loss 2.9295005798339844\n",
      "\t Params: tensor([  5.3421, -17.1595])\n",
      "\t Grad: tensor([-0.0044,  0.0247])\n",
      "Epoch 2819, Loss 2.9294958114624023\n",
      "\t Params: tensor([  5.3421, -17.1597])\n",
      "\t Grad: tensor([-0.0044,  0.0247])\n",
      "Epoch 2820, Loss 2.9294893741607666\n",
      "\t Params: tensor([  5.3421, -17.1600])\n",
      "\t Grad: tensor([-0.0044,  0.0246])\n",
      "Epoch 2821, Loss 2.9294822216033936\n",
      "\t Params: tensor([  5.3422, -17.1602])\n",
      "\t Grad: tensor([-0.0043,  0.0246])\n",
      "Epoch 2822, Loss 2.9294755458831787\n",
      "\t Params: tensor([  5.3422, -17.1605])\n",
      "\t Grad: tensor([-0.0043,  0.0246])\n",
      "Epoch 2823, Loss 2.929471015930176\n",
      "\t Params: tensor([  5.3423, -17.1607])\n",
      "\t Grad: tensor([-0.0043,  0.0245])\n",
      "Epoch 2824, Loss 2.9294631481170654\n",
      "\t Params: tensor([  5.3423, -17.1610])\n",
      "\t Grad: tensor([-0.0043,  0.0245])\n",
      "Epoch 2825, Loss 2.929457664489746\n",
      "\t Params: tensor([  5.3424, -17.1612])\n",
      "\t Grad: tensor([-0.0043,  0.0244])\n",
      "Epoch 2826, Loss 2.929452419281006\n",
      "\t Params: tensor([  5.3424, -17.1615])\n",
      "\t Grad: tensor([-0.0043,  0.0244])\n",
      "Epoch 2827, Loss 2.9294447898864746\n",
      "\t Params: tensor([  5.3424, -17.1617])\n",
      "\t Grad: tensor([-0.0043,  0.0243])\n",
      "Epoch 2828, Loss 2.9294393062591553\n",
      "\t Params: tensor([  5.3425, -17.1619])\n",
      "\t Grad: tensor([-0.0043,  0.0243])\n",
      "Epoch 2829, Loss 2.9294326305389404\n",
      "\t Params: tensor([  5.3425, -17.1622])\n",
      "\t Grad: tensor([-0.0043,  0.0243])\n",
      "Epoch 2830, Loss 2.929426670074463\n",
      "\t Params: tensor([  5.3426, -17.1624])\n",
      "\t Grad: tensor([-0.0043,  0.0242])\n",
      "Epoch 2831, Loss 2.9294214248657227\n",
      "\t Params: tensor([  5.3426, -17.1627])\n",
      "\t Grad: tensor([-0.0043,  0.0242])\n",
      "Epoch 2832, Loss 2.929415225982666\n",
      "\t Params: tensor([  5.3427, -17.1629])\n",
      "\t Grad: tensor([-0.0043,  0.0241])\n",
      "Epoch 2833, Loss 2.9294090270996094\n",
      "\t Params: tensor([  5.3427, -17.1632])\n",
      "\t Grad: tensor([-0.0043,  0.0241])\n",
      "Epoch 2834, Loss 2.929403781890869\n",
      "\t Params: tensor([  5.3427, -17.1634])\n",
      "\t Grad: tensor([-0.0043,  0.0241])\n",
      "Epoch 2835, Loss 2.929396152496338\n",
      "\t Params: tensor([  5.3428, -17.1636])\n",
      "\t Grad: tensor([-0.0042,  0.0240])\n",
      "Epoch 2836, Loss 2.9293906688690186\n",
      "\t Params: tensor([  5.3428, -17.1639])\n",
      "\t Grad: tensor([-0.0042,  0.0240])\n",
      "Epoch 2837, Loss 2.9293832778930664\n",
      "\t Params: tensor([  5.3429, -17.1641])\n",
      "\t Grad: tensor([-0.0042,  0.0239])\n",
      "Epoch 2838, Loss 2.929380178451538\n",
      "\t Params: tensor([  5.3429, -17.1644])\n",
      "\t Grad: tensor([-0.0042,  0.0239])\n",
      "Epoch 2839, Loss 2.929373025894165\n",
      "\t Params: tensor([  5.3430, -17.1646])\n",
      "\t Grad: tensor([-0.0042,  0.0239])\n",
      "Epoch 2840, Loss 2.929367780685425\n",
      "\t Params: tensor([  5.3430, -17.1648])\n",
      "\t Grad: tensor([-0.0042,  0.0238])\n",
      "Epoch 2841, Loss 2.92936110496521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params: tensor([  5.3430, -17.1651])\n",
      "\t Grad: tensor([-0.0042,  0.0238])\n",
      "Epoch 2842, Loss 2.9293556213378906\n",
      "\t Params: tensor([  5.3431, -17.1653])\n",
      "\t Grad: tensor([-0.0042,  0.0237])\n",
      "Epoch 2843, Loss 2.9293508529663086\n",
      "\t Params: tensor([  5.3431, -17.1655])\n",
      "\t Grad: tensor([-0.0042,  0.0237])\n",
      "Epoch 2844, Loss 2.9293439388275146\n",
      "\t Params: tensor([  5.3432, -17.1658])\n",
      "\t Grad: tensor([-0.0042,  0.0237])\n",
      "Epoch 2845, Loss 2.929338216781616\n",
      "\t Params: tensor([  5.3432, -17.1660])\n",
      "\t Grad: tensor([-0.0042,  0.0236])\n",
      "Epoch 2846, Loss 2.9293320178985596\n",
      "\t Params: tensor([  5.3432, -17.1662])\n",
      "\t Grad: tensor([-0.0042,  0.0236])\n",
      "Epoch 2847, Loss 2.929327964782715\n",
      "\t Params: tensor([  5.3433, -17.1665])\n",
      "\t Grad: tensor([-0.0042,  0.0235])\n",
      "Epoch 2848, Loss 2.9293205738067627\n",
      "\t Params: tensor([  5.3433, -17.1667])\n",
      "\t Grad: tensor([-0.0041,  0.0235])\n",
      "Epoch 2849, Loss 2.9293158054351807\n",
      "\t Params: tensor([  5.3434, -17.1670])\n",
      "\t Grad: tensor([-0.0041,  0.0235])\n",
      "Epoch 2850, Loss 2.929309129714966\n",
      "\t Params: tensor([  5.3434, -17.1672])\n",
      "\t Grad: tensor([-0.0041,  0.0234])\n",
      "Epoch 2851, Loss 2.9293036460876465\n",
      "\t Params: tensor([  5.3435, -17.1674])\n",
      "\t Grad: tensor([-0.0041,  0.0234])\n",
      "Epoch 2852, Loss 2.92930006980896\n",
      "\t Params: tensor([  5.3435, -17.1677])\n",
      "\t Grad: tensor([-0.0041,  0.0233])\n",
      "Epoch 2853, Loss 2.929293155670166\n",
      "\t Params: tensor([  5.3435, -17.1679])\n",
      "\t Grad: tensor([-0.0041,  0.0233])\n",
      "Epoch 2854, Loss 2.929287910461426\n",
      "\t Params: tensor([  5.3436, -17.1681])\n",
      "\t Grad: tensor([-0.0041,  0.0233])\n",
      "Epoch 2855, Loss 2.929281711578369\n",
      "\t Params: tensor([  5.3436, -17.1684])\n",
      "\t Grad: tensor([-0.0041,  0.0232])\n",
      "Epoch 2856, Loss 2.929276943206787\n",
      "\t Params: tensor([  5.3437, -17.1686])\n",
      "\t Grad: tensor([-0.0041,  0.0232])\n",
      "Epoch 2857, Loss 2.9292707443237305\n",
      "\t Params: tensor([  5.3437, -17.1688])\n",
      "\t Grad: tensor([-0.0041,  0.0231])\n",
      "Epoch 2858, Loss 2.9292662143707275\n",
      "\t Params: tensor([  5.3437, -17.1690])\n",
      "\t Grad: tensor([-0.0041,  0.0231])\n",
      "Epoch 2859, Loss 2.92926025390625\n",
      "\t Params: tensor([  5.3438, -17.1693])\n",
      "\t Grad: tensor([-0.0041,  0.0231])\n",
      "Epoch 2860, Loss 2.9292550086975098\n",
      "\t Params: tensor([  5.3438, -17.1695])\n",
      "\t Grad: tensor([-0.0041,  0.0230])\n",
      "Epoch 2861, Loss 2.9292502403259277\n",
      "\t Params: tensor([  5.3439, -17.1697])\n",
      "\t Grad: tensor([-0.0041,  0.0230])\n",
      "Epoch 2862, Loss 2.929243564605713\n",
      "\t Params: tensor([  5.3439, -17.1700])\n",
      "\t Grad: tensor([-0.0040,  0.0229])\n",
      "Epoch 2863, Loss 2.9292376041412354\n",
      "\t Params: tensor([  5.3439, -17.1702])\n",
      "\t Grad: tensor([-0.0040,  0.0229])\n",
      "Epoch 2864, Loss 2.929234266281128\n",
      "\t Params: tensor([  5.3440, -17.1704])\n",
      "\t Grad: tensor([-0.0040,  0.0229])\n",
      "Epoch 2865, Loss 2.9292280673980713\n",
      "\t Params: tensor([  5.3440, -17.1707])\n",
      "\t Grad: tensor([-0.0040,  0.0228])\n",
      "Epoch 2866, Loss 2.9292221069335938\n",
      "\t Params: tensor([  5.3441, -17.1709])\n",
      "\t Grad: tensor([-0.0040,  0.0228])\n",
      "Epoch 2867, Loss 2.9292166233062744\n",
      "\t Params: tensor([  5.3441, -17.1711])\n",
      "\t Grad: tensor([-0.0040,  0.0227])\n",
      "Epoch 2868, Loss 2.929210901260376\n",
      "\t Params: tensor([  5.3441, -17.1713])\n",
      "\t Grad: tensor([-0.0040,  0.0227])\n",
      "Epoch 2869, Loss 2.9292075634002686\n",
      "\t Params: tensor([  5.3442, -17.1716])\n",
      "\t Grad: tensor([-0.0040,  0.0227])\n",
      "Epoch 2870, Loss 2.9292006492614746\n",
      "\t Params: tensor([  5.3442, -17.1718])\n",
      "\t Grad: tensor([-0.0040,  0.0226])\n",
      "Epoch 2871, Loss 2.9291954040527344\n",
      "\t Params: tensor([  5.3443, -17.1720])\n",
      "\t Grad: tensor([-0.0040,  0.0226])\n",
      "Epoch 2872, Loss 2.9291908740997314\n",
      "\t Params: tensor([  5.3443, -17.1722])\n",
      "\t Grad: tensor([-0.0040,  0.0226])\n",
      "Epoch 2873, Loss 2.929184675216675\n",
      "\t Params: tensor([  5.3443, -17.1725])\n",
      "\t Grad: tensor([-0.0040,  0.0225])\n",
      "Epoch 2874, Loss 2.929180383682251\n",
      "\t Params: tensor([  5.3444, -17.1727])\n",
      "\t Grad: tensor([-0.0040,  0.0225])\n",
      "Epoch 2875, Loss 2.9291746616363525\n",
      "\t Params: tensor([  5.3444, -17.1729])\n",
      "\t Grad: tensor([-0.0040,  0.0224])\n",
      "Epoch 2876, Loss 2.9291696548461914\n",
      "\t Params: tensor([  5.3445, -17.1731])\n",
      "\t Grad: tensor([-0.0040,  0.0224])\n",
      "Epoch 2877, Loss 2.9291646480560303\n",
      "\t Params: tensor([  5.3445, -17.1734])\n",
      "\t Grad: tensor([-0.0040,  0.0224])\n",
      "Epoch 2878, Loss 2.9291601181030273\n",
      "\t Params: tensor([  5.3445, -17.1736])\n",
      "\t Grad: tensor([-0.0039,  0.0223])\n",
      "Epoch 2879, Loss 2.929154872894287\n",
      "\t Params: tensor([  5.3446, -17.1738])\n",
      "\t Grad: tensor([-0.0039,  0.0223])\n",
      "Epoch 2880, Loss 2.9291486740112305\n",
      "\t Params: tensor([  5.3446, -17.1740])\n",
      "\t Grad: tensor([-0.0039,  0.0223])\n",
      "Epoch 2881, Loss 2.9291434288024902\n",
      "\t Params: tensor([  5.3447, -17.1742])\n",
      "\t Grad: tensor([-0.0039,  0.0222])\n",
      "Epoch 2882, Loss 2.9291391372680664\n",
      "\t Params: tensor([  5.3447, -17.1745])\n",
      "\t Grad: tensor([-0.0039,  0.0222])\n",
      "Epoch 2883, Loss 2.929133415222168\n",
      "\t Params: tensor([  5.3447, -17.1747])\n",
      "\t Grad: tensor([-0.0039,  0.0221])\n",
      "Epoch 2884, Loss 2.9291276931762695\n",
      "\t Params: tensor([  5.3448, -17.1749])\n",
      "\t Grad: tensor([-0.0039,  0.0221])\n",
      "Epoch 2885, Loss 2.92912220954895\n",
      "\t Params: tensor([  5.3448, -17.1751])\n",
      "\t Grad: tensor([-0.0039,  0.0221])\n",
      "Epoch 2886, Loss 2.9291188716888428\n",
      "\t Params: tensor([  5.3449, -17.1754])\n",
      "\t Grad: tensor([-0.0039,  0.0220])\n",
      "Epoch 2887, Loss 2.9291129112243652\n",
      "\t Params: tensor([  5.3449, -17.1756])\n",
      "\t Grad: tensor([-0.0039,  0.0220])\n",
      "Epoch 2888, Loss 2.929107904434204\n",
      "\t Params: tensor([  5.3449, -17.1758])\n",
      "\t Grad: tensor([-0.0039,  0.0220])\n",
      "Epoch 2889, Loss 2.9291040897369385\n",
      "\t Params: tensor([  5.3450, -17.1760])\n",
      "\t Grad: tensor([-0.0039,  0.0219])\n",
      "Epoch 2890, Loss 2.929098606109619\n",
      "\t Params: tensor([  5.3450, -17.1762])\n",
      "\t Grad: tensor([-0.0039,  0.0219])\n",
      "Epoch 2891, Loss 2.9290926456451416\n",
      "\t Params: tensor([  5.3450, -17.1764])\n",
      "\t Grad: tensor([-0.0039,  0.0218])\n",
      "Epoch 2892, Loss 2.9290883541107178\n",
      "\t Params: tensor([  5.3451, -17.1767])\n",
      "\t Grad: tensor([-0.0039,  0.0218])\n",
      "Epoch 2893, Loss 2.9290831089019775\n",
      "\t Params: tensor([  5.3451, -17.1769])\n",
      "\t Grad: tensor([-0.0038,  0.0218])\n",
      "Epoch 2894, Loss 2.929079294204712\n",
      "\t Params: tensor([  5.3452, -17.1771])\n",
      "\t Grad: tensor([-0.0038,  0.0217])\n",
      "Epoch 2895, Loss 2.929074287414551\n",
      "\t Params: tensor([  5.3452, -17.1773])\n",
      "\t Grad: tensor([-0.0038,  0.0217])\n",
      "Epoch 2896, Loss 2.9290685653686523\n",
      "\t Params: tensor([  5.3452, -17.1775])\n",
      "\t Grad: tensor([-0.0038,  0.0217])\n",
      "Epoch 2897, Loss 2.9290647506713867\n",
      "\t Params: tensor([  5.3453, -17.1777])\n",
      "\t Grad: tensor([-0.0038,  0.0216])\n",
      "Epoch 2898, Loss 2.929058313369751\n",
      "\t Params: tensor([  5.3453, -17.1780])\n",
      "\t Grad: tensor([-0.0038,  0.0216])\n",
      "Epoch 2899, Loss 2.9290544986724854\n",
      "\t Params: tensor([  5.3454, -17.1782])\n",
      "\t Grad: tensor([-0.0038,  0.0215])\n",
      "Epoch 2900, Loss 2.9290504455566406\n",
      "\t Params: tensor([  5.3454, -17.1784])\n",
      "\t Grad: tensor([-0.0038,  0.0215])\n",
      "Epoch 2901, Loss 2.929043769836426\n",
      "\t Params: tensor([  5.3454, -17.1786])\n",
      "\t Grad: tensor([-0.0038,  0.0215])\n",
      "Epoch 2902, Loss 2.9290411472320557\n",
      "\t Params: tensor([  5.3455, -17.1788])\n",
      "\t Grad: tensor([-0.0038,  0.0214])\n",
      "Epoch 2903, Loss 2.9290359020233154\n",
      "\t Params: tensor([  5.3455, -17.1790])\n",
      "\t Grad: tensor([-0.0038,  0.0214])\n",
      "Epoch 2904, Loss 2.9290311336517334\n",
      "\t Params: tensor([  5.3455, -17.1793])\n",
      "\t Grad: tensor([-0.0038,  0.0214])\n",
      "Epoch 2905, Loss 2.929025411605835\n",
      "\t Params: tensor([  5.3456, -17.1795])\n",
      "\t Grad: tensor([-0.0038,  0.0213])\n",
      "Epoch 2906, Loss 2.9290213584899902\n",
      "\t Params: tensor([  5.3456, -17.1797])\n",
      "\t Grad: tensor([-0.0038,  0.0213])\n",
      "Epoch 2907, Loss 2.9290170669555664\n",
      "\t Params: tensor([  5.3457, -17.1799])\n",
      "\t Grad: tensor([-0.0037,  0.0213])\n",
      "Epoch 2908, Loss 2.929011583328247\n",
      "\t Params: tensor([  5.3457, -17.1801])\n",
      "\t Grad: tensor([-0.0037,  0.0212])\n",
      "Epoch 2909, Loss 2.929007053375244\n",
      "\t Params: tensor([  5.3457, -17.1803])\n",
      "\t Grad: tensor([-0.0037,  0.0212])\n",
      "Epoch 2910, Loss 2.9290032386779785\n",
      "\t Params: tensor([  5.3458, -17.1805])\n",
      "\t Grad: tensor([-0.0037,  0.0211])\n",
      "Epoch 2911, Loss 2.928999423980713\n",
      "\t Params: tensor([  5.3458, -17.1807])\n",
      "\t Grad: tensor([-0.0037,  0.0211])\n",
      "Epoch 2912, Loss 2.9289934635162354\n",
      "\t Params: tensor([  5.3458, -17.1809])\n",
      "\t Grad: tensor([-0.0037,  0.0211])\n",
      "Epoch 2913, Loss 2.9289894104003906\n",
      "\t Params: tensor([  5.3459, -17.1812])\n",
      "\t Grad: tensor([-0.0037,  0.0210])\n",
      "Epoch 2914, Loss 2.9289846420288086\n",
      "\t Params: tensor([  5.3459, -17.1814])\n",
      "\t Grad: tensor([-0.0037,  0.0210])\n",
      "Epoch 2915, Loss 2.9289801120758057\n",
      "\t Params: tensor([  5.3460, -17.1816])\n",
      "\t Grad: tensor([-0.0037,  0.0210])\n",
      "Epoch 2916, Loss 2.92897629737854\n",
      "\t Params: tensor([  5.3460, -17.1818])\n",
      "\t Grad: tensor([-0.0037,  0.0209])\n",
      "Epoch 2917, Loss 2.9289710521698\n",
      "\t Params: tensor([  5.3460, -17.1820])\n",
      "\t Grad: tensor([-0.0037,  0.0209])\n",
      "Epoch 2918, Loss 2.9289674758911133\n",
      "\t Params: tensor([  5.3461, -17.1822])\n",
      "\t Grad: tensor([-0.0037,  0.0209])\n",
      "Epoch 2919, Loss 2.928961992263794\n",
      "\t Params: tensor([  5.3461, -17.1824])\n",
      "\t Grad: tensor([-0.0037,  0.0208])\n",
      "Epoch 2920, Loss 2.928957939147949\n",
      "\t Params: tensor([  5.3461, -17.1826])\n",
      "\t Grad: tensor([-0.0037,  0.0208])\n",
      "Epoch 2921, Loss 2.9289534091949463\n",
      "\t Params: tensor([  5.3462, -17.1828])\n",
      "\t Grad: tensor([-0.0037,  0.0208])\n",
      "Epoch 2922, Loss 2.9289474487304688\n",
      "\t Params: tensor([  5.3462, -17.1830])\n",
      "\t Grad: tensor([-0.0036,  0.0207])\n",
      "Epoch 2923, Loss 2.928943395614624\n",
      "\t Params: tensor([  5.3462, -17.1832])\n",
      "\t Grad: tensor([-0.0037,  0.0207])\n",
      "Epoch 2924, Loss 2.9289400577545166\n",
      "\t Params: tensor([  5.3463, -17.1834])\n",
      "\t Grad: tensor([-0.0036,  0.0206])\n",
      "Epoch 2925, Loss 2.9289352893829346\n",
      "\t Params: tensor([  5.3463, -17.1837])\n",
      "\t Grad: tensor([-0.0036,  0.0206])\n",
      "Epoch 2926, Loss 2.928931951522827\n",
      "\t Params: tensor([  5.3464, -17.1839])\n",
      "\t Grad: tensor([-0.0036,  0.0206])\n",
      "Epoch 2927, Loss 2.9289262294769287\n",
      "\t Params: tensor([  5.3464, -17.1841])\n",
      "\t Grad: tensor([-0.0036,  0.0205])\n",
      "Epoch 2928, Loss 2.9289231300354004\n",
      "\t Params: tensor([  5.3464, -17.1843])\n",
      "\t Grad: tensor([-0.0036,  0.0205])\n",
      "Epoch 2929, Loss 2.9289186000823975\n",
      "\t Params: tensor([  5.3465, -17.1845])\n",
      "\t Grad: tensor([-0.0036,  0.0205])\n",
      "Epoch 2930, Loss 2.928913116455078\n",
      "\t Params: tensor([  5.3465, -17.1847])\n",
      "\t Grad: tensor([-0.0036,  0.0204])\n",
      "Epoch 2931, Loss 2.9289093017578125\n",
      "\t Params: tensor([  5.3465, -17.1849])\n",
      "\t Grad: tensor([-0.0036,  0.0204])\n",
      "Epoch 2932, Loss 2.9289040565490723\n",
      "\t Params: tensor([  5.3466, -17.1851])\n",
      "\t Grad: tensor([-0.0036,  0.0204])\n",
      "Epoch 2933, Loss 2.9289016723632812\n",
      "\t Params: tensor([  5.3466, -17.1853])\n",
      "\t Grad: tensor([-0.0036,  0.0203])\n",
      "Epoch 2934, Loss 2.928896903991699\n",
      "\t Params: tensor([  5.3466, -17.1855])\n",
      "\t Grad: tensor([-0.0036,  0.0203])\n",
      "Epoch 2935, Loss 2.9288930892944336\n",
      "\t Params: tensor([  5.3467, -17.1857])\n",
      "\t Grad: tensor([-0.0036,  0.0203])\n",
      "Epoch 2936, Loss 2.928887367248535\n",
      "\t Params: tensor([  5.3467, -17.1859])\n",
      "\t Grad: tensor([-0.0036,  0.0202])\n",
      "Epoch 2937, Loss 2.9288833141326904\n",
      "\t Params: tensor([  5.3468, -17.1861])\n",
      "\t Grad: tensor([-0.0035,  0.0202])\n",
      "Epoch 2938, Loss 2.928880214691162\n",
      "\t Params: tensor([  5.3468, -17.1863])\n",
      "\t Grad: tensor([-0.0036,  0.0202])\n",
      "Epoch 2939, Loss 2.928877830505371\n",
      "\t Params: tensor([  5.3468, -17.1865])\n",
      "\t Grad: tensor([-0.0036,  0.0201])\n",
      "Epoch 2940, Loss 2.9288711547851562\n",
      "\t Params: tensor([  5.3469, -17.1867])\n",
      "\t Grad: tensor([-0.0035,  0.0201])\n",
      "Epoch 2941, Loss 2.9288671016693115\n",
      "\t Params: tensor([  5.3469, -17.1869])\n",
      "\t Grad: tensor([-0.0035,  0.0201])\n",
      "Epoch 2942, Loss 2.928863763809204\n",
      "\t Params: tensor([  5.3469, -17.1871])\n",
      "\t Grad: tensor([-0.0035,  0.0200])\n",
      "Epoch 2943, Loss 2.9288597106933594\n",
      "\t Params: tensor([  5.3470, -17.1873])\n",
      "\t Grad: tensor([-0.0035,  0.0200])\n",
      "Epoch 2944, Loss 2.9288551807403564\n",
      "\t Params: tensor([  5.3470, -17.1875])\n",
      "\t Grad: tensor([-0.0035,  0.0200])\n",
      "Epoch 2945, Loss 2.9288504123687744\n",
      "\t Params: tensor([  5.3470, -17.1877])\n",
      "\t Grad: tensor([-0.0035,  0.0199])\n",
      "Epoch 2946, Loss 2.9288454055786133\n",
      "\t Params: tensor([  5.3471, -17.1879])\n",
      "\t Grad: tensor([-0.0035,  0.0199])\n",
      "Epoch 2947, Loss 2.928842782974243\n",
      "\t Params: tensor([  5.3471, -17.1881])\n",
      "\t Grad: tensor([-0.0035,  0.0199])\n",
      "Epoch 2948, Loss 2.9288382530212402\n",
      "\t Params: tensor([  5.3471, -17.1883])\n",
      "\t Grad: tensor([-0.0035,  0.0198])\n",
      "Epoch 2949, Loss 2.9288330078125\n",
      "\t Params: tensor([  5.3472, -17.1885])\n",
      "\t Grad: tensor([-0.0035,  0.0198])\n",
      "Epoch 2950, Loss 2.928830146789551\n",
      "\t Params: tensor([  5.3472, -17.1887])\n",
      "\t Grad: tensor([-0.0035,  0.0198])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2951, Loss 2.928826093673706\n",
      "\t Params: tensor([  5.3472, -17.1889])\n",
      "\t Grad: tensor([-0.0035,  0.0197])\n",
      "Epoch 2952, Loss 2.9288225173950195\n",
      "\t Params: tensor([  5.3473, -17.1891])\n",
      "\t Grad: tensor([-0.0035,  0.0197])\n",
      "Epoch 2953, Loss 2.9288175106048584\n",
      "\t Params: tensor([  5.3473, -17.1893])\n",
      "\t Grad: tensor([-0.0035,  0.0197])\n",
      "Epoch 2954, Loss 2.9288156032562256\n",
      "\t Params: tensor([  5.3474, -17.1895])\n",
      "\t Grad: tensor([-0.0035,  0.0196])\n",
      "Epoch 2955, Loss 2.9288108348846436\n",
      "\t Params: tensor([  5.3474, -17.1897])\n",
      "\t Grad: tensor([-0.0035,  0.0196])\n",
      "Epoch 2956, Loss 2.928804636001587\n",
      "\t Params: tensor([  5.3474, -17.1899])\n",
      "\t Grad: tensor([-0.0034,  0.0196])\n",
      "Epoch 2957, Loss 2.9288015365600586\n",
      "\t Params: tensor([  5.3475, -17.1901])\n",
      "\t Grad: tensor([-0.0035,  0.0195])\n",
      "Epoch 2958, Loss 2.9287989139556885\n",
      "\t Params: tensor([  5.3475, -17.1903])\n",
      "\t Grad: tensor([-0.0034,  0.0195])\n",
      "Epoch 2959, Loss 2.9287948608398438\n",
      "\t Params: tensor([  5.3475, -17.1905])\n",
      "\t Grad: tensor([-0.0034,  0.0195])\n",
      "Epoch 2960, Loss 2.9287893772125244\n",
      "\t Params: tensor([  5.3476, -17.1907])\n",
      "\t Grad: tensor([-0.0034,  0.0194])\n",
      "Epoch 2961, Loss 2.928788661956787\n",
      "\t Params: tensor([  5.3476, -17.1908])\n",
      "\t Grad: tensor([-0.0034,  0.0194])\n",
      "Epoch 2962, Loss 2.9287827014923096\n",
      "\t Params: tensor([  5.3476, -17.1910])\n",
      "\t Grad: tensor([-0.0034,  0.0194])\n",
      "Epoch 2963, Loss 2.928778648376465\n",
      "\t Params: tensor([  5.3477, -17.1912])\n",
      "\t Grad: tensor([-0.0034,  0.0193])\n",
      "Epoch 2964, Loss 2.928774833679199\n",
      "\t Params: tensor([  5.3477, -17.1914])\n",
      "\t Grad: tensor([-0.0034,  0.0193])\n",
      "Epoch 2965, Loss 2.9287710189819336\n",
      "\t Params: tensor([  5.3477, -17.1916])\n",
      "\t Grad: tensor([-0.0034,  0.0193])\n",
      "Epoch 2966, Loss 2.9287667274475098\n",
      "\t Params: tensor([  5.3478, -17.1918])\n",
      "\t Grad: tensor([-0.0034,  0.0192])\n",
      "Epoch 2967, Loss 2.928764581680298\n",
      "\t Params: tensor([  5.3478, -17.1920])\n",
      "\t Grad: tensor([-0.0034,  0.0192])\n",
      "Epoch 2968, Loss 2.928760528564453\n",
      "\t Params: tensor([  5.3478, -17.1922])\n",
      "\t Grad: tensor([-0.0034,  0.0192])\n",
      "Epoch 2969, Loss 2.928757667541504\n",
      "\t Params: tensor([  5.3479, -17.1924])\n",
      "\t Grad: tensor([-0.0034,  0.0191])\n",
      "Epoch 2970, Loss 2.9287521839141846\n",
      "\t Params: tensor([  5.3479, -17.1926])\n",
      "\t Grad: tensor([-0.0034,  0.0191])\n",
      "Epoch 2971, Loss 2.9287497997283936\n",
      "\t Params: tensor([  5.3479, -17.1928])\n",
      "\t Grad: tensor([-0.0034,  0.0191])\n",
      "Epoch 2972, Loss 2.9287452697753906\n",
      "\t Params: tensor([  5.3480, -17.1930])\n",
      "\t Grad: tensor([-0.0034,  0.0190])\n",
      "Epoch 2973, Loss 2.928741216659546\n",
      "\t Params: tensor([  5.3480, -17.1931])\n",
      "\t Grad: tensor([-0.0034,  0.0190])\n",
      "Epoch 2974, Loss 2.9287374019622803\n",
      "\t Params: tensor([  5.3480, -17.1933])\n",
      "\t Grad: tensor([-0.0034,  0.0190])\n",
      "Epoch 2975, Loss 2.92873477935791\n",
      "\t Params: tensor([  5.3481, -17.1935])\n",
      "\t Grad: tensor([-0.0033,  0.0189])\n",
      "Epoch 2976, Loss 2.928729772567749\n",
      "\t Params: tensor([  5.3481, -17.1937])\n",
      "\t Grad: tensor([-0.0033,  0.0189])\n",
      "Epoch 2977, Loss 2.928727149963379\n",
      "\t Params: tensor([  5.3481, -17.1939])\n",
      "\t Grad: tensor([-0.0033,  0.0189])\n",
      "Epoch 2978, Loss 2.928722620010376\n",
      "\t Params: tensor([  5.3482, -17.1941])\n",
      "\t Grad: tensor([-0.0033,  0.0188])\n",
      "Epoch 2979, Loss 2.9287185668945312\n",
      "\t Params: tensor([  5.3482, -17.1943])\n",
      "\t Grad: tensor([-0.0033,  0.0188])\n",
      "Epoch 2980, Loss 2.928715705871582\n",
      "\t Params: tensor([  5.3482, -17.1945])\n",
      "\t Grad: tensor([-0.0033,  0.0188])\n",
      "Epoch 2981, Loss 2.9287118911743164\n",
      "\t Params: tensor([  5.3483, -17.1947])\n",
      "\t Grad: tensor([-0.0033,  0.0187])\n",
      "Epoch 2982, Loss 2.9287078380584717\n",
      "\t Params: tensor([  5.3483, -17.1948])\n",
      "\t Grad: tensor([-0.0033,  0.0187])\n",
      "Epoch 2983, Loss 2.9287049770355225\n",
      "\t Params: tensor([  5.3483, -17.1950])\n",
      "\t Grad: tensor([-0.0033,  0.0187])\n",
      "Epoch 2984, Loss 2.9286997318267822\n",
      "\t Params: tensor([  5.3484, -17.1952])\n",
      "\t Grad: tensor([-0.0033,  0.0186])\n",
      "Epoch 2985, Loss 2.9286980628967285\n",
      "\t Params: tensor([  5.3484, -17.1954])\n",
      "\t Grad: tensor([-0.0033,  0.0186])\n",
      "Epoch 2986, Loss 2.9286949634552\n",
      "\t Params: tensor([  5.3484, -17.1956])\n",
      "\t Grad: tensor([-0.0033,  0.0186])\n",
      "Epoch 2987, Loss 2.928690195083618\n",
      "\t Params: tensor([  5.3485, -17.1958])\n",
      "\t Grad: tensor([-0.0033,  0.0186])\n",
      "Epoch 2988, Loss 2.928687334060669\n",
      "\t Params: tensor([  5.3485, -17.1960])\n",
      "\t Grad: tensor([-0.0033,  0.0185])\n",
      "Epoch 2989, Loss 2.9286839962005615\n",
      "\t Params: tensor([  5.3485, -17.1961])\n",
      "\t Grad: tensor([-0.0033,  0.0185])\n",
      "Epoch 2990, Loss 2.9286789894104004\n",
      "\t Params: tensor([  5.3486, -17.1963])\n",
      "\t Grad: tensor([-0.0032,  0.0185])\n",
      "Epoch 2991, Loss 2.9286773204803467\n",
      "\t Params: tensor([  5.3486, -17.1965])\n",
      "\t Grad: tensor([-0.0033,  0.0184])\n",
      "Epoch 2992, Loss 2.9286725521087646\n",
      "\t Params: tensor([  5.3486, -17.1967])\n",
      "\t Grad: tensor([-0.0033,  0.0184])\n",
      "Epoch 2993, Loss 2.9286692142486572\n",
      "\t Params: tensor([  5.3487, -17.1969])\n",
      "\t Grad: tensor([-0.0033,  0.0184])\n",
      "Epoch 2994, Loss 2.92866587638855\n",
      "\t Params: tensor([  5.3487, -17.1971])\n",
      "\t Grad: tensor([-0.0032,  0.0183])\n",
      "Epoch 2995, Loss 2.9286623001098633\n",
      "\t Params: tensor([  5.3487, -17.1972])\n",
      "\t Grad: tensor([-0.0032,  0.0183])\n",
      "Epoch 2996, Loss 2.928659677505493\n",
      "\t Params: tensor([  5.3488, -17.1974])\n",
      "\t Grad: tensor([-0.0032,  0.0183])\n",
      "Epoch 2997, Loss 2.9286558628082275\n",
      "\t Params: tensor([  5.3488, -17.1976])\n",
      "\t Grad: tensor([-0.0032,  0.0182])\n",
      "Epoch 2998, Loss 2.9286506175994873\n",
      "\t Params: tensor([  5.3488, -17.1978])\n",
      "\t Grad: tensor([-0.0032,  0.0182])\n",
      "Epoch 2999, Loss 2.9286484718322754\n",
      "\t Params: tensor([  5.3489, -17.1980])\n",
      "\t Grad: tensor([-0.0032,  0.0182])\n",
      "Epoch 3000, Loss 2.9286458492279053\n",
      "\t Params: tensor([  5.3489, -17.1982])\n",
      "\t Grad: tensor([-0.0032,  0.0181])\n",
      "Epoch 3001, Loss 2.928643226623535\n",
      "\t Params: tensor([  5.3489, -17.1983])\n",
      "\t Grad: tensor([-0.0032,  0.0181])\n",
      "Epoch 3002, Loss 2.928637742996216\n",
      "\t Params: tensor([  5.3489, -17.1985])\n",
      "\t Grad: tensor([-0.0032,  0.0181])\n",
      "Epoch 3003, Loss 2.928635358810425\n",
      "\t Params: tensor([  5.3490, -17.1987])\n",
      "\t Grad: tensor([-0.0032,  0.0181])\n",
      "Epoch 3004, Loss 2.9286317825317383\n",
      "\t Params: tensor([  5.3490, -17.1989])\n",
      "\t Grad: tensor([-0.0032,  0.0180])\n",
      "Epoch 3005, Loss 2.92862868309021\n",
      "\t Params: tensor([  5.3490, -17.1991])\n",
      "\t Grad: tensor([-0.0032,  0.0180])\n",
      "Epoch 3006, Loss 2.9286253452301025\n",
      "\t Params: tensor([  5.3491, -17.1992])\n",
      "\t Grad: tensor([-0.0032,  0.0180])\n",
      "Epoch 3007, Loss 2.9286210536956787\n",
      "\t Params: tensor([  5.3491, -17.1994])\n",
      "\t Grad: tensor([-0.0032,  0.0179])\n",
      "Epoch 3008, Loss 2.928617000579834\n",
      "\t Params: tensor([  5.3491, -17.1996])\n",
      "\t Grad: tensor([-0.0032,  0.0179])\n",
      "Epoch 3009, Loss 2.9286158084869385\n",
      "\t Params: tensor([  5.3492, -17.1998])\n",
      "\t Grad: tensor([-0.0032,  0.0179])\n",
      "Epoch 3010, Loss 2.928611993789673\n",
      "\t Params: tensor([  5.3492, -17.2000])\n",
      "\t Grad: tensor([-0.0032,  0.0178])\n",
      "Epoch 3011, Loss 2.928607940673828\n",
      "\t Params: tensor([  5.3492, -17.2001])\n",
      "\t Grad: tensor([-0.0032,  0.0178])\n",
      "Epoch 3012, Loss 2.9286043643951416\n",
      "\t Params: tensor([  5.3493, -17.2003])\n",
      "\t Grad: tensor([-0.0031,  0.0178])\n",
      "Epoch 3013, Loss 2.9286012649536133\n",
      "\t Params: tensor([  5.3493, -17.2005])\n",
      "\t Grad: tensor([-0.0031,  0.0177])\n",
      "Epoch 3014, Loss 2.928598642349243\n",
      "\t Params: tensor([  5.3493, -17.2007])\n",
      "\t Grad: tensor([-0.0031,  0.0177])\n",
      "Epoch 3015, Loss 2.928595542907715\n",
      "\t Params: tensor([  5.3494, -17.2008])\n",
      "\t Grad: tensor([-0.0031,  0.0177])\n",
      "Epoch 3016, Loss 2.928591728210449\n",
      "\t Params: tensor([  5.3494, -17.2010])\n",
      "\t Grad: tensor([-0.0031,  0.0177])\n",
      "Epoch 3017, Loss 2.9285881519317627\n",
      "\t Params: tensor([  5.3494, -17.2012])\n",
      "\t Grad: tensor([-0.0031,  0.0176])\n",
      "Epoch 3018, Loss 2.928586483001709\n",
      "\t Params: tensor([  5.3495, -17.2014])\n",
      "\t Grad: tensor([-0.0031,  0.0176])\n",
      "Epoch 3019, Loss 2.9285826683044434\n",
      "\t Params: tensor([  5.3495, -17.2015])\n",
      "\t Grad: tensor([-0.0031,  0.0176])\n",
      "Epoch 3020, Loss 2.9285802841186523\n",
      "\t Params: tensor([  5.3495, -17.2017])\n",
      "\t Grad: tensor([-0.0031,  0.0175])\n",
      "Epoch 3021, Loss 2.9285759925842285\n",
      "\t Params: tensor([  5.3495, -17.2019])\n",
      "\t Grad: tensor([-0.0031,  0.0175])\n",
      "Epoch 3022, Loss 2.928574323654175\n",
      "\t Params: tensor([  5.3496, -17.2021])\n",
      "\t Grad: tensor([-0.0031,  0.0175])\n",
      "Epoch 3023, Loss 2.9285695552825928\n",
      "\t Params: tensor([  5.3496, -17.2022])\n",
      "\t Grad: tensor([-0.0031,  0.0175])\n",
      "Epoch 3024, Loss 2.928567409515381\n",
      "\t Params: tensor([  5.3496, -17.2024])\n",
      "\t Grad: tensor([-0.0031,  0.0174])\n",
      "Epoch 3025, Loss 2.9285638332366943\n",
      "\t Params: tensor([  5.3497, -17.2026])\n",
      "\t Grad: tensor([-0.0031,  0.0174])\n",
      "Epoch 3026, Loss 2.928560733795166\n",
      "\t Params: tensor([  5.3497, -17.2028])\n",
      "\t Grad: tensor([-0.0030,  0.0174])\n",
      "Epoch 3027, Loss 2.9285566806793213\n",
      "\t Params: tensor([  5.3497, -17.2029])\n",
      "\t Grad: tensor([-0.0031,  0.0173])\n",
      "Epoch 3028, Loss 2.9285547733306885\n",
      "\t Params: tensor([  5.3498, -17.2031])\n",
      "\t Grad: tensor([-0.0031,  0.0173])\n",
      "Epoch 3029, Loss 2.9285507202148438\n",
      "\t Params: tensor([  5.3498, -17.2033])\n",
      "\t Grad: tensor([-0.0031,  0.0173])\n",
      "Epoch 3030, Loss 2.9285478591918945\n",
      "\t Params: tensor([  5.3498, -17.2035])\n",
      "\t Grad: tensor([-0.0031,  0.0172])\n",
      "Epoch 3031, Loss 2.9285454750061035\n",
      "\t Params: tensor([  5.3498, -17.2036])\n",
      "\t Grad: tensor([-0.0030,  0.0172])\n",
      "Epoch 3032, Loss 2.9285430908203125\n",
      "\t Params: tensor([  5.3499, -17.2038])\n",
      "\t Grad: tensor([-0.0030,  0.0172])\n",
      "Epoch 3033, Loss 2.9285390377044678\n",
      "\t Params: tensor([  5.3499, -17.2040])\n",
      "\t Grad: tensor([-0.0030,  0.0172])\n",
      "Epoch 3034, Loss 2.9285356998443604\n",
      "\t Params: tensor([  5.3499, -17.2041])\n",
      "\t Grad: tensor([-0.0030,  0.0171])\n",
      "Epoch 3035, Loss 2.928532361984253\n",
      "\t Params: tensor([  5.3500, -17.2043])\n",
      "\t Grad: tensor([-0.0030,  0.0171])\n",
      "Epoch 3036, Loss 2.9285309314727783\n",
      "\t Params: tensor([  5.3500, -17.2045])\n",
      "\t Grad: tensor([-0.0030,  0.0171])\n",
      "Epoch 3037, Loss 2.92852783203125\n",
      "\t Params: tensor([  5.3500, -17.2047])\n",
      "\t Grad: tensor([-0.0030,  0.0170])\n",
      "Epoch 3038, Loss 2.9285240173339844\n",
      "\t Params: tensor([  5.3501, -17.2048])\n",
      "\t Grad: tensor([-0.0030,  0.0170])\n",
      "Epoch 3039, Loss 2.928521156311035\n",
      "\t Params: tensor([  5.3501, -17.2050])\n",
      "\t Grad: tensor([-0.0030,  0.0170])\n",
      "Epoch 3040, Loss 2.928518533706665\n",
      "\t Params: tensor([  5.3501, -17.2052])\n",
      "\t Grad: tensor([-0.0030,  0.0170])\n",
      "Epoch 3041, Loss 2.928514242172241\n",
      "\t Params: tensor([  5.3502, -17.2053])\n",
      "\t Grad: tensor([-0.0030,  0.0169])\n",
      "Epoch 3042, Loss 2.928511619567871\n",
      "\t Params: tensor([  5.3502, -17.2055])\n",
      "\t Grad: tensor([-0.0030,  0.0169])\n",
      "Epoch 3043, Loss 2.928508758544922\n",
      "\t Params: tensor([  5.3502, -17.2057])\n",
      "\t Grad: tensor([-0.0030,  0.0169])\n",
      "Epoch 3044, Loss 2.9285051822662354\n",
      "\t Params: tensor([  5.3502, -17.2058])\n",
      "\t Grad: tensor([-0.0030,  0.0168])\n",
      "Epoch 3045, Loss 2.9285032749176025\n",
      "\t Params: tensor([  5.3503, -17.2060])\n",
      "\t Grad: tensor([-0.0030,  0.0168])\n",
      "Epoch 3046, Loss 2.928500175476074\n",
      "\t Params: tensor([  5.3503, -17.2062])\n",
      "\t Grad: tensor([-0.0030,  0.0168])\n",
      "Epoch 3047, Loss 2.928497552871704\n",
      "\t Params: tensor([  5.3503, -17.2063])\n",
      "\t Grad: tensor([-0.0030,  0.0168])\n",
      "Epoch 3048, Loss 2.928495168685913\n",
      "\t Params: tensor([  5.3504, -17.2065])\n",
      "\t Grad: tensor([-0.0030,  0.0167])\n",
      "Epoch 3049, Loss 2.92849063873291\n",
      "\t Params: tensor([  5.3504, -17.2067])\n",
      "\t Grad: tensor([-0.0030,  0.0167])\n",
      "Epoch 3050, Loss 2.9284887313842773\n",
      "\t Params: tensor([  5.3504, -17.2068])\n",
      "\t Grad: tensor([-0.0030,  0.0167])\n",
      "Epoch 3051, Loss 2.9284861087799072\n",
      "\t Params: tensor([  5.3504, -17.2070])\n",
      "\t Grad: tensor([-0.0029,  0.0166])\n",
      "Epoch 3052, Loss 2.9284844398498535\n",
      "\t Params: tensor([  5.3505, -17.2072])\n",
      "\t Grad: tensor([-0.0029,  0.0166])\n",
      "Epoch 3053, Loss 2.928480625152588\n",
      "\t Params: tensor([  5.3505, -17.2073])\n",
      "\t Grad: tensor([-0.0029,  0.0166])\n",
      "Epoch 3054, Loss 2.9284772872924805\n",
      "\t Params: tensor([  5.3505, -17.2075])\n",
      "\t Grad: tensor([-0.0029,  0.0165])\n",
      "Epoch 3055, Loss 2.9284744262695312\n",
      "\t Params: tensor([  5.3506, -17.2077])\n",
      "\t Grad: tensor([-0.0029,  0.0165])\n",
      "Epoch 3056, Loss 2.9284722805023193\n",
      "\t Params: tensor([  5.3506, -17.2078])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad: tensor([-0.0029,  0.0165])\n",
      "Epoch 3057, Loss 2.928469181060791\n",
      "\t Params: tensor([  5.3506, -17.2080])\n",
      "\t Grad: tensor([-0.0029,  0.0165])\n",
      "Epoch 3058, Loss 2.9284677505493164\n",
      "\t Params: tensor([  5.3507, -17.2082])\n",
      "\t Grad: tensor([-0.0029,  0.0164])\n",
      "Epoch 3059, Loss 2.9284627437591553\n",
      "\t Params: tensor([  5.3507, -17.2083])\n",
      "\t Grad: tensor([-0.0029,  0.0164])\n",
      "Epoch 3060, Loss 2.928460121154785\n",
      "\t Params: tensor([  5.3507, -17.2085])\n",
      "\t Grad: tensor([-0.0029,  0.0164])\n",
      "Epoch 3061, Loss 2.9284582138061523\n",
      "\t Params: tensor([  5.3507, -17.2087])\n",
      "\t Grad: tensor([-0.0029,  0.0164])\n",
      "Epoch 3062, Loss 2.9284555912017822\n",
      "\t Params: tensor([  5.3508, -17.2088])\n",
      "\t Grad: tensor([-0.0029,  0.0163])\n",
      "Epoch 3063, Loss 2.928452491760254\n",
      "\t Params: tensor([  5.3508, -17.2090])\n",
      "\t Grad: tensor([-0.0029,  0.0163])\n",
      "Epoch 3064, Loss 2.9284493923187256\n",
      "\t Params: tensor([  5.3508, -17.2091])\n",
      "\t Grad: tensor([-0.0029,  0.0163])\n",
      "Epoch 3065, Loss 2.9284470081329346\n",
      "\t Params: tensor([  5.3509, -17.2093])\n",
      "\t Grad: tensor([-0.0029,  0.0162])\n",
      "Epoch 3066, Loss 2.928443193435669\n",
      "\t Params: tensor([  5.3509, -17.2095])\n",
      "\t Grad: tensor([-0.0029,  0.0162])\n",
      "Epoch 3067, Loss 2.928443670272827\n",
      "\t Params: tensor([  5.3509, -17.2096])\n",
      "\t Grad: tensor([-0.0029,  0.0162])\n",
      "Epoch 3068, Loss 2.9284400939941406\n",
      "\t Params: tensor([  5.3509, -17.2098])\n",
      "\t Grad: tensor([-0.0029,  0.0162])\n",
      "Epoch 3069, Loss 2.9284353256225586\n",
      "\t Params: tensor([  5.3510, -17.2100])\n",
      "\t Grad: tensor([-0.0029,  0.0161])\n",
      "Epoch 3070, Loss 2.9284353256225586\n",
      "\t Params: tensor([  5.3510, -17.2101])\n",
      "\t Grad: tensor([-0.0029,  0.0161])\n",
      "Epoch 3071, Loss 2.92842960357666\n",
      "\t Params: tensor([  5.3510, -17.2103])\n",
      "\t Grad: tensor([-0.0028,  0.0161])\n",
      "Epoch 3072, Loss 2.9284284114837646\n",
      "\t Params: tensor([  5.3511, -17.2104])\n",
      "\t Grad: tensor([-0.0028,  0.0161])\n",
      "Epoch 3073, Loss 2.9284262657165527\n",
      "\t Params: tensor([  5.3511, -17.2106])\n",
      "\t Grad: tensor([-0.0028,  0.0160])\n",
      "Epoch 3074, Loss 2.928422689437866\n",
      "\t Params: tensor([  5.3511, -17.2108])\n",
      "\t Grad: tensor([-0.0028,  0.0160])\n",
      "Epoch 3075, Loss 2.9284212589263916\n",
      "\t Params: tensor([  5.3511, -17.2109])\n",
      "\t Grad: tensor([-0.0028,  0.0160])\n",
      "Epoch 3076, Loss 2.928417205810547\n",
      "\t Params: tensor([  5.3512, -17.2111])\n",
      "\t Grad: tensor([-0.0028,  0.0159])\n",
      "Epoch 3077, Loss 2.928415536880493\n",
      "\t Params: tensor([  5.3512, -17.2112])\n",
      "\t Grad: tensor([-0.0028,  0.0159])\n",
      "Epoch 3078, Loss 2.928410530090332\n",
      "\t Params: tensor([  5.3512, -17.2114])\n",
      "\t Grad: tensor([-0.0028,  0.0159])\n",
      "Epoch 3079, Loss 2.928410291671753\n",
      "\t Params: tensor([  5.3512, -17.2116])\n",
      "\t Grad: tensor([-0.0028,  0.0159])\n",
      "Epoch 3080, Loss 2.9284071922302246\n",
      "\t Params: tensor([  5.3513, -17.2117])\n",
      "\t Grad: tensor([-0.0028,  0.0158])\n",
      "Epoch 3081, Loss 2.9284040927886963\n",
      "\t Params: tensor([  5.3513, -17.2119])\n",
      "\t Grad: tensor([-0.0028,  0.0158])\n",
      "Epoch 3082, Loss 2.9284021854400635\n",
      "\t Params: tensor([  5.3513, -17.2120])\n",
      "\t Grad: tensor([-0.0028,  0.0158])\n",
      "Epoch 3083, Loss 2.928398847579956\n",
      "\t Params: tensor([  5.3514, -17.2122])\n",
      "\t Grad: tensor([-0.0028,  0.0158])\n",
      "Epoch 3084, Loss 2.9283957481384277\n",
      "\t Params: tensor([  5.3514, -17.2123])\n",
      "\t Grad: tensor([-0.0028,  0.0157])\n",
      "Epoch 3085, Loss 2.9283952713012695\n",
      "\t Params: tensor([  5.3514, -17.2125])\n",
      "\t Grad: tensor([-0.0028,  0.0157])\n",
      "Epoch 3086, Loss 2.928391933441162\n",
      "\t Params: tensor([  5.3514, -17.2127])\n",
      "\t Grad: tensor([-0.0027,  0.0157])\n",
      "Epoch 3087, Loss 2.928388833999634\n",
      "\t Params: tensor([  5.3515, -17.2128])\n",
      "\t Grad: tensor([-0.0027,  0.0157])\n",
      "Epoch 3088, Loss 2.9283859729766846\n",
      "\t Params: tensor([  5.3515, -17.2130])\n",
      "\t Grad: tensor([-0.0027,  0.0156])\n",
      "Epoch 3089, Loss 2.9283828735351562\n",
      "\t Params: tensor([  5.3515, -17.2131])\n",
      "\t Grad: tensor([-0.0028,  0.0156])\n",
      "Epoch 3090, Loss 2.928382158279419\n",
      "\t Params: tensor([  5.3516, -17.2133])\n",
      "\t Grad: tensor([-0.0028,  0.0156])\n",
      "Epoch 3091, Loss 2.9283790588378906\n",
      "\t Params: tensor([  5.3516, -17.2134])\n",
      "\t Grad: tensor([-0.0027,  0.0155])\n",
      "Epoch 3092, Loss 2.928378105163574\n",
      "\t Params: tensor([  5.3516, -17.2136])\n",
      "\t Grad: tensor([-0.0027,  0.0155])\n",
      "Epoch 3093, Loss 2.928375005722046\n",
      "\t Params: tensor([  5.3516, -17.2137])\n",
      "\t Grad: tensor([-0.0027,  0.0155])\n",
      "Epoch 3094, Loss 2.928372383117676\n",
      "\t Params: tensor([  5.3517, -17.2139])\n",
      "\t Grad: tensor([-0.0027,  0.0155])\n",
      "Epoch 3095, Loss 2.928370475769043\n",
      "\t Params: tensor([  5.3517, -17.2141])\n",
      "\t Grad: tensor([-0.0027,  0.0154])\n",
      "Epoch 3096, Loss 2.928368330001831\n",
      "\t Params: tensor([  5.3517, -17.2142])\n",
      "\t Grad: tensor([-0.0027,  0.0154])\n",
      "Epoch 3097, Loss 2.9283640384674072\n",
      "\t Params: tensor([  5.3517, -17.2144])\n",
      "\t Grad: tensor([-0.0027,  0.0154])\n",
      "Epoch 3098, Loss 2.9283623695373535\n",
      "\t Params: tensor([  5.3518, -17.2145])\n",
      "\t Grad: tensor([-0.0027,  0.0154])\n",
      "Epoch 3099, Loss 2.928360939025879\n",
      "\t Params: tensor([  5.3518, -17.2147])\n",
      "\t Grad: tensor([-0.0027,  0.0153])\n",
      "Epoch 3100, Loss 2.928356409072876\n",
      "\t Params: tensor([  5.3518, -17.2148])\n",
      "\t Grad: tensor([-0.0027,  0.0153])\n",
      "Epoch 3101, Loss 2.9283552169799805\n",
      "\t Params: tensor([  5.3519, -17.2150])\n",
      "\t Grad: tensor([-0.0027,  0.0153])\n",
      "Epoch 3102, Loss 2.9283533096313477\n",
      "\t Params: tensor([  5.3519, -17.2151])\n",
      "\t Grad: tensor([-0.0027,  0.0153])\n",
      "Epoch 3103, Loss 2.9283487796783447\n",
      "\t Params: tensor([  5.3519, -17.2153])\n",
      "\t Grad: tensor([-0.0027,  0.0152])\n",
      "Epoch 3104, Loss 2.9283478260040283\n",
      "\t Params: tensor([  5.3519, -17.2154])\n",
      "\t Grad: tensor([-0.0027,  0.0152])\n",
      "Epoch 3105, Loss 2.928344964981079\n",
      "\t Params: tensor([  5.3520, -17.2156])\n",
      "\t Grad: tensor([-0.0027,  0.0152])\n",
      "Epoch 3106, Loss 2.9283432960510254\n",
      "\t Params: tensor([  5.3520, -17.2157])\n",
      "\t Grad: tensor([-0.0027,  0.0152])\n",
      "Epoch 3107, Loss 2.928339958190918\n",
      "\t Params: tensor([  5.3520, -17.2159])\n",
      "\t Grad: tensor([-0.0027,  0.0151])\n",
      "Epoch 3108, Loss 2.9283385276794434\n",
      "\t Params: tensor([  5.3520, -17.2160])\n",
      "\t Grad: tensor([-0.0027,  0.0151])\n",
      "Epoch 3109, Loss 2.9283370971679688\n",
      "\t Params: tensor([  5.3521, -17.2162])\n",
      "\t Grad: tensor([-0.0027,  0.0151])\n",
      "Epoch 3110, Loss 2.928333282470703\n",
      "\t Params: tensor([  5.3521, -17.2163])\n",
      "\t Grad: tensor([-0.0027,  0.0151])\n",
      "Epoch 3111, Loss 2.9283316135406494\n",
      "\t Params: tensor([  5.3521, -17.2165])\n",
      "\t Grad: tensor([-0.0027,  0.0150])\n",
      "Epoch 3112, Loss 2.928328037261963\n",
      "\t Params: tensor([  5.3521, -17.2166])\n",
      "\t Grad: tensor([-0.0026,  0.0150])\n",
      "Epoch 3113, Loss 2.9283287525177\n",
      "\t Params: tensor([  5.3522, -17.2168])\n",
      "\t Grad: tensor([-0.0027,  0.0150])\n",
      "Epoch 3114, Loss 2.928323984146118\n",
      "\t Params: tensor([  5.3522, -17.2169])\n",
      "\t Grad: tensor([-0.0026,  0.0149])\n",
      "Epoch 3115, Loss 2.9283227920532227\n",
      "\t Params: tensor([  5.3522, -17.2171])\n",
      "\t Grad: tensor([-0.0026,  0.0149])\n",
      "Epoch 3116, Loss 2.9283204078674316\n",
      "\t Params: tensor([  5.3523, -17.2172])\n",
      "\t Grad: tensor([-0.0026,  0.0149])\n",
      "Epoch 3117, Loss 2.928318738937378\n",
      "\t Params: tensor([  5.3523, -17.2174])\n",
      "\t Grad: tensor([-0.0026,  0.0149])\n",
      "Epoch 3118, Loss 2.9283149242401123\n",
      "\t Params: tensor([  5.3523, -17.2175])\n",
      "\t Grad: tensor([-0.0026,  0.0148])\n",
      "Epoch 3119, Loss 2.9283125400543213\n",
      "\t Params: tensor([  5.3523, -17.2177])\n",
      "\t Grad: tensor([-0.0026,  0.0148])\n",
      "Epoch 3120, Loss 2.9283103942871094\n",
      "\t Params: tensor([  5.3524, -17.2178])\n",
      "\t Grad: tensor([-0.0026,  0.0148])\n",
      "Epoch 3121, Loss 2.9283082485198975\n",
      "\t Params: tensor([  5.3524, -17.2180])\n",
      "\t Grad: tensor([-0.0026,  0.0148])\n",
      "Epoch 3122, Loss 2.9283058643341064\n",
      "\t Params: tensor([  5.3524, -17.2181])\n",
      "\t Grad: tensor([-0.0026,  0.0147])\n",
      "Epoch 3123, Loss 2.9283041954040527\n",
      "\t Params: tensor([  5.3524, -17.2183])\n",
      "\t Grad: tensor([-0.0026,  0.0147])\n",
      "Epoch 3124, Loss 2.928302526473999\n",
      "\t Params: tensor([  5.3525, -17.2184])\n",
      "\t Grad: tensor([-0.0026,  0.0147])\n",
      "Epoch 3125, Loss 2.9282991886138916\n",
      "\t Params: tensor([  5.3525, -17.2186])\n",
      "\t Grad: tensor([-0.0026,  0.0147])\n",
      "Epoch 3126, Loss 2.9282960891723633\n",
      "\t Params: tensor([  5.3525, -17.2187])\n",
      "\t Grad: tensor([-0.0026,  0.0146])\n",
      "Epoch 3127, Loss 2.928295135498047\n",
      "\t Params: tensor([  5.3525, -17.2189])\n",
      "\t Grad: tensor([-0.0026,  0.0146])\n",
      "Epoch 3128, Loss 2.928292751312256\n",
      "\t Params: tensor([  5.3526, -17.2190])\n",
      "\t Grad: tensor([-0.0026,  0.0146])\n",
      "Epoch 3129, Loss 2.9282913208007812\n",
      "\t Params: tensor([  5.3526, -17.2192])\n",
      "\t Grad: tensor([-0.0026,  0.0146])\n",
      "Epoch 3130, Loss 2.9282877445220947\n",
      "\t Params: tensor([  5.3526, -17.2193])\n",
      "\t Grad: tensor([-0.0026,  0.0145])\n",
      "Epoch 3131, Loss 2.9282867908477783\n",
      "\t Params: tensor([  5.3526, -17.2194])\n",
      "\t Grad: tensor([-0.0026,  0.0145])\n",
      "Epoch 3132, Loss 2.9282846450805664\n",
      "\t Params: tensor([  5.3527, -17.2196])\n",
      "\t Grad: tensor([-0.0025,  0.0145])\n",
      "Epoch 3133, Loss 2.9282822608947754\n",
      "\t Params: tensor([  5.3527, -17.2197])\n",
      "\t Grad: tensor([-0.0026,  0.0145])\n",
      "Epoch 3134, Loss 2.9282796382904053\n",
      "\t Params: tensor([  5.3527, -17.2199])\n",
      "\t Grad: tensor([-0.0026,  0.0144])\n",
      "Epoch 3135, Loss 2.928276300430298\n",
      "\t Params: tensor([  5.3527, -17.2200])\n",
      "\t Grad: tensor([-0.0025,  0.0144])\n",
      "Epoch 3136, Loss 2.9282753467559814\n",
      "\t Params: tensor([  5.3528, -17.2202])\n",
      "\t Grad: tensor([-0.0026,  0.0144])\n",
      "Epoch 3137, Loss 2.9282732009887695\n",
      "\t Params: tensor([  5.3528, -17.2203])\n",
      "\t Grad: tensor([-0.0025,  0.0144])\n",
      "Epoch 3138, Loss 2.9282712936401367\n",
      "\t Params: tensor([  5.3528, -17.2205])\n",
      "\t Grad: tensor([-0.0025,  0.0144])\n",
      "Epoch 3139, Loss 2.9282681941986084\n",
      "\t Params: tensor([  5.3528, -17.2206])\n",
      "\t Grad: tensor([-0.0025,  0.0143])\n",
      "Epoch 3140, Loss 2.928266763687134\n",
      "\t Params: tensor([  5.3529, -17.2207])\n",
      "\t Grad: tensor([-0.0025,  0.0143])\n",
      "Epoch 3141, Loss 2.9282643795013428\n",
      "\t Params: tensor([  5.3529, -17.2209])\n",
      "\t Grad: tensor([-0.0025,  0.0143])\n",
      "Epoch 3142, Loss 2.9282631874084473\n",
      "\t Params: tensor([  5.3529, -17.2210])\n",
      "\t Grad: tensor([-0.0025,  0.0143])\n",
      "Epoch 3143, Loss 2.928260087966919\n",
      "\t Params: tensor([  5.3529, -17.2212])\n",
      "\t Grad: tensor([-0.0025,  0.0142])\n",
      "Epoch 3144, Loss 2.9282593727111816\n",
      "\t Params: tensor([  5.3530, -17.2213])\n",
      "\t Grad: tensor([-0.0025,  0.0142])\n",
      "Epoch 3145, Loss 2.928256034851074\n",
      "\t Params: tensor([  5.3530, -17.2214])\n",
      "\t Grad: tensor([-0.0025,  0.0142])\n",
      "Epoch 3146, Loss 2.928255319595337\n",
      "\t Params: tensor([  5.3530, -17.2216])\n",
      "\t Grad: tensor([-0.0025,  0.0142])\n",
      "Epoch 3147, Loss 2.9282517433166504\n",
      "\t Params: tensor([  5.3530, -17.2217])\n",
      "\t Grad: tensor([-0.0025,  0.0141])\n",
      "Epoch 3148, Loss 2.928250312805176\n",
      "\t Params: tensor([  5.3531, -17.2219])\n",
      "\t Grad: tensor([-0.0025,  0.0141])\n",
      "Epoch 3149, Loss 2.928248643875122\n",
      "\t Params: tensor([  5.3531, -17.2220])\n",
      "\t Grad: tensor([-0.0025,  0.0141])\n",
      "Epoch 3150, Loss 2.9282455444335938\n",
      "\t Params: tensor([  5.3531, -17.2222])\n",
      "\t Grad: tensor([-0.0025,  0.0141])\n",
      "Epoch 3151, Loss 2.9282448291778564\n",
      "\t Params: tensor([  5.3531, -17.2223])\n",
      "\t Grad: tensor([-0.0025,  0.0140])\n",
      "Epoch 3152, Loss 2.928241729736328\n",
      "\t Params: tensor([  5.3532, -17.2224])\n",
      "\t Grad: tensor([-0.0025,  0.0140])\n",
      "Epoch 3153, Loss 2.928239345550537\n",
      "\t Params: tensor([  5.3532, -17.2226])\n",
      "\t Grad: tensor([-0.0025,  0.0140])\n",
      "Epoch 3154, Loss 2.928236484527588\n",
      "\t Params: tensor([  5.3532, -17.2227])\n",
      "\t Grad: tensor([-0.0025,  0.0140])\n",
      "Epoch 3155, Loss 2.928236246109009\n",
      "\t Params: tensor([  5.3532, -17.2229])\n",
      "\t Grad: tensor([-0.0024,  0.0139])\n",
      "Epoch 3156, Loss 2.9282326698303223\n",
      "\t Params: tensor([  5.3533, -17.2230])\n",
      "\t Grad: tensor([-0.0025,  0.0139])\n",
      "Epoch 3157, Loss 2.9282312393188477\n",
      "\t Params: tensor([  5.3533, -17.2231])\n",
      "\t Grad: tensor([-0.0024,  0.0139])\n",
      "Epoch 3158, Loss 2.928230047225952\n",
      "\t Params: tensor([  5.3533, -17.2233])\n",
      "\t Grad: tensor([-0.0025,  0.0139])\n",
      "Epoch 3159, Loss 2.928227424621582\n",
      "\t Params: tensor([  5.3533, -17.2234])\n",
      "\t Grad: tensor([-0.0024,  0.0138])\n",
      "Epoch 3160, Loss 2.9282257556915283\n",
      "\t Params: tensor([  5.3534, -17.2235])\n",
      "\t Grad: tensor([-0.0025,  0.0138])\n",
      "Epoch 3161, Loss 2.928224802017212\n",
      "\t Params: tensor([  5.3534, -17.2237])\n",
      "\t Grad: tensor([-0.0024,  0.0138])\n",
      "Epoch 3162, Loss 2.928222417831421\n",
      "\t Params: tensor([  5.3534, -17.2238])\n",
      "\t Grad: tensor([-0.0024,  0.0138])\n",
      "Epoch 3163, Loss 2.9282188415527344\n",
      "\t Params: tensor([  5.3534, -17.2240])\n",
      "\t Grad: tensor([-0.0024,  0.0138])\n",
      "Epoch 3164, Loss 2.928218126296997\n",
      "\t Params: tensor([  5.3535, -17.2241])\n",
      "\t Grad: tensor([-0.0024,  0.0137])\n",
      "Epoch 3165, Loss 2.928215980529785\n",
      "\t Params: tensor([  5.3535, -17.2242])\n",
      "\t Grad: tensor([-0.0024,  0.0137])\n",
      "Epoch 3166, Loss 2.9282150268554688\n",
      "\t Params: tensor([  5.3535, -17.2244])\n",
      "\t Grad: tensor([-0.0024,  0.0137])\n",
      "Epoch 3167, Loss 2.9282121658325195\n",
      "\t Params: tensor([  5.3535, -17.2245])\n",
      "\t Grad: tensor([-0.0024,  0.0137])\n",
      "Epoch 3168, Loss 2.928210973739624\n",
      "\t Params: tensor([  5.3536, -17.2246])\n",
      "\t Grad: tensor([-0.0024,  0.0136])\n",
      "Epoch 3169, Loss 2.928209066390991\n",
      "\t Params: tensor([  5.3536, -17.2248])\n",
      "\t Grad: tensor([-0.0024,  0.0136])\n",
      "Epoch 3170, Loss 2.928206443786621\n",
      "\t Params: tensor([  5.3536, -17.2249])\n",
      "\t Grad: tensor([-0.0024,  0.0136])\n",
      "Epoch 3171, Loss 2.9282050132751465\n",
      "\t Params: tensor([  5.3536, -17.2250])\n",
      "\t Grad: tensor([-0.0024,  0.0136])\n",
      "Epoch 3172, Loss 2.928203582763672\n",
      "\t Params: tensor([  5.3537, -17.2252])\n",
      "\t Grad: tensor([-0.0024,  0.0135])\n",
      "Epoch 3173, Loss 2.928201913833618\n",
      "\t Params: tensor([  5.3537, -17.2253])\n",
      "\t Grad: tensor([-0.0024,  0.0135])\n",
      "Epoch 3174, Loss 2.9282000064849854\n",
      "\t Params: tensor([  5.3537, -17.2255])\n",
      "\t Grad: tensor([-0.0024,  0.0135])\n",
      "Epoch 3175, Loss 2.9281961917877197\n",
      "\t Params: tensor([  5.3537, -17.2256])\n",
      "\t Grad: tensor([-0.0024,  0.0135])\n",
      "Epoch 3176, Loss 2.928194522857666\n",
      "\t Params: tensor([  5.3538, -17.2257])\n",
      "\t Grad: tensor([-0.0024,  0.0134])\n",
      "Epoch 3177, Loss 2.928194522857666\n",
      "\t Params: tensor([  5.3538, -17.2259])\n",
      "\t Grad: tensor([-0.0024,  0.0134])\n",
      "Epoch 3178, Loss 2.9281914234161377\n",
      "\t Params: tensor([  5.3538, -17.2260])\n",
      "\t Grad: tensor([-0.0024,  0.0134])\n",
      "Epoch 3179, Loss 2.928189992904663\n",
      "\t Params: tensor([  5.3538, -17.2261])\n",
      "\t Grad: tensor([-0.0024,  0.0134])\n",
      "Epoch 3180, Loss 2.9281883239746094\n",
      "\t Params: tensor([  5.3538, -17.2263])\n",
      "\t Grad: tensor([-0.0023,  0.0134])\n",
      "Epoch 3181, Loss 2.9281859397888184\n",
      "\t Params: tensor([  5.3539, -17.2264])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad: tensor([-0.0023,  0.0133])\n",
      "Epoch 3182, Loss 2.928184747695923\n",
      "\t Params: tensor([  5.3539, -17.2265])\n",
      "\t Grad: tensor([-0.0024,  0.0133])\n",
      "Epoch 3183, Loss 2.9281835556030273\n",
      "\t Params: tensor([  5.3539, -17.2267])\n",
      "\t Grad: tensor([-0.0023,  0.0133])\n",
      "Epoch 3184, Loss 2.9281821250915527\n",
      "\t Params: tensor([  5.3539, -17.2268])\n",
      "\t Grad: tensor([-0.0024,  0.0133])\n",
      "Epoch 3185, Loss 2.9281797409057617\n",
      "\t Params: tensor([  5.3540, -17.2269])\n",
      "\t Grad: tensor([-0.0024,  0.0132])\n",
      "Epoch 3186, Loss 2.928177833557129\n",
      "\t Params: tensor([  5.3540, -17.2271])\n",
      "\t Grad: tensor([-0.0023,  0.0132])\n",
      "Epoch 3187, Loss 2.928175210952759\n",
      "\t Params: tensor([  5.3540, -17.2272])\n",
      "\t Grad: tensor([-0.0023,  0.0132])\n",
      "Epoch 3188, Loss 2.9281723499298096\n",
      "\t Params: tensor([  5.3540, -17.2273])\n",
      "\t Grad: tensor([-0.0023,  0.0132])\n",
      "Epoch 3189, Loss 2.928170680999756\n",
      "\t Params: tensor([  5.3541, -17.2275])\n",
      "\t Grad: tensor([-0.0023,  0.0132])\n",
      "Epoch 3190, Loss 2.9281699657440186\n",
      "\t Params: tensor([  5.3541, -17.2276])\n",
      "\t Grad: tensor([-0.0023,  0.0131])\n",
      "Epoch 3191, Loss 2.9281692504882812\n",
      "\t Params: tensor([  5.3541, -17.2277])\n",
      "\t Grad: tensor([-0.0023,  0.0131])\n",
      "Epoch 3192, Loss 2.9281668663024902\n",
      "\t Params: tensor([  5.3541, -17.2278])\n",
      "\t Grad: tensor([-0.0023,  0.0131])\n",
      "Epoch 3193, Loss 2.928164005279541\n",
      "\t Params: tensor([  5.3542, -17.2280])\n",
      "\t Grad: tensor([-0.0023,  0.0131])\n",
      "Epoch 3194, Loss 2.9281632900238037\n",
      "\t Params: tensor([  5.3542, -17.2281])\n",
      "\t Grad: tensor([-0.0023,  0.0130])\n",
      "Epoch 3195, Loss 2.92816162109375\n",
      "\t Params: tensor([  5.3542, -17.2282])\n",
      "\t Grad: tensor([-0.0023,  0.0130])\n",
      "Epoch 3196, Loss 2.9281599521636963\n",
      "\t Params: tensor([  5.3542, -17.2284])\n",
      "\t Grad: tensor([-0.0023,  0.0130])\n",
      "Epoch 3197, Loss 2.9281575679779053\n",
      "\t Params: tensor([  5.3542, -17.2285])\n",
      "\t Grad: tensor([-0.0023,  0.0130])\n",
      "Epoch 3198, Loss 2.928157091140747\n",
      "\t Params: tensor([  5.3543, -17.2286])\n",
      "\t Grad: tensor([-0.0023,  0.0130])\n",
      "Epoch 3199, Loss 2.9281539916992188\n",
      "\t Params: tensor([  5.3543, -17.2288])\n",
      "\t Grad: tensor([-0.0023,  0.0129])\n",
      "Epoch 3200, Loss 2.9281516075134277\n",
      "\t Params: tensor([  5.3543, -17.2289])\n",
      "\t Grad: tensor([-0.0023,  0.0129])\n",
      "Epoch 3201, Loss 2.9281492233276367\n",
      "\t Params: tensor([  5.3543, -17.2290])\n",
      "\t Grad: tensor([-0.0023,  0.0129])\n",
      "Epoch 3202, Loss 2.928149938583374\n",
      "\t Params: tensor([  5.3544, -17.2291])\n",
      "\t Grad: tensor([-0.0023,  0.0129])\n",
      "Epoch 3203, Loss 2.928147077560425\n",
      "\t Params: tensor([  5.3544, -17.2293])\n",
      "\t Grad: tensor([-0.0022,  0.0129])\n",
      "Epoch 3204, Loss 2.92814564704895\n",
      "\t Params: tensor([  5.3544, -17.2294])\n",
      "\t Grad: tensor([-0.0023,  0.0128])\n",
      "Epoch 3205, Loss 2.9281439781188965\n",
      "\t Params: tensor([  5.3544, -17.2295])\n",
      "\t Grad: tensor([-0.0023,  0.0128])\n",
      "Epoch 3206, Loss 2.9281418323516846\n",
      "\t Params: tensor([  5.3544, -17.2297])\n",
      "\t Grad: tensor([-0.0023,  0.0128])\n",
      "Epoch 3207, Loss 2.928140163421631\n",
      "\t Params: tensor([  5.3545, -17.2298])\n",
      "\t Grad: tensor([-0.0022,  0.0128])\n",
      "Epoch 3208, Loss 2.928138494491577\n",
      "\t Params: tensor([  5.3545, -17.2299])\n",
      "\t Grad: tensor([-0.0022,  0.0127])\n",
      "Epoch 3209, Loss 2.9281365871429443\n",
      "\t Params: tensor([  5.3545, -17.2300])\n",
      "\t Grad: tensor([-0.0023,  0.0127])\n",
      "Epoch 3210, Loss 2.9281346797943115\n",
      "\t Params: tensor([  5.3545, -17.2302])\n",
      "\t Grad: tensor([-0.0023,  0.0127])\n",
      "Epoch 3211, Loss 2.9281349182128906\n",
      "\t Params: tensor([  5.3546, -17.2303])\n",
      "\t Grad: tensor([-0.0023,  0.0127])\n",
      "Epoch 3212, Loss 2.9281325340270996\n",
      "\t Params: tensor([  5.3546, -17.2304])\n",
      "\t Grad: tensor([-0.0022,  0.0127])\n",
      "Epoch 3213, Loss 2.928130865097046\n",
      "\t Params: tensor([  5.3546, -17.2305])\n",
      "\t Grad: tensor([-0.0022,  0.0126])\n",
      "Epoch 3214, Loss 2.9281301498413086\n",
      "\t Params: tensor([  5.3546, -17.2307])\n",
      "\t Grad: tensor([-0.0022,  0.0126])\n",
      "Epoch 3215, Loss 2.9281256198883057\n",
      "\t Params: tensor([  5.3546, -17.2308])\n",
      "\t Grad: tensor([-0.0022,  0.0126])\n",
      "Epoch 3216, Loss 2.9281249046325684\n",
      "\t Params: tensor([  5.3547, -17.2309])\n",
      "\t Grad: tensor([-0.0022,  0.0126])\n",
      "Epoch 3217, Loss 2.928124189376831\n",
      "\t Params: tensor([  5.3547, -17.2310])\n",
      "\t Grad: tensor([-0.0022,  0.0125])\n",
      "Epoch 3218, Loss 2.9281210899353027\n",
      "\t Params: tensor([  5.3547, -17.2312])\n",
      "\t Grad: tensor([-0.0022,  0.0125])\n",
      "Epoch 3219, Loss 2.9281210899353027\n",
      "\t Params: tensor([  5.3547, -17.2313])\n",
      "\t Grad: tensor([-0.0022,  0.0125])\n",
      "Epoch 3220, Loss 2.9281203746795654\n",
      "\t Params: tensor([  5.3548, -17.2314])\n",
      "\t Grad: tensor([-0.0022,  0.0125])\n",
      "Epoch 3221, Loss 2.9281179904937744\n",
      "\t Params: tensor([  5.3548, -17.2315])\n",
      "\t Grad: tensor([-0.0022,  0.0125])\n",
      "Epoch 3222, Loss 2.9281165599823\n",
      "\t Params: tensor([  5.3548, -17.2317])\n",
      "\t Grad: tensor([-0.0022,  0.0124])\n",
      "Epoch 3223, Loss 2.928115129470825\n",
      "\t Params: tensor([  5.3548, -17.2318])\n",
      "\t Grad: tensor([-0.0022,  0.0124])\n",
      "Epoch 3224, Loss 2.9281129837036133\n",
      "\t Params: tensor([  5.3548, -17.2319])\n",
      "\t Grad: tensor([-0.0022,  0.0124])\n",
      "Epoch 3225, Loss 2.928110361099243\n",
      "\t Params: tensor([  5.3549, -17.2320])\n",
      "\t Grad: tensor([-0.0022,  0.0124])\n",
      "Epoch 3226, Loss 2.9281091690063477\n",
      "\t Params: tensor([  5.3549, -17.2322])\n",
      "\t Grad: tensor([-0.0022,  0.0124])\n",
      "Epoch 3227, Loss 2.9281082153320312\n",
      "\t Params: tensor([  5.3549, -17.2323])\n",
      "\t Grad: tensor([-0.0022,  0.0123])\n",
      "Epoch 3228, Loss 2.9281046390533447\n",
      "\t Params: tensor([  5.3549, -17.2324])\n",
      "\t Grad: tensor([-0.0022,  0.0123])\n",
      "Epoch 3229, Loss 2.9281046390533447\n",
      "\t Params: tensor([  5.3550, -17.2325])\n",
      "\t Grad: tensor([-0.0022,  0.0123])\n",
      "Epoch 3230, Loss 2.9281041622161865\n",
      "\t Params: tensor([  5.3550, -17.2327])\n",
      "\t Grad: tensor([-0.0022,  0.0123])\n",
      "Epoch 3231, Loss 2.9281020164489746\n",
      "\t Params: tensor([  5.3550, -17.2328])\n",
      "\t Grad: tensor([-0.0021,  0.0123])\n",
      "Epoch 3232, Loss 2.9281013011932373\n",
      "\t Params: tensor([  5.3550, -17.2329])\n",
      "\t Grad: tensor([-0.0022,  0.0122])\n",
      "Epoch 3233, Loss 2.928098201751709\n",
      "\t Params: tensor([  5.3550, -17.2330])\n",
      "\t Grad: tensor([-0.0022,  0.0122])\n",
      "Epoch 3234, Loss 2.9280970096588135\n",
      "\t Params: tensor([  5.3551, -17.2331])\n",
      "\t Grad: tensor([-0.0022,  0.0122])\n",
      "Epoch 3235, Loss 2.9280951023101807\n",
      "\t Params: tensor([  5.3551, -17.2333])\n",
      "\t Grad: tensor([-0.0022,  0.0122])\n",
      "Epoch 3236, Loss 2.9280943870544434\n",
      "\t Params: tensor([  5.3551, -17.2334])\n",
      "\t Grad: tensor([-0.0022,  0.0121])\n",
      "Epoch 3237, Loss 2.928093194961548\n",
      "\t Params: tensor([  5.3551, -17.2335])\n",
      "\t Grad: tensor([-0.0022,  0.0121])\n",
      "Epoch 3238, Loss 2.928091287612915\n",
      "\t Params: tensor([  5.3551, -17.2336])\n",
      "\t Grad: tensor([-0.0022,  0.0121])\n",
      "Epoch 3239, Loss 2.9280898571014404\n",
      "\t Params: tensor([  5.3552, -17.2338])\n",
      "\t Grad: tensor([-0.0021,  0.0121])\n",
      "Epoch 3240, Loss 2.9280877113342285\n",
      "\t Params: tensor([  5.3552, -17.2339])\n",
      "\t Grad: tensor([-0.0021,  0.0121])\n",
      "Epoch 3241, Loss 2.9280855655670166\n",
      "\t Params: tensor([  5.3552, -17.2340])\n",
      "\t Grad: tensor([-0.0021,  0.0120])\n",
      "Epoch 3242, Loss 2.9280853271484375\n",
      "\t Params: tensor([  5.3552, -17.2341])\n",
      "\t Grad: tensor([-0.0021,  0.0120])\n",
      "Epoch 3243, Loss 2.928083896636963\n",
      "\t Params: tensor([  5.3553, -17.2342])\n",
      "\t Grad: tensor([-0.0021,  0.0120])\n",
      "Epoch 3244, Loss 2.9280824661254883\n",
      "\t Params: tensor([  5.3553, -17.2344])\n",
      "\t Grad: tensor([-0.0021,  0.0120])\n",
      "Epoch 3245, Loss 2.928079843521118\n",
      "\t Params: tensor([  5.3553, -17.2345])\n",
      "\t Grad: tensor([-0.0021,  0.0120])\n",
      "Epoch 3246, Loss 2.9280786514282227\n",
      "\t Params: tensor([  5.3553, -17.2346])\n",
      "\t Grad: tensor([-0.0021,  0.0119])\n",
      "Epoch 3247, Loss 2.9280762672424316\n",
      "\t Params: tensor([  5.3553, -17.2347])\n",
      "\t Grad: tensor([-0.0021,  0.0119])\n",
      "Epoch 3248, Loss 2.92807674407959\n",
      "\t Params: tensor([  5.3554, -17.2348])\n",
      "\t Grad: tensor([-0.0021,  0.0119])\n",
      "Epoch 3249, Loss 2.928074836730957\n",
      "\t Params: tensor([  5.3554, -17.2350])\n",
      "\t Grad: tensor([-0.0021,  0.0119])\n",
      "Epoch 3250, Loss 2.928072452545166\n",
      "\t Params: tensor([  5.3554, -17.2351])\n",
      "\t Grad: tensor([-0.0021,  0.0119])\n",
      "Epoch 3251, Loss 2.9280717372894287\n",
      "\t Params: tensor([  5.3554, -17.2352])\n",
      "\t Grad: tensor([-0.0021,  0.0118])\n",
      "Epoch 3252, Loss 2.9280710220336914\n",
      "\t Params: tensor([  5.3554, -17.2353])\n",
      "\t Grad: tensor([-0.0021,  0.0118])\n",
      "Epoch 3253, Loss 2.9280683994293213\n",
      "\t Params: tensor([  5.3555, -17.2354])\n",
      "\t Grad: tensor([-0.0021,  0.0118])\n",
      "Epoch 3254, Loss 2.9280686378479004\n",
      "\t Params: tensor([  5.3555, -17.2355])\n",
      "\t Grad: tensor([-0.0021,  0.0118])\n",
      "Epoch 3255, Loss 2.928065538406372\n",
      "\t Params: tensor([  5.3555, -17.2357])\n",
      "\t Grad: tensor([-0.0021,  0.0118])\n",
      "Epoch 3256, Loss 2.9280645847320557\n",
      "\t Params: tensor([  5.3555, -17.2358])\n",
      "\t Grad: tensor([-0.0021,  0.0117])\n",
      "Epoch 3257, Loss 2.9280638694763184\n",
      "\t Params: tensor([  5.3555, -17.2359])\n",
      "\t Grad: tensor([-0.0021,  0.0117])\n",
      "Epoch 3258, Loss 2.928061008453369\n",
      "\t Params: tensor([  5.3556, -17.2360])\n",
      "\t Grad: tensor([-0.0021,  0.0117])\n",
      "Epoch 3259, Loss 2.928060293197632\n",
      "\t Params: tensor([  5.3556, -17.2361])\n",
      "\t Grad: tensor([-0.0021,  0.0117])\n",
      "Epoch 3260, Loss 2.9280571937561035\n",
      "\t Params: tensor([  5.3556, -17.2362])\n",
      "\t Grad: tensor([-0.0021,  0.0117])\n",
      "Epoch 3261, Loss 2.9280576705932617\n",
      "\t Params: tensor([  5.3556, -17.2364])\n",
      "\t Grad: tensor([-0.0021,  0.0116])\n",
      "Epoch 3262, Loss 2.928055763244629\n",
      "\t Params: tensor([  5.3557, -17.2365])\n",
      "\t Grad: tensor([-0.0021,  0.0116])\n",
      "Epoch 3263, Loss 2.9280545711517334\n",
      "\t Params: tensor([  5.3557, -17.2366])\n",
      "\t Grad: tensor([-0.0021,  0.0116])\n",
      "Epoch 3264, Loss 2.9280524253845215\n",
      "\t Params: tensor([  5.3557, -17.2367])\n",
      "\t Grad: tensor([-0.0021,  0.0116])\n",
      "Epoch 3265, Loss 2.9280526638031006\n",
      "\t Params: tensor([  5.3557, -17.2368])\n",
      "\t Grad: tensor([-0.0021,  0.0116])\n",
      "Epoch 3266, Loss 2.9280507564544678\n",
      "\t Params: tensor([  5.3557, -17.2369])\n",
      "\t Grad: tensor([-0.0021,  0.0115])\n",
      "Epoch 3267, Loss 2.9280495643615723\n",
      "\t Params: tensor([  5.3558, -17.2371])\n",
      "\t Grad: tensor([-0.0021,  0.0115])\n",
      "Epoch 3268, Loss 2.9280471801757812\n",
      "\t Params: tensor([  5.3558, -17.2372])\n",
      "\t Grad: tensor([-0.0020,  0.0115])\n",
      "Epoch 3269, Loss 2.928046226501465\n",
      "\t Params: tensor([  5.3558, -17.2373])\n",
      "\t Grad: tensor([-0.0020,  0.0115])\n",
      "Epoch 3270, Loss 2.9280455112457275\n",
      "\t Params: tensor([  5.3558, -17.2374])\n",
      "\t Grad: tensor([-0.0020,  0.0115])\n",
      "Epoch 3271, Loss 2.928044319152832\n",
      "\t Params: tensor([  5.3558, -17.2375])\n",
      "\t Grad: tensor([-0.0020,  0.0115])\n",
      "Epoch 3272, Loss 2.928041934967041\n",
      "\t Params: tensor([  5.3559, -17.2376])\n",
      "\t Grad: tensor([-0.0020,  0.0114])\n",
      "Epoch 3273, Loss 2.9280402660369873\n",
      "\t Params: tensor([  5.3559, -17.2377])\n",
      "\t Grad: tensor([-0.0020,  0.0114])\n",
      "Epoch 3274, Loss 2.92803955078125\n",
      "\t Params: tensor([  5.3559, -17.2379])\n",
      "\t Grad: tensor([-0.0020,  0.0114])\n",
      "Epoch 3275, Loss 2.9280364513397217\n",
      "\t Params: tensor([  5.3559, -17.2380])\n",
      "\t Grad: tensor([-0.0020,  0.0114])\n",
      "Epoch 3276, Loss 2.9280364513397217\n",
      "\t Params: tensor([  5.3559, -17.2381])\n",
      "\t Grad: tensor([-0.0020,  0.0113])\n",
      "Epoch 3277, Loss 2.928036689758301\n",
      "\t Params: tensor([  5.3560, -17.2382])\n",
      "\t Grad: tensor([-0.0020,  0.0113])\n",
      "Epoch 3278, Loss 2.9280340671539307\n",
      "\t Params: tensor([  5.3560, -17.2383])\n",
      "\t Grad: tensor([-0.0020,  0.0113])\n",
      "Epoch 3279, Loss 2.9280340671539307\n",
      "\t Params: tensor([  5.3560, -17.2384])\n",
      "\t Grad: tensor([-0.0020,  0.0113])\n",
      "Epoch 3280, Loss 2.9280312061309814\n",
      "\t Params: tensor([  5.3560, -17.2385])\n",
      "\t Grad: tensor([-0.0020,  0.0113])\n",
      "Epoch 3281, Loss 2.9280316829681396\n",
      "\t Params: tensor([  5.3560, -17.2386])\n",
      "\t Grad: tensor([-0.0020,  0.0113])\n",
      "Epoch 3282, Loss 2.9280283451080322\n",
      "\t Params: tensor([  5.3561, -17.2388])\n",
      "\t Grad: tensor([-0.0020,  0.0112])\n",
      "Epoch 3283, Loss 2.9280266761779785\n",
      "\t Params: tensor([  5.3561, -17.2389])\n",
      "\t Grad: tensor([-0.0020,  0.0112])\n",
      "Epoch 3284, Loss 2.928025960922241\n",
      "\t Params: tensor([  5.3561, -17.2390])\n",
      "\t Grad: tensor([-0.0020,  0.0112])\n",
      "Epoch 3285, Loss 2.928025007247925\n",
      "\t Params: tensor([  5.3561, -17.2391])\n",
      "\t Grad: tensor([-0.0020,  0.0112])\n",
      "Epoch 3286, Loss 2.9280240535736084\n",
      "\t Params: tensor([  5.3561, -17.2392])\n",
      "\t Grad: tensor([-0.0020,  0.0112])\n",
      "Epoch 3287, Loss 2.9280221462249756\n",
      "\t Params: tensor([  5.3562, -17.2393])\n",
      "\t Grad: tensor([-0.0020,  0.0111])\n",
      "Epoch 3288, Loss 2.928022623062134\n",
      "\t Params: tensor([  5.3562, -17.2394])\n",
      "\t Grad: tensor([-0.0020,  0.0111])\n",
      "Epoch 3289, Loss 2.928020715713501\n",
      "\t Params: tensor([  5.3562, -17.2395])\n",
      "\t Grad: tensor([-0.0020,  0.0111])\n",
      "Epoch 3290, Loss 2.9280190467834473\n",
      "\t Params: tensor([  5.3562, -17.2397])\n",
      "\t Grad: tensor([-0.0020,  0.0111])\n",
      "Epoch 3291, Loss 2.9280176162719727\n",
      "\t Params: tensor([  5.3562, -17.2398])\n",
      "\t Grad: tensor([-0.0020,  0.0111])\n",
      "Epoch 3292, Loss 2.9280169010162354\n",
      "\t Params: tensor([  5.3563, -17.2399])\n",
      "\t Grad: tensor([-0.0020,  0.0110])\n",
      "Epoch 3293, Loss 2.9280145168304443\n",
      "\t Params: tensor([  5.3563, -17.2400])\n",
      "\t Grad: tensor([-0.0020,  0.0110])\n",
      "Epoch 3294, Loss 2.9280130863189697\n",
      "\t Params: tensor([  5.3563, -17.2401])\n",
      "\t Grad: tensor([-0.0020,  0.0110])\n",
      "Epoch 3295, Loss 2.9280126094818115\n",
      "\t Params: tensor([  5.3563, -17.2402])\n",
      "\t Grad: tensor([-0.0019,  0.0110])\n",
      "Epoch 3296, Loss 2.928011178970337\n",
      "\t Params: tensor([  5.3563, -17.2403])\n",
      "\t Grad: tensor([-0.0019,  0.0110])\n",
      "Epoch 3297, Loss 2.928009033203125\n",
      "\t Params: tensor([  5.3563, -17.2404])\n",
      "\t Grad: tensor([-0.0019,  0.0110])\n",
      "Epoch 3298, Loss 2.9280083179473877\n",
      "\t Params: tensor([  5.3564, -17.2405])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad: tensor([-0.0019,  0.0109])\n",
      "Epoch 3299, Loss 2.928006172180176\n",
      "\t Params: tensor([  5.3564, -17.2406])\n",
      "\t Grad: tensor([-0.0019,  0.0109])\n",
      "Epoch 3300, Loss 2.9280073642730713\n",
      "\t Params: tensor([  5.3564, -17.2407])\n",
      "\t Grad: tensor([-0.0019,  0.0109])\n",
      "Epoch 3301, Loss 2.928006649017334\n",
      "\t Params: tensor([  5.3564, -17.2409])\n",
      "\t Grad: tensor([-0.0019,  0.0109])\n",
      "Epoch 3302, Loss 2.928004264831543\n",
      "\t Params: tensor([  5.3564, -17.2410])\n",
      "\t Grad: tensor([-0.0019,  0.0109])\n",
      "Epoch 3303, Loss 2.92800235748291\n",
      "\t Params: tensor([  5.3565, -17.2411])\n",
      "\t Grad: tensor([-0.0019,  0.0108])\n",
      "Epoch 3304, Loss 2.928001642227173\n",
      "\t Params: tensor([  5.3565, -17.2412])\n",
      "\t Grad: tensor([-0.0019,  0.0108])\n",
      "Epoch 3305, Loss 2.927999973297119\n",
      "\t Params: tensor([  5.3565, -17.2413])\n",
      "\t Grad: tensor([-0.0019,  0.0108])\n",
      "Epoch 3306, Loss 2.92799973487854\n",
      "\t Params: tensor([  5.3565, -17.2414])\n",
      "\t Grad: tensor([-0.0019,  0.0108])\n",
      "Epoch 3307, Loss 2.9279983043670654\n",
      "\t Params: tensor([  5.3565, -17.2415])\n",
      "\t Grad: tensor([-0.0019,  0.0108])\n",
      "Epoch 3308, Loss 2.927994728088379\n",
      "\t Params: tensor([  5.3566, -17.2416])\n",
      "\t Grad: tensor([-0.0019,  0.0107])\n",
      "Epoch 3309, Loss 2.927995443344116\n",
      "\t Params: tensor([  5.3566, -17.2417])\n",
      "\t Grad: tensor([-0.0019,  0.0107])\n",
      "Epoch 3310, Loss 2.9279935359954834\n",
      "\t Params: tensor([  5.3566, -17.2418])\n",
      "\t Grad: tensor([-0.0019,  0.0107])\n",
      "Epoch 3311, Loss 2.9279940128326416\n",
      "\t Params: tensor([  5.3566, -17.2419])\n",
      "\t Grad: tensor([-0.0019,  0.0107])\n",
      "Epoch 3312, Loss 2.9279913902282715\n",
      "\t Params: tensor([  5.3566, -17.2420])\n",
      "\t Grad: tensor([-0.0019,  0.0107])\n",
      "Epoch 3313, Loss 2.9279913902282715\n",
      "\t Params: tensor([  5.3567, -17.2421])\n",
      "\t Grad: tensor([-0.0019,  0.0107])\n",
      "Epoch 3314, Loss 2.9279897212982178\n",
      "\t Params: tensor([  5.3567, -17.2423])\n",
      "\t Grad: tensor([-0.0019,  0.0106])\n",
      "Epoch 3315, Loss 2.9279890060424805\n",
      "\t Params: tensor([  5.3567, -17.2424])\n",
      "\t Grad: tensor([-0.0019,  0.0106])\n",
      "Epoch 3316, Loss 2.927987575531006\n",
      "\t Params: tensor([  5.3567, -17.2425])\n",
      "\t Grad: tensor([-0.0019,  0.0106])\n",
      "Epoch 3317, Loss 2.9279863834381104\n",
      "\t Params: tensor([  5.3567, -17.2426])\n",
      "\t Grad: tensor([-0.0019,  0.0106])\n",
      "Epoch 3318, Loss 2.927985429763794\n",
      "\t Params: tensor([  5.3567, -17.2427])\n",
      "\t Grad: tensor([-0.0019,  0.0106])\n",
      "Epoch 3319, Loss 2.927983283996582\n",
      "\t Params: tensor([  5.3568, -17.2428])\n",
      "\t Grad: tensor([-0.0018,  0.0106])\n",
      "Epoch 3320, Loss 2.927983283996582\n",
      "\t Params: tensor([  5.3568, -17.2429])\n",
      "\t Grad: tensor([-0.0018,  0.0105])\n",
      "Epoch 3321, Loss 2.927980899810791\n",
      "\t Params: tensor([  5.3568, -17.2430])\n",
      "\t Grad: tensor([-0.0018,  0.0105])\n",
      "Epoch 3322, Loss 2.9279801845550537\n",
      "\t Params: tensor([  5.3568, -17.2431])\n",
      "\t Grad: tensor([-0.0018,  0.0105])\n",
      "Epoch 3323, Loss 2.927978515625\n",
      "\t Params: tensor([  5.3568, -17.2432])\n",
      "\t Grad: tensor([-0.0018,  0.0105])\n",
      "Epoch 3324, Loss 2.927978515625\n",
      "\t Params: tensor([  5.3569, -17.2433])\n",
      "\t Grad: tensor([-0.0018,  0.0105])\n",
      "Epoch 3325, Loss 2.9279768466949463\n",
      "\t Params: tensor([  5.3569, -17.2434])\n",
      "\t Grad: tensor([-0.0018,  0.0104])\n",
      "Epoch 3326, Loss 2.9279749393463135\n",
      "\t Params: tensor([  5.3569, -17.2435])\n",
      "\t Grad: tensor([-0.0019,  0.0104])\n",
      "Epoch 3327, Loss 2.9279730319976807\n",
      "\t Params: tensor([  5.3569, -17.2436])\n",
      "\t Grad: tensor([-0.0018,  0.0104])\n",
      "Epoch 3328, Loss 2.927973747253418\n",
      "\t Params: tensor([  5.3569, -17.2437])\n",
      "\t Grad: tensor([-0.0018,  0.0104])\n",
      "Epoch 3329, Loss 2.927973985671997\n",
      "\t Params: tensor([  5.3570, -17.2438])\n",
      "\t Grad: tensor([-0.0018,  0.0104])\n",
      "Epoch 3330, Loss 2.927971601486206\n",
      "\t Params: tensor([  5.3570, -17.2439])\n",
      "\t Grad: tensor([-0.0018,  0.0104])\n",
      "Epoch 3331, Loss 2.927971839904785\n",
      "\t Params: tensor([  5.3570, -17.2440])\n",
      "\t Grad: tensor([-0.0018,  0.0103])\n",
      "Epoch 3332, Loss 2.9279685020446777\n",
      "\t Params: tensor([  5.3570, -17.2441])\n",
      "\t Grad: tensor([-0.0018,  0.0103])\n",
      "Epoch 3333, Loss 2.9279685020446777\n",
      "\t Params: tensor([  5.3570, -17.2442])\n",
      "\t Grad: tensor([-0.0018,  0.0103])\n",
      "Epoch 3334, Loss 2.927966833114624\n",
      "\t Params: tensor([  5.3570, -17.2443])\n",
      "\t Grad: tensor([-0.0018,  0.0103])\n",
      "Epoch 3335, Loss 2.927967071533203\n",
      "\t Params: tensor([  5.3571, -17.2444])\n",
      "\t Grad: tensor([-0.0018,  0.0103])\n",
      "Epoch 3336, Loss 2.9279634952545166\n",
      "\t Params: tensor([  5.3571, -17.2446])\n",
      "\t Grad: tensor([-0.0018,  0.0102])\n",
      "Epoch 3337, Loss 2.9279634952545166\n",
      "\t Params: tensor([  5.3571, -17.2447])\n",
      "\t Grad: tensor([-0.0018,  0.0102])\n",
      "Epoch 3338, Loss 2.927962303161621\n",
      "\t Params: tensor([  5.3571, -17.2448])\n",
      "\t Grad: tensor([-0.0018,  0.0102])\n",
      "Epoch 3339, Loss 2.927962303161621\n",
      "\t Params: tensor([  5.3571, -17.2449])\n",
      "\t Grad: tensor([-0.0018,  0.0102])\n",
      "Epoch 3340, Loss 2.9279603958129883\n",
      "\t Params: tensor([  5.3572, -17.2450])\n",
      "\t Grad: tensor([-0.0018,  0.0102])\n",
      "Epoch 3341, Loss 2.9279603958129883\n",
      "\t Params: tensor([  5.3572, -17.2451])\n",
      "\t Grad: tensor([-0.0018,  0.0102])\n",
      "Epoch 3342, Loss 2.9279592037200928\n",
      "\t Params: tensor([  5.3572, -17.2452])\n",
      "\t Grad: tensor([-0.0018,  0.0101])\n",
      "Epoch 3343, Loss 2.9279580116271973\n",
      "\t Params: tensor([  5.3572, -17.2453])\n",
      "\t Grad: tensor([-0.0018,  0.0101])\n",
      "Epoch 3344, Loss 2.9279556274414062\n",
      "\t Params: tensor([  5.3572, -17.2454])\n",
      "\t Grad: tensor([-0.0018,  0.0101])\n",
      "Epoch 3345, Loss 2.9279556274414062\n",
      "\t Params: tensor([  5.3572, -17.2455])\n",
      "\t Grad: tensor([-0.0018,  0.0101])\n",
      "Epoch 3346, Loss 2.92795467376709\n",
      "\t Params: tensor([  5.3573, -17.2456])\n",
      "\t Grad: tensor([-0.0018,  0.0101])\n",
      "Epoch 3347, Loss 2.9279532432556152\n",
      "\t Params: tensor([  5.3573, -17.2457])\n",
      "\t Grad: tensor([-0.0018,  0.0101])\n",
      "Epoch 3348, Loss 2.9279534816741943\n",
      "\t Params: tensor([  5.3573, -17.2458])\n",
      "\t Grad: tensor([-0.0018,  0.0100])\n",
      "Epoch 3349, Loss 2.927950859069824\n",
      "\t Params: tensor([  5.3573, -17.2459])\n",
      "\t Grad: tensor([-0.0018,  0.0100])\n",
      "Epoch 3350, Loss 2.927950382232666\n",
      "\t Params: tensor([  5.3573, -17.2460])\n",
      "\t Grad: tensor([-0.0018,  0.0100])\n",
      "Epoch 3351, Loss 2.927948236465454\n",
      "\t Params: tensor([  5.3573, -17.2461])\n",
      "\t Grad: tensor([-0.0018,  0.0100])\n",
      "Epoch 3352, Loss 2.9279470443725586\n",
      "\t Params: tensor([  5.3574, -17.2462])\n",
      "\t Grad: tensor([-0.0017,  0.0100])\n",
      "Epoch 3353, Loss 2.927947998046875\n",
      "\t Params: tensor([  5.3574, -17.2463])\n",
      "\t Grad: tensor([-0.0018,  0.0100])\n",
      "Epoch 3354, Loss 2.927945137023926\n",
      "\t Params: tensor([  5.3574, -17.2464])\n",
      "\t Grad: tensor([-0.0017,  0.0099])\n",
      "Epoch 3355, Loss 2.9279441833496094\n",
      "\t Params: tensor([  5.3574, -17.2465])\n",
      "\t Grad: tensor([-0.0017,  0.0099])\n",
      "Epoch 3356, Loss 2.927943468093872\n",
      "\t Params: tensor([  5.3574, -17.2466])\n",
      "\t Grad: tensor([-0.0018,  0.0099])\n",
      "Epoch 3357, Loss 2.9279441833496094\n",
      "\t Params: tensor([  5.3575, -17.2467])\n",
      "\t Grad: tensor([-0.0017,  0.0099])\n",
      "Epoch 3358, Loss 2.9279417991638184\n",
      "\t Params: tensor([  5.3575, -17.2468])\n",
      "\t Grad: tensor([-0.0017,  0.0099])\n",
      "Epoch 3359, Loss 2.927940607070923\n",
      "\t Params: tensor([  5.3575, -17.2469])\n",
      "\t Grad: tensor([-0.0018,  0.0099])\n",
      "Epoch 3360, Loss 2.9279396533966064\n",
      "\t Params: tensor([  5.3575, -17.2470])\n",
      "\t Grad: tensor([-0.0017,  0.0098])\n",
      "Epoch 3361, Loss 2.9279379844665527\n",
      "\t Params: tensor([  5.3575, -17.2471])\n",
      "\t Grad: tensor([-0.0017,  0.0098])\n",
      "Epoch 3362, Loss 2.9279375076293945\n",
      "\t Params: tensor([  5.3575, -17.2472])\n",
      "\t Grad: tensor([-0.0018,  0.0098])\n",
      "Epoch 3363, Loss 2.927936315536499\n",
      "\t Params: tensor([  5.3576, -17.2473])\n",
      "\t Grad: tensor([-0.0017,  0.0098])\n",
      "Epoch 3364, Loss 2.9279356002807617\n",
      "\t Params: tensor([  5.3576, -17.2474])\n",
      "\t Grad: tensor([-0.0017,  0.0098])\n",
      "Epoch 3365, Loss 2.927936553955078\n",
      "\t Params: tensor([  5.3576, -17.2474])\n",
      "\t Grad: tensor([-0.0017,  0.0098])\n",
      "Epoch 3366, Loss 2.927934169769287\n",
      "\t Params: tensor([  5.3576, -17.2475])\n",
      "\t Grad: tensor([-0.0017,  0.0097])\n",
      "Epoch 3367, Loss 2.92793345451355\n",
      "\t Params: tensor([  5.3576, -17.2476])\n",
      "\t Grad: tensor([-0.0017,  0.0097])\n",
      "Epoch 3368, Loss 2.927932024002075\n",
      "\t Params: tensor([  5.3576, -17.2477])\n",
      "\t Grad: tensor([-0.0017,  0.0097])\n",
      "Epoch 3369, Loss 2.9279298782348633\n",
      "\t Params: tensor([  5.3577, -17.2478])\n",
      "\t Grad: tensor([-0.0017,  0.0097])\n",
      "Epoch 3370, Loss 2.9279282093048096\n",
      "\t Params: tensor([  5.3577, -17.2479])\n",
      "\t Grad: tensor([-0.0017,  0.0097])\n",
      "Epoch 3371, Loss 2.927931070327759\n",
      "\t Params: tensor([  5.3577, -17.2480])\n",
      "\t Grad: tensor([-0.0017,  0.0097])\n",
      "Epoch 3372, Loss 2.927928924560547\n",
      "\t Params: tensor([  5.3577, -17.2481])\n",
      "\t Grad: tensor([-0.0017,  0.0096])\n",
      "Epoch 3373, Loss 2.927926540374756\n",
      "\t Params: tensor([  5.3577, -17.2482])\n",
      "\t Grad: tensor([-0.0017,  0.0096])\n",
      "Epoch 3374, Loss 2.9279258251190186\n",
      "\t Params: tensor([  5.3577, -17.2483])\n",
      "\t Grad: tensor([-0.0017,  0.0096])\n",
      "Epoch 3375, Loss 2.9279251098632812\n",
      "\t Params: tensor([  5.3578, -17.2484])\n",
      "\t Grad: tensor([-0.0017,  0.0096])\n",
      "Epoch 3376, Loss 2.927924156188965\n",
      "\t Params: tensor([  5.3578, -17.2485])\n",
      "\t Grad: tensor([-0.0017,  0.0096])\n",
      "Epoch 3377, Loss 2.9279227256774902\n",
      "\t Params: tensor([  5.3578, -17.2486])\n",
      "\t Grad: tensor([-0.0017,  0.0096])\n",
      "Epoch 3378, Loss 2.9279236793518066\n",
      "\t Params: tensor([  5.3578, -17.2487])\n",
      "\t Grad: tensor([-0.0017,  0.0095])\n",
      "Epoch 3379, Loss 2.927922010421753\n",
      "\t Params: tensor([  5.3578, -17.2488])\n",
      "\t Grad: tensor([-0.0017,  0.0095])\n",
      "Epoch 3380, Loss 2.9279215335845947\n",
      "\t Params: tensor([  5.3578, -17.2489])\n",
      "\t Grad: tensor([-0.0017,  0.0095])\n",
      "Epoch 3381, Loss 2.927919626235962\n",
      "\t Params: tensor([  5.3579, -17.2490])\n",
      "\t Grad: tensor([-0.0017,  0.0095])\n",
      "Epoch 3382, Loss 2.9279181957244873\n",
      "\t Params: tensor([  5.3579, -17.2491])\n",
      "\t Grad: tensor([-0.0017,  0.0095])\n",
      "Epoch 3383, Loss 2.92791748046875\n",
      "\t Params: tensor([  5.3579, -17.2492])\n",
      "\t Grad: tensor([-0.0017,  0.0095])\n",
      "Epoch 3384, Loss 2.9279167652130127\n",
      "\t Params: tensor([  5.3579, -17.2493])\n",
      "\t Grad: tensor([-0.0017,  0.0094])\n",
      "Epoch 3385, Loss 2.927914619445801\n",
      "\t Params: tensor([  5.3579, -17.2494])\n",
      "\t Grad: tensor([-0.0017,  0.0094])\n",
      "Epoch 3386, Loss 2.927915334701538\n",
      "\t Params: tensor([  5.3579, -17.2495])\n",
      "\t Grad: tensor([-0.0017,  0.0094])\n",
      "Epoch 3387, Loss 2.9279143810272217\n",
      "\t Params: tensor([  5.3580, -17.2496])\n",
      "\t Grad: tensor([-0.0016,  0.0094])\n",
      "Epoch 3388, Loss 2.9279134273529053\n",
      "\t Params: tensor([  5.3580, -17.2496])\n",
      "\t Grad: tensor([-0.0017,  0.0094])\n",
      "Epoch 3389, Loss 2.9279112815856934\n",
      "\t Params: tensor([  5.3580, -17.2497])\n",
      "\t Grad: tensor([-0.0016,  0.0094])\n",
      "Epoch 3390, Loss 2.927912950515747\n",
      "\t Params: tensor([  5.3580, -17.2498])\n",
      "\t Grad: tensor([-0.0017,  0.0093])\n",
      "Epoch 3391, Loss 2.9279112815856934\n",
      "\t Params: tensor([  5.3580, -17.2499])\n",
      "\t Grad: tensor([-0.0016,  0.0093])\n",
      "Epoch 3392, Loss 2.9279098510742188\n",
      "\t Params: tensor([  5.3580, -17.2500])\n",
      "\t Grad: tensor([-0.0016,  0.0093])\n",
      "Epoch 3393, Loss 2.9279088973999023\n",
      "\t Params: tensor([  5.3581, -17.2501])\n",
      "\t Grad: tensor([-0.0016,  0.0093])\n",
      "Epoch 3394, Loss 2.927908420562744\n",
      "\t Params: tensor([  5.3581, -17.2502])\n",
      "\t Grad: tensor([-0.0016,  0.0093])\n",
      "Epoch 3395, Loss 2.9279069900512695\n",
      "\t Params: tensor([  5.3581, -17.2503])\n",
      "\t Grad: tensor([-0.0017,  0.0093])\n",
      "Epoch 3396, Loss 2.927906036376953\n",
      "\t Params: tensor([  5.3581, -17.2504])\n",
      "\t Grad: tensor([-0.0016,  0.0093])\n",
      "Epoch 3397, Loss 2.9279050827026367\n",
      "\t Params: tensor([  5.3581, -17.2505])\n",
      "\t Grad: tensor([-0.0017,  0.0092])\n",
      "Epoch 3398, Loss 2.9279050827026367\n",
      "\t Params: tensor([  5.3581, -17.2506])\n",
      "\t Grad: tensor([-0.0016,  0.0092])\n",
      "Epoch 3399, Loss 2.927903890609741\n",
      "\t Params: tensor([  5.3582, -17.2507])\n",
      "\t Grad: tensor([-0.0016,  0.0092])\n",
      "Epoch 3400, Loss 2.9279019832611084\n",
      "\t Params: tensor([  5.3582, -17.2508])\n",
      "\t Grad: tensor([-0.0016,  0.0092])\n",
      "Epoch 3401, Loss 2.9279022216796875\n",
      "\t Params: tensor([  5.3582, -17.2509])\n",
      "\t Grad: tensor([-0.0016,  0.0092])\n",
      "Epoch 3402, Loss 2.9279022216796875\n",
      "\t Params: tensor([  5.3582, -17.2509])\n",
      "\t Grad: tensor([-0.0016,  0.0092])\n",
      "Epoch 3403, Loss 2.927899122238159\n",
      "\t Params: tensor([  5.3582, -17.2510])\n",
      "\t Grad: tensor([-0.0016,  0.0091])\n",
      "Epoch 3404, Loss 2.927899122238159\n",
      "\t Params: tensor([  5.3582, -17.2511])\n",
      "\t Grad: tensor([-0.0016,  0.0091])\n",
      "Epoch 3405, Loss 2.9278981685638428\n",
      "\t Params: tensor([  5.3583, -17.2512])\n",
      "\t Grad: tensor([-0.0016,  0.0091])\n",
      "Epoch 3406, Loss 2.927898645401001\n",
      "\t Params: tensor([  5.3583, -17.2513])\n",
      "\t Grad: tensor([-0.0016,  0.0091])\n",
      "Epoch 3407, Loss 2.927896022796631\n",
      "\t Params: tensor([  5.3583, -17.2514])\n",
      "\t Grad: tensor([-0.0016,  0.0091])\n",
      "Epoch 3408, Loss 2.9278948307037354\n",
      "\t Params: tensor([  5.3583, -17.2515])\n",
      "\t Grad: tensor([-0.0016,  0.0091])\n",
      "Epoch 3409, Loss 2.9278955459594727\n",
      "\t Params: tensor([  5.3583, -17.2516])\n",
      "\t Grad: tensor([-0.0016,  0.0091])\n",
      "Epoch 3410, Loss 2.92789363861084\n",
      "\t Params: tensor([  5.3583, -17.2517])\n",
      "\t Grad: tensor([-0.0016,  0.0090])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3411, Loss 2.9278924465179443\n",
      "\t Params: tensor([  5.3584, -17.2518])\n",
      "\t Grad: tensor([-0.0016,  0.0090])\n",
      "Epoch 3412, Loss 2.9278924465179443\n",
      "\t Params: tensor([  5.3584, -17.2519])\n",
      "\t Grad: tensor([-0.0016,  0.0090])\n",
      "Epoch 3413, Loss 2.927891492843628\n",
      "\t Params: tensor([  5.3584, -17.2519])\n",
      "\t Grad: tensor([-0.0016,  0.0090])\n",
      "Epoch 3414, Loss 2.9278907775878906\n",
      "\t Params: tensor([  5.3584, -17.2520])\n",
      "\t Grad: tensor([-0.0016,  0.0090])\n",
      "Epoch 3415, Loss 2.927889823913574\n",
      "\t Params: tensor([  5.3584, -17.2521])\n",
      "\t Grad: tensor([-0.0016,  0.0090])\n",
      "Epoch 3416, Loss 2.9278907775878906\n",
      "\t Params: tensor([  5.3584, -17.2522])\n",
      "\t Grad: tensor([-0.0016,  0.0089])\n",
      "Epoch 3417, Loss 2.9278879165649414\n",
      "\t Params: tensor([  5.3584, -17.2523])\n",
      "\t Grad: tensor([-0.0016,  0.0089])\n",
      "Epoch 3418, Loss 2.9278879165649414\n",
      "\t Params: tensor([  5.3585, -17.2524])\n",
      "\t Grad: tensor([-0.0016,  0.0089])\n",
      "Epoch 3419, Loss 2.9278855323791504\n",
      "\t Params: tensor([  5.3585, -17.2525])\n",
      "\t Grad: tensor([-0.0016,  0.0089])\n",
      "Epoch 3420, Loss 2.927886962890625\n",
      "\t Params: tensor([  5.3585, -17.2526])\n",
      "\t Grad: tensor([-0.0016,  0.0089])\n",
      "Epoch 3421, Loss 2.9278855323791504\n",
      "\t Params: tensor([  5.3585, -17.2527])\n",
      "\t Grad: tensor([-0.0016,  0.0089])\n",
      "Epoch 3422, Loss 2.9278838634490967\n",
      "\t Params: tensor([  5.3585, -17.2527])\n",
      "\t Grad: tensor([-0.0016,  0.0089])\n",
      "Epoch 3423, Loss 2.9278829097747803\n",
      "\t Params: tensor([  5.3585, -17.2528])\n",
      "\t Grad: tensor([-0.0015,  0.0088])\n",
      "Epoch 3424, Loss 2.9278814792633057\n",
      "\t Params: tensor([  5.3586, -17.2529])\n",
      "\t Grad: tensor([-0.0015,  0.0088])\n",
      "Epoch 3425, Loss 2.9278810024261475\n",
      "\t Params: tensor([  5.3586, -17.2530])\n",
      "\t Grad: tensor([-0.0016,  0.0088])\n",
      "Epoch 3426, Loss 2.92788028717041\n",
      "\t Params: tensor([  5.3586, -17.2531])\n",
      "\t Grad: tensor([-0.0015,  0.0088])\n",
      "Epoch 3427, Loss 2.927880048751831\n",
      "\t Params: tensor([  5.3586, -17.2532])\n",
      "\t Grad: tensor([-0.0016,  0.0088])\n",
      "Epoch 3428, Loss 2.9278790950775146\n",
      "\t Params: tensor([  5.3586, -17.2533])\n",
      "\t Grad: tensor([-0.0015,  0.0088])\n",
      "Epoch 3429, Loss 2.927877187728882\n",
      "\t Params: tensor([  5.3586, -17.2534])\n",
      "\t Grad: tensor([-0.0016,  0.0087])\n",
      "Epoch 3430, Loss 2.9278764724731445\n",
      "\t Params: tensor([  5.3586, -17.2534])\n",
      "\t Grad: tensor([-0.0015,  0.0087])\n",
      "Epoch 3431, Loss 2.9278764724731445\n",
      "\t Params: tensor([  5.3587, -17.2535])\n",
      "\t Grad: tensor([-0.0015,  0.0087])\n",
      "Epoch 3432, Loss 2.9278762340545654\n",
      "\t Params: tensor([  5.3587, -17.2536])\n",
      "\t Grad: tensor([-0.0015,  0.0087])\n",
      "Epoch 3433, Loss 2.9278764724731445\n",
      "\t Params: tensor([  5.3587, -17.2537])\n",
      "\t Grad: tensor([-0.0016,  0.0087])\n",
      "Epoch 3434, Loss 2.927875518798828\n",
      "\t Params: tensor([  5.3587, -17.2538])\n",
      "\t Grad: tensor([-0.0016,  0.0087])\n",
      "Epoch 3435, Loss 2.927874803543091\n",
      "\t Params: tensor([  5.3587, -17.2539])\n",
      "\t Grad: tensor([-0.0015,  0.0087])\n",
      "Epoch 3436, Loss 2.927873134613037\n",
      "\t Params: tensor([  5.3587, -17.2540])\n",
      "\t Grad: tensor([-0.0015,  0.0087])\n",
      "Epoch 3437, Loss 2.9278719425201416\n",
      "\t Params: tensor([  5.3588, -17.2541])\n",
      "\t Grad: tensor([-0.0015,  0.0086])\n",
      "Epoch 3438, Loss 2.927870750427246\n",
      "\t Params: tensor([  5.3588, -17.2541])\n",
      "\t Grad: tensor([-0.0015,  0.0086])\n",
      "Epoch 3439, Loss 2.927870273590088\n",
      "\t Params: tensor([  5.3588, -17.2542])\n",
      "\t Grad: tensor([-0.0015,  0.0086])\n",
      "Epoch 3440, Loss 2.927870988845825\n",
      "\t Params: tensor([  5.3588, -17.2543])\n",
      "\t Grad: tensor([-0.0015,  0.0086])\n",
      "Epoch 3441, Loss 2.927868604660034\n",
      "\t Params: tensor([  5.3588, -17.2544])\n",
      "\t Grad: tensor([-0.0015,  0.0086])\n",
      "Epoch 3442, Loss 2.9278688430786133\n",
      "\t Params: tensor([  5.3588, -17.2545])\n",
      "\t Grad: tensor([-0.0015,  0.0086])\n",
      "Epoch 3443, Loss 2.9278669357299805\n",
      "\t Params: tensor([  5.3588, -17.2546])\n",
      "\t Grad: tensor([-0.0015,  0.0085])\n",
      "Epoch 3444, Loss 2.927865505218506\n",
      "\t Params: tensor([  5.3589, -17.2547])\n",
      "\t Grad: tensor([-0.0015,  0.0085])\n",
      "Epoch 3445, Loss 2.927866220474243\n",
      "\t Params: tensor([  5.3589, -17.2547])\n",
      "\t Grad: tensor([-0.0015,  0.0085])\n",
      "Epoch 3446, Loss 2.927865743637085\n",
      "\t Params: tensor([  5.3589, -17.2548])\n",
      "\t Grad: tensor([-0.0015,  0.0085])\n",
      "Epoch 3447, Loss 2.9278640747070312\n",
      "\t Params: tensor([  5.3589, -17.2549])\n",
      "\t Grad: tensor([-0.0015,  0.0085])\n",
      "Epoch 3448, Loss 2.9278626441955566\n",
      "\t Params: tensor([  5.3589, -17.2550])\n",
      "\t Grad: tensor([-0.0015,  0.0085])\n",
      "Epoch 3449, Loss 2.927863359451294\n",
      "\t Params: tensor([  5.3589, -17.2551])\n",
      "\t Grad: tensor([-0.0015,  0.0085])\n",
      "Epoch 3450, Loss 2.9278619289398193\n",
      "\t Params: tensor([  5.3590, -17.2552])\n",
      "\t Grad: tensor([-0.0015,  0.0084])\n",
      "Epoch 3451, Loss 2.927863121032715\n",
      "\t Params: tensor([  5.3590, -17.2552])\n",
      "\t Grad: tensor([-0.0015,  0.0084])\n",
      "Epoch 3452, Loss 2.9278602600097656\n",
      "\t Params: tensor([  5.3590, -17.2553])\n",
      "\t Grad: tensor([-0.0015,  0.0084])\n",
      "Epoch 3453, Loss 2.9278600215911865\n",
      "\t Params: tensor([  5.3590, -17.2554])\n",
      "\t Grad: tensor([-0.0015,  0.0084])\n",
      "Epoch 3454, Loss 2.9278600215911865\n",
      "\t Params: tensor([  5.3590, -17.2555])\n",
      "\t Grad: tensor([-0.0015,  0.0084])\n",
      "Epoch 3455, Loss 2.927858591079712\n",
      "\t Params: tensor([  5.3590, -17.2556])\n",
      "\t Grad: tensor([-0.0015,  0.0084])\n",
      "Epoch 3456, Loss 2.9278578758239746\n",
      "\t Params: tensor([  5.3590, -17.2557])\n",
      "\t Grad: tensor([-0.0015,  0.0084])\n",
      "Epoch 3457, Loss 2.9278581142425537\n",
      "\t Params: tensor([  5.3591, -17.2557])\n",
      "\t Grad: tensor([-0.0015,  0.0083])\n",
      "Epoch 3458, Loss 2.9278557300567627\n",
      "\t Params: tensor([  5.3591, -17.2558])\n",
      "\t Grad: tensor([-0.0015,  0.0083])\n",
      "Epoch 3459, Loss 2.927856683731079\n",
      "\t Params: tensor([  5.3591, -17.2559])\n",
      "\t Grad: tensor([-0.0015,  0.0083])\n",
      "Epoch 3460, Loss 2.927853584289551\n",
      "\t Params: tensor([  5.3591, -17.2560])\n",
      "\t Grad: tensor([-0.0015,  0.0083])\n",
      "Epoch 3461, Loss 2.9278554916381836\n",
      "\t Params: tensor([  5.3591, -17.2561])\n",
      "\t Grad: tensor([-0.0015,  0.0083])\n",
      "Epoch 3462, Loss 2.927853584289551\n",
      "\t Params: tensor([  5.3591, -17.2562])\n",
      "\t Grad: tensor([-0.0015,  0.0083])\n",
      "Epoch 3463, Loss 2.927853584289551\n",
      "\t Params: tensor([  5.3591, -17.2562])\n",
      "\t Grad: tensor([-0.0015,  0.0083])\n",
      "Epoch 3464, Loss 2.9278512001037598\n",
      "\t Params: tensor([  5.3592, -17.2563])\n",
      "\t Grad: tensor([-0.0014,  0.0082])\n",
      "Epoch 3465, Loss 2.9278528690338135\n",
      "\t Params: tensor([  5.3592, -17.2564])\n",
      "\t Grad: tensor([-0.0015,  0.0082])\n",
      "Epoch 3466, Loss 2.927851676940918\n",
      "\t Params: tensor([  5.3592, -17.2565])\n",
      "\t Grad: tensor([-0.0014,  0.0082])\n",
      "Epoch 3467, Loss 2.9278502464294434\n",
      "\t Params: tensor([  5.3592, -17.2566])\n",
      "\t Grad: tensor([-0.0014,  0.0082])\n",
      "Epoch 3468, Loss 2.927849054336548\n",
      "\t Params: tensor([  5.3592, -17.2567])\n",
      "\t Grad: tensor([-0.0015,  0.0082])\n",
      "Epoch 3469, Loss 2.9278488159179688\n",
      "\t Params: tensor([  5.3592, -17.2567])\n",
      "\t Grad: tensor([-0.0015,  0.0082])\n",
      "Epoch 3470, Loss 2.9278481006622314\n",
      "\t Params: tensor([  5.3592, -17.2568])\n",
      "\t Grad: tensor([-0.0014,  0.0082])\n",
      "Epoch 3471, Loss 2.9278481006622314\n",
      "\t Params: tensor([  5.3593, -17.2569])\n",
      "\t Grad: tensor([-0.0014,  0.0081])\n",
      "Epoch 3472, Loss 2.9278459548950195\n",
      "\t Params: tensor([  5.3593, -17.2570])\n",
      "\t Grad: tensor([-0.0015,  0.0081])\n",
      "Epoch 3473, Loss 2.9278459548950195\n",
      "\t Params: tensor([  5.3593, -17.2571])\n",
      "\t Grad: tensor([-0.0015,  0.0081])\n",
      "Epoch 3474, Loss 2.927845001220703\n",
      "\t Params: tensor([  5.3593, -17.2571])\n",
      "\t Grad: tensor([-0.0014,  0.0081])\n",
      "Epoch 3475, Loss 2.9278440475463867\n",
      "\t Params: tensor([  5.3593, -17.2572])\n",
      "\t Grad: tensor([-0.0014,  0.0081])\n",
      "Epoch 3476, Loss 2.927844285964966\n",
      "\t Params: tensor([  5.3593, -17.2573])\n",
      "\t Grad: tensor([-0.0014,  0.0081])\n",
      "Epoch 3477, Loss 2.9278440475463867\n",
      "\t Params: tensor([  5.3593, -17.2574])\n",
      "\t Grad: tensor([-0.0014,  0.0081])\n",
      "Epoch 3478, Loss 2.927842855453491\n",
      "\t Params: tensor([  5.3594, -17.2575])\n",
      "\t Grad: tensor([-0.0014,  0.0081])\n",
      "Epoch 3479, Loss 2.927842617034912\n",
      "\t Params: tensor([  5.3594, -17.2575])\n",
      "\t Grad: tensor([-0.0014,  0.0080])\n",
      "Epoch 3480, Loss 2.927841901779175\n",
      "\t Params: tensor([  5.3594, -17.2576])\n",
      "\t Grad: tensor([-0.0014,  0.0080])\n",
      "Epoch 3481, Loss 2.927839756011963\n",
      "\t Params: tensor([  5.3594, -17.2577])\n",
      "\t Grad: tensor([-0.0014,  0.0080])\n",
      "Epoch 3482, Loss 2.927841901779175\n",
      "\t Params: tensor([  5.3594, -17.2578])\n",
      "\t Grad: tensor([-0.0014,  0.0080])\n",
      "Epoch 3483, Loss 2.9278390407562256\n",
      "\t Params: tensor([  5.3594, -17.2579])\n",
      "\t Grad: tensor([-0.0014,  0.0080])\n",
      "Epoch 3484, Loss 2.9278383255004883\n",
      "\t Params: tensor([  5.3594, -17.2579])\n",
      "\t Grad: tensor([-0.0014,  0.0080])\n",
      "Epoch 3485, Loss 2.9278388023376465\n",
      "\t Params: tensor([  5.3595, -17.2580])\n",
      "\t Grad: tensor([-0.0014,  0.0080])\n",
      "Epoch 3486, Loss 2.9278383255004883\n",
      "\t Params: tensor([  5.3595, -17.2581])\n",
      "\t Grad: tensor([-0.0014,  0.0079])\n",
      "Epoch 3487, Loss 2.9278366565704346\n",
      "\t Params: tensor([  5.3595, -17.2582])\n",
      "\t Grad: tensor([-0.0014,  0.0079])\n",
      "Epoch 3488, Loss 2.927834987640381\n",
      "\t Params: tensor([  5.3595, -17.2583])\n",
      "\t Grad: tensor([-0.0014,  0.0079])\n",
      "Epoch 3489, Loss 2.9278366565704346\n",
      "\t Params: tensor([  5.3595, -17.2583])\n",
      "\t Grad: tensor([-0.0014,  0.0079])\n",
      "Epoch 3490, Loss 2.927835702896118\n",
      "\t Params: tensor([  5.3595, -17.2584])\n",
      "\t Grad: tensor([-0.0014,  0.0079])\n",
      "Epoch 3491, Loss 2.9278345108032227\n",
      "\t Params: tensor([  5.3595, -17.2585])\n",
      "\t Grad: tensor([-0.0014,  0.0079])\n",
      "Epoch 3492, Loss 2.927833318710327\n",
      "\t Params: tensor([  5.3596, -17.2586])\n",
      "\t Grad: tensor([-0.0014,  0.0079])\n",
      "Epoch 3493, Loss 2.927832841873169\n",
      "\t Params: tensor([  5.3596, -17.2587])\n",
      "\t Grad: tensor([-0.0014,  0.0079])\n",
      "Epoch 3494, Loss 2.927832841873169\n",
      "\t Params: tensor([  5.3596, -17.2587])\n",
      "\t Grad: tensor([-0.0014,  0.0078])\n",
      "Epoch 3495, Loss 2.9278321266174316\n",
      "\t Params: tensor([  5.3596, -17.2588])\n",
      "\t Grad: tensor([-0.0014,  0.0078])\n",
      "Epoch 3496, Loss 2.9278311729431152\n",
      "\t Params: tensor([  5.3596, -17.2589])\n",
      "\t Grad: tensor([-0.0014,  0.0078])\n",
      "Epoch 3497, Loss 2.927830457687378\n",
      "\t Params: tensor([  5.3596, -17.2590])\n",
      "\t Grad: tensor([-0.0014,  0.0078])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3498, Loss 2.9278299808502197\n",
      "\t Params: tensor([  5.3596, -17.2590])\n",
      "\t Grad: tensor([-0.0014,  0.0078])\n",
      "Epoch 3499, Loss 2.9278297424316406\n",
      "\t Params: tensor([  5.3597, -17.2591])\n",
      "\t Grad: tensor([-0.0014,  0.0078])\n",
      "Epoch 3500, Loss 2.9278290271759033\n",
      "\t Params: tensor([  5.3597, -17.2592])\n",
      "\t Grad: tensor([-0.0014,  0.0078])\n",
      "Epoch 3501, Loss 2.9278275966644287\n",
      "\t Params: tensor([  5.3597, -17.2593])\n",
      "\t Grad: tensor([-0.0014,  0.0077])\n",
      "Epoch 3502, Loss 2.927828073501587\n",
      "\t Params: tensor([  5.3597, -17.2594])\n",
      "\t Grad: tensor([-0.0014,  0.0077])\n",
      "Epoch 3503, Loss 2.9278268814086914\n",
      "\t Params: tensor([  5.3597, -17.2594])\n",
      "\t Grad: tensor([-0.0014,  0.0077])\n",
      "Epoch 3504, Loss 2.9278249740600586\n",
      "\t Params: tensor([  5.3597, -17.2595])\n",
      "\t Grad: tensor([-0.0014,  0.0077])\n",
      "Epoch 3505, Loss 2.9278268814086914\n",
      "\t Params: tensor([  5.3597, -17.2596])\n",
      "\t Grad: tensor([-0.0014,  0.0077])\n",
      "Epoch 3506, Loss 2.9278249740600586\n",
      "\t Params: tensor([  5.3597, -17.2597])\n",
      "\t Grad: tensor([-0.0014,  0.0077])\n",
      "Epoch 3507, Loss 2.9278242588043213\n",
      "\t Params: tensor([  5.3598, -17.2597])\n",
      "\t Grad: tensor([-0.0013,  0.0077])\n",
      "Epoch 3508, Loss 2.927823781967163\n",
      "\t Params: tensor([  5.3598, -17.2598])\n",
      "\t Grad: tensor([-0.0013,  0.0077])\n",
      "Epoch 3509, Loss 2.927823781967163\n",
      "\t Params: tensor([  5.3598, -17.2599])\n",
      "\t Grad: tensor([-0.0013,  0.0076])\n",
      "Epoch 3510, Loss 2.9278218746185303\n",
      "\t Params: tensor([  5.3598, -17.2600])\n",
      "\t Grad: tensor([-0.0014,  0.0076])\n",
      "Epoch 3511, Loss 2.9278223514556885\n",
      "\t Params: tensor([  5.3598, -17.2600])\n",
      "\t Grad: tensor([-0.0014,  0.0076])\n",
      "Epoch 3512, Loss 2.927821397781372\n",
      "\t Params: tensor([  5.3598, -17.2601])\n",
      "\t Grad: tensor([-0.0013,  0.0076])\n",
      "Epoch 3513, Loss 2.9278199672698975\n",
      "\t Params: tensor([  5.3598, -17.2602])\n",
      "\t Grad: tensor([-0.0013,  0.0076])\n",
      "Epoch 3514, Loss 2.9278197288513184\n",
      "\t Params: tensor([  5.3599, -17.2603])\n",
      "\t Grad: tensor([-0.0014,  0.0076])\n",
      "Epoch 3515, Loss 2.9278206825256348\n",
      "\t Params: tensor([  5.3599, -17.2604])\n",
      "\t Grad: tensor([-0.0014,  0.0076])\n",
      "Epoch 3516, Loss 2.92781925201416\n",
      "\t Params: tensor([  5.3599, -17.2604])\n",
      "\t Grad: tensor([-0.0014,  0.0075])\n",
      "Epoch 3517, Loss 2.92781925201416\n",
      "\t Params: tensor([  5.3599, -17.2605])\n",
      "\t Grad: tensor([-0.0014,  0.0075])\n",
      "Epoch 3518, Loss 2.927818536758423\n",
      "\t Params: tensor([  5.3599, -17.2606])\n",
      "\t Grad: tensor([-0.0013,  0.0075])\n",
      "Epoch 3519, Loss 2.9278173446655273\n",
      "\t Params: tensor([  5.3599, -17.2607])\n",
      "\t Grad: tensor([-0.0013,  0.0075])\n",
      "Epoch 3520, Loss 2.9278173446655273\n",
      "\t Params: tensor([  5.3599, -17.2607])\n",
      "\t Grad: tensor([-0.0013,  0.0075])\n",
      "Epoch 3521, Loss 2.927816152572632\n",
      "\t Params: tensor([  5.3599, -17.2608])\n",
      "\t Grad: tensor([-0.0013,  0.0075])\n",
      "Epoch 3522, Loss 2.9278151988983154\n",
      "\t Params: tensor([  5.3600, -17.2609])\n",
      "\t Grad: tensor([-0.0013,  0.0075])\n",
      "Epoch 3523, Loss 2.9278159141540527\n",
      "\t Params: tensor([  5.3600, -17.2610])\n",
      "\t Grad: tensor([-0.0013,  0.0075])\n",
      "Epoch 3524, Loss 2.9278147220611572\n",
      "\t Params: tensor([  5.3600, -17.2610])\n",
      "\t Grad: tensor([-0.0013,  0.0074])\n",
      "Epoch 3525, Loss 2.9278135299682617\n",
      "\t Params: tensor([  5.3600, -17.2611])\n",
      "\t Grad: tensor([-0.0013,  0.0074])\n",
      "Epoch 3526, Loss 2.9278130531311035\n",
      "\t Params: tensor([  5.3600, -17.2612])\n",
      "\t Grad: tensor([-0.0013,  0.0074])\n",
      "Epoch 3527, Loss 2.927812337875366\n",
      "\t Params: tensor([  5.3600, -17.2612])\n",
      "\t Grad: tensor([-0.0013,  0.0074])\n",
      "Epoch 3528, Loss 2.9278109073638916\n",
      "\t Params: tensor([  5.3600, -17.2613])\n",
      "\t Grad: tensor([-0.0013,  0.0074])\n",
      "Epoch 3529, Loss 2.927811622619629\n",
      "\t Params: tensor([  5.3601, -17.2614])\n",
      "\t Grad: tensor([-0.0013,  0.0074])\n",
      "Epoch 3530, Loss 2.927811622619629\n",
      "\t Params: tensor([  5.3601, -17.2615])\n",
      "\t Grad: tensor([-0.0013,  0.0074])\n",
      "Epoch 3531, Loss 2.927809953689575\n",
      "\t Params: tensor([  5.3601, -17.2615])\n",
      "\t Grad: tensor([-0.0013,  0.0074])\n",
      "Epoch 3532, Loss 2.927809000015259\n",
      "\t Params: tensor([  5.3601, -17.2616])\n",
      "\t Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch 3533, Loss 2.9278104305267334\n",
      "\t Params: tensor([  5.3601, -17.2617])\n",
      "\t Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch 3534, Loss 2.927809238433838\n",
      "\t Params: tensor([  5.3601, -17.2618])\n",
      "\t Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch 3535, Loss 2.9278082847595215\n",
      "\t Params: tensor([  5.3601, -17.2618])\n",
      "\t Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch 3536, Loss 2.9278082847595215\n",
      "\t Params: tensor([  5.3601, -17.2619])\n",
      "\t Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch 3537, Loss 2.9278059005737305\n",
      "\t Params: tensor([  5.3602, -17.2620])\n",
      "\t Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch 3538, Loss 2.9278061389923096\n",
      "\t Params: tensor([  5.3602, -17.2621])\n",
      "\t Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch 3539, Loss 2.9278054237365723\n",
      "\t Params: tensor([  5.3602, -17.2621])\n",
      "\t Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch 3540, Loss 2.9278039932250977\n",
      "\t Params: tensor([  5.3602, -17.2622])\n",
      "\t Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch 3541, Loss 2.927804708480835\n",
      "\t Params: tensor([  5.3602, -17.2623])\n",
      "\t Grad: tensor([-0.0013,  0.0072])\n",
      "Epoch 3542, Loss 2.9278037548065186\n",
      "\t Params: tensor([  5.3602, -17.2623])\n",
      "\t Grad: tensor([-0.0013,  0.0072])\n",
      "Epoch 3543, Loss 2.927805185317993\n",
      "\t Params: tensor([  5.3602, -17.2624])\n",
      "\t Grad: tensor([-0.0013,  0.0072])\n",
      "Epoch 3544, Loss 2.9278037548065186\n",
      "\t Params: tensor([  5.3602, -17.2625])\n",
      "\t Grad: tensor([-0.0013,  0.0072])\n",
      "Epoch 3545, Loss 2.9278037548065186\n",
      "\t Params: tensor([  5.3603, -17.2626])\n",
      "\t Grad: tensor([-0.0013,  0.0072])\n",
      "Epoch 3546, Loss 2.927802801132202\n",
      "\t Params: tensor([  5.3603, -17.2626])\n",
      "\t Grad: tensor([-0.0013,  0.0072])\n",
      "Epoch 3547, Loss 2.927802085876465\n",
      "\t Params: tensor([  5.3603, -17.2627])\n",
      "\t Grad: tensor([-0.0013,  0.0072])\n",
      "Epoch 3548, Loss 2.9278006553649902\n",
      "\t Params: tensor([  5.3603, -17.2628])\n",
      "\t Grad: tensor([-0.0013,  0.0071])\n",
      "Epoch 3549, Loss 2.9278006553649902\n",
      "\t Params: tensor([  5.3603, -17.2628])\n",
      "\t Grad: tensor([-0.0013,  0.0071])\n",
      "Epoch 3550, Loss 2.9277992248535156\n",
      "\t Params: tensor([  5.3603, -17.2629])\n",
      "\t Grad: tensor([-0.0012,  0.0071])\n",
      "Epoch 3551, Loss 2.9278008937835693\n",
      "\t Params: tensor([  5.3603, -17.2630])\n",
      "\t Grad: tensor([-0.0012,  0.0071])\n",
      "Epoch 3552, Loss 2.927797794342041\n",
      "\t Params: tensor([  5.3603, -17.2631])\n",
      "\t Grad: tensor([-0.0012,  0.0071])\n",
      "Epoch 3553, Loss 2.927798271179199\n",
      "\t Params: tensor([  5.3604, -17.2631])\n",
      "\t Grad: tensor([-0.0012,  0.0071])\n",
      "Epoch 3554, Loss 2.927798271179199\n",
      "\t Params: tensor([  5.3604, -17.2632])\n",
      "\t Grad: tensor([-0.0013,  0.0071])\n",
      "Epoch 3555, Loss 2.927797794342041\n",
      "\t Params: tensor([  5.3604, -17.2633])\n",
      "\t Grad: tensor([-0.0013,  0.0071])\n",
      "Epoch 3556, Loss 2.927797794342041\n",
      "\t Params: tensor([  5.3604, -17.2633])\n",
      "\t Grad: tensor([-0.0013,  0.0071])\n",
      "Epoch 3557, Loss 2.9277961254119873\n",
      "\t Params: tensor([  5.3604, -17.2634])\n",
      "\t Grad: tensor([-0.0013,  0.0070])\n",
      "Epoch 3558, Loss 2.92779541015625\n",
      "\t Params: tensor([  5.3604, -17.2635])\n",
      "\t Grad: tensor([-0.0013,  0.0070])\n",
      "Epoch 3559, Loss 2.9277963638305664\n",
      "\t Params: tensor([  5.3604, -17.2636])\n",
      "\t Grad: tensor([-0.0013,  0.0070])\n",
      "Epoch 3560, Loss 2.9277939796447754\n",
      "\t Params: tensor([  5.3604, -17.2636])\n",
      "\t Grad: tensor([-0.0013,  0.0070])\n",
      "Epoch 3561, Loss 2.92779541015625\n",
      "\t Params: tensor([  5.3605, -17.2637])\n",
      "\t Grad: tensor([-0.0013,  0.0070])\n",
      "Epoch 3562, Loss 2.927795171737671\n",
      "\t Params: tensor([  5.3605, -17.2638])\n",
      "\t Grad: tensor([-0.0013,  0.0070])\n",
      "Epoch 3563, Loss 2.927793025970459\n",
      "\t Params: tensor([  5.3605, -17.2638])\n",
      "\t Grad: tensor([-0.0013,  0.0070])\n",
      "Epoch 3564, Loss 2.9277946949005127\n",
      "\t Params: tensor([  5.3605, -17.2639])\n",
      "\t Grad: tensor([-0.0012,  0.0070])\n",
      "Epoch 3565, Loss 2.9277913570404053\n",
      "\t Params: tensor([  5.3605, -17.2640])\n",
      "\t Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch 3566, Loss 2.9277913570404053\n",
      "\t Params: tensor([  5.3605, -17.2640])\n",
      "\t Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch 3567, Loss 2.9277913570404053\n",
      "\t Params: tensor([  5.3605, -17.2641])\n",
      "\t Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch 3568, Loss 2.9277901649475098\n",
      "\t Params: tensor([  5.3605, -17.2642])\n",
      "\t Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch 3569, Loss 2.9277899265289307\n",
      "\t Params: tensor([  5.3606, -17.2642])\n",
      "\t Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch 3570, Loss 2.9277894496917725\n",
      "\t Params: tensor([  5.3606, -17.2643])\n",
      "\t Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch 3571, Loss 2.9277899265289307\n",
      "\t Params: tensor([  5.3606, -17.2644])\n",
      "\t Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch 3572, Loss 2.9277894496917725\n",
      "\t Params: tensor([  5.3606, -17.2645])\n",
      "\t Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch 3573, Loss 2.9277894496917725\n",
      "\t Params: tensor([  5.3606, -17.2645])\n",
      "\t Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch 3574, Loss 2.9277892112731934\n",
      "\t Params: tensor([  5.3606, -17.2646])\n",
      "\t Grad: tensor([-0.0012,  0.0068])\n",
      "Epoch 3575, Loss 2.9277868270874023\n",
      "\t Params: tensor([  5.3606, -17.2647])\n",
      "\t Grad: tensor([-0.0012,  0.0068])\n",
      "Epoch 3576, Loss 2.927786111831665\n",
      "\t Params: tensor([  5.3606, -17.2647])\n",
      "\t Grad: tensor([-0.0012,  0.0068])\n",
      "Epoch 3577, Loss 2.927788019180298\n",
      "\t Params: tensor([  5.3607, -17.2648])\n",
      "\t Grad: tensor([-0.0012,  0.0068])\n",
      "Epoch 3578, Loss 2.9277853965759277\n",
      "\t Params: tensor([  5.3607, -17.2649])\n",
      "\t Grad: tensor([-0.0012,  0.0068])\n",
      "Epoch 3579, Loss 2.9277849197387695\n",
      "\t Params: tensor([  5.3607, -17.2649])\n",
      "\t Grad: tensor([-0.0012,  0.0068])\n",
      "Epoch 3580, Loss 2.927786350250244\n",
      "\t Params: tensor([  5.3607, -17.2650])\n",
      "\t Grad: tensor([-0.0012,  0.0068])\n",
      "Epoch 3581, Loss 2.9277849197387695\n",
      "\t Params: tensor([  5.3607, -17.2651])\n",
      "\t Grad: tensor([-0.0012,  0.0068])\n",
      "Epoch 3582, Loss 2.9277842044830322\n",
      "\t Params: tensor([  5.3607, -17.2651])\n",
      "\t Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch 3583, Loss 2.927783966064453\n",
      "\t Params: tensor([  5.3607, -17.2652])\n",
      "\t Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch 3584, Loss 2.9277825355529785\n",
      "\t Params: tensor([  5.3607, -17.2653])\n",
      "\t Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch 3585, Loss 2.9277830123901367\n",
      "\t Params: tensor([  5.3607, -17.2653])\n",
      "\t Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch 3586, Loss 2.927781105041504\n",
      "\t Params: tensor([  5.3608, -17.2654])\n",
      "\t Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch 3587, Loss 2.927781820297241\n",
      "\t Params: tensor([  5.3608, -17.2655])\n",
      "\t Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch 3588, Loss 2.927780866622925\n",
      "\t Params: tensor([  5.3608, -17.2655])\n",
      "\t Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch 3589, Loss 2.927780866622925\n",
      "\t Params: tensor([  5.3608, -17.2656])\n",
      "\t Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch 3590, Loss 2.927781105041504\n",
      "\t Params: tensor([  5.3608, -17.2657])\n",
      "\t Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch 3591, Loss 2.9277799129486084\n",
      "\t Params: tensor([  5.3608, -17.2657])\n",
      "\t Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch 3592, Loss 2.9277801513671875\n",
      "\t Params: tensor([  5.3608, -17.2658])\n",
      "\t Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch 3593, Loss 2.927778482437134\n",
      "\t Params: tensor([  5.3608, -17.2659])\n",
      "\t Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch 3594, Loss 2.927778720855713\n",
      "\t Params: tensor([  5.3609, -17.2659])\n",
      "\t Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch 3595, Loss 2.9277777671813965\n",
      "\t Params: tensor([  5.3609, -17.2660])\n",
      "\t Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch 3596, Loss 2.9277780055999756\n",
      "\t Params: tensor([  5.3609, -17.2661])\n",
      "\t Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch 3597, Loss 2.927778720855713\n",
      "\t Params: tensor([  5.3609, -17.2661])\n",
      "\t Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch 3598, Loss 2.927776575088501\n",
      "\t Params: tensor([  5.3609, -17.2662])\n",
      "\t Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch 3599, Loss 2.9277760982513428\n",
      "\t Params: tensor([  5.3609, -17.2663])\n",
      "\t Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch 3600, Loss 2.9277753829956055\n",
      "\t Params: tensor([  5.3609, -17.2663])\n",
      "\t Grad: tensor([-0.0012,  0.0065])\n",
      "Epoch 3601, Loss 2.9277756214141846\n",
      "\t Params: tensor([  5.3609, -17.2664])\n",
      "\t Grad: tensor([-0.0012,  0.0065])\n",
      "Epoch 3602, Loss 2.9277732372283936\n",
      "\t Params: tensor([  5.3609, -17.2665])\n",
      "\t Grad: tensor([-0.0012,  0.0065])\n",
      "Epoch 3603, Loss 2.9277749061584473\n",
      "\t Params: tensor([  5.3610, -17.2665])\n",
      "\t Grad: tensor([-0.0012,  0.0065])\n",
      "Epoch 3604, Loss 2.9277749061584473\n",
      "\t Params: tensor([  5.3610, -17.2666])\n",
      "\t Grad: tensor([-0.0011,  0.0065])\n",
      "Epoch 3605, Loss 2.927774667739868\n",
      "\t Params: tensor([  5.3610, -17.2667])\n",
      "\t Grad: tensor([-0.0011,  0.0065])\n",
      "Epoch 3606, Loss 2.9277732372283936\n",
      "\t Params: tensor([  5.3610, -17.2667])\n",
      "\t Grad: tensor([-0.0011,  0.0065])\n",
      "Epoch 3607, Loss 2.9277727603912354\n",
      "\t Params: tensor([  5.3610, -17.2668])\n",
      "\t Grad: tensor([-0.0011,  0.0065])\n",
      "Epoch 3608, Loss 2.9277725219726562\n",
      "\t Params: tensor([  5.3610, -17.2668])\n",
      "\t Grad: tensor([-0.0011,  0.0065])\n",
      "Epoch 3609, Loss 2.927772283554077\n",
      "\t Params: tensor([  5.3610, -17.2669])\n",
      "\t Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch 3610, Loss 2.927771806716919\n",
      "\t Params: tensor([  5.3610, -17.2670])\n",
      "\t Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch 3611, Loss 2.9277701377868652\n",
      "\t Params: tensor([  5.3611, -17.2670])\n",
      "\t Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch 3612, Loss 2.92777156829834\n",
      "\t Params: tensor([  5.3611, -17.2671])\n",
      "\t Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch 3613, Loss 2.9277710914611816\n",
      "\t Params: tensor([  5.3611, -17.2672])\n",
      "\t Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch 3614, Loss 2.927769660949707\n",
      "\t Params: tensor([  5.3611, -17.2672])\n",
      "\t Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch 3615, Loss 2.9277701377868652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params: tensor([  5.3611, -17.2673])\n",
      "\t Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch 3616, Loss 2.9277689456939697\n",
      "\t Params: tensor([  5.3611, -17.2674])\n",
      "\t Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch 3617, Loss 2.9277679920196533\n",
      "\t Params: tensor([  5.3611, -17.2674])\n",
      "\t Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch 3618, Loss 2.927769422531128\n",
      "\t Params: tensor([  5.3611, -17.2675])\n",
      "\t Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch 3619, Loss 2.9277684688568115\n",
      "\t Params: tensor([  5.3611, -17.2675])\n",
      "\t Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch 3620, Loss 2.9277665615081787\n",
      "\t Params: tensor([  5.3612, -17.2676])\n",
      "\t Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch 3621, Loss 2.927767276763916\n",
      "\t Params: tensor([  5.3612, -17.2677])\n",
      "\t Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch 3622, Loss 2.927767038345337\n",
      "\t Params: tensor([  5.3612, -17.2677])\n",
      "\t Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch 3623, Loss 2.927765130996704\n",
      "\t Params: tensor([  5.3612, -17.2678])\n",
      "\t Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch 3624, Loss 2.9277658462524414\n",
      "\t Params: tensor([  5.3612, -17.2679])\n",
      "\t Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch 3625, Loss 2.927765130996704\n",
      "\t Params: tensor([  5.3612, -17.2679])\n",
      "\t Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch 3626, Loss 2.927765130996704\n",
      "\t Params: tensor([  5.3612, -17.2680])\n",
      "\t Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch 3627, Loss 2.9277639389038086\n",
      "\t Params: tensor([  5.3612, -17.2681])\n",
      "\t Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch 3628, Loss 2.9277641773223877\n",
      "\t Params: tensor([  5.3612, -17.2681])\n",
      "\t Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch 3629, Loss 2.9277639389038086\n",
      "\t Params: tensor([  5.3613, -17.2682])\n",
      "\t Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch 3630, Loss 2.927762031555176\n",
      "\t Params: tensor([  5.3613, -17.2682])\n",
      "\t Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch 3631, Loss 2.927762746810913\n",
      "\t Params: tensor([  5.3613, -17.2683])\n",
      "\t Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch 3632, Loss 2.927762508392334\n",
      "\t Params: tensor([  5.3613, -17.2684])\n",
      "\t Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch 3633, Loss 2.927762031555176\n",
      "\t Params: tensor([  5.3613, -17.2684])\n",
      "\t Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch 3634, Loss 2.9277613162994385\n",
      "\t Params: tensor([  5.3613, -17.2685])\n",
      "\t Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch 3635, Loss 2.9277617931365967\n",
      "\t Params: tensor([  5.3613, -17.2685])\n",
      "\t Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch 3636, Loss 2.9277594089508057\n",
      "\t Params: tensor([  5.3613, -17.2686])\n",
      "\t Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch 3637, Loss 2.9277608394622803\n",
      "\t Params: tensor([  5.3613, -17.2687])\n",
      "\t Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch 3638, Loss 2.9277610778808594\n",
      "\t Params: tensor([  5.3614, -17.2687])\n",
      "\t Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch 3639, Loss 2.927760362625122\n",
      "\t Params: tensor([  5.3614, -17.2688])\n",
      "\t Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch 3640, Loss 2.9277594089508057\n",
      "\t Params: tensor([  5.3614, -17.2689])\n",
      "\t Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch 3641, Loss 2.92775821685791\n",
      "\t Params: tensor([  5.3614, -17.2689])\n",
      "\t Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch 3642, Loss 2.9277594089508057\n",
      "\t Params: tensor([  5.3614, -17.2690])\n",
      "\t Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch 3643, Loss 2.9277572631835938\n",
      "\t Params: tensor([  5.3614, -17.2690])\n",
      "\t Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch 3644, Loss 2.927757978439331\n",
      "\t Params: tensor([  5.3614, -17.2691])\n",
      "\t Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch 3645, Loss 2.9277572631835938\n",
      "\t Params: tensor([  5.3614, -17.2692])\n",
      "\t Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch 3646, Loss 2.9277570247650146\n",
      "\t Params: tensor([  5.3614, -17.2692])\n",
      "\t Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch 3647, Loss 2.9277565479278564\n",
      "\t Params: tensor([  5.3614, -17.2693])\n",
      "\t Grad: tensor([-0.0011,  0.0060])\n",
      "Epoch 3648, Loss 2.92775559425354\n",
      "\t Params: tensor([  5.3615, -17.2693])\n",
      "\t Grad: tensor([-0.0011,  0.0060])\n",
      "Epoch 3649, Loss 2.9277570247650146\n",
      "\t Params: tensor([  5.3615, -17.2694])\n",
      "\t Grad: tensor([-0.0011,  0.0060])\n",
      "Epoch 3650, Loss 2.92775559425354\n",
      "\t Params: tensor([  5.3615, -17.2695])\n",
      "\t Grad: tensor([-0.0011,  0.0060])\n",
      "Epoch 3651, Loss 2.92775559425354\n",
      "\t Params: tensor([  5.3615, -17.2695])\n",
      "\t Grad: tensor([-0.0010,  0.0060])\n",
      "Epoch 3652, Loss 2.927755117416382\n",
      "\t Params: tensor([  5.3615, -17.2696])\n",
      "\t Grad: tensor([-0.0010,  0.0060])\n",
      "Epoch 3653, Loss 2.927755117416382\n",
      "\t Params: tensor([  5.3615, -17.2696])\n",
      "\t Grad: tensor([-0.0010,  0.0060])\n",
      "Epoch 3654, Loss 2.9277541637420654\n",
      "\t Params: tensor([  5.3615, -17.2697])\n",
      "\t Grad: tensor([-0.0010,  0.0060])\n",
      "Epoch 3655, Loss 2.9277536869049072\n",
      "\t Params: tensor([  5.3615, -17.2698])\n",
      "\t Grad: tensor([-0.0010,  0.0060])\n",
      "Epoch 3656, Loss 2.9277548789978027\n",
      "\t Params: tensor([  5.3615, -17.2698])\n",
      "\t Grad: tensor([-0.0010,  0.0060])\n",
      "Epoch 3657, Loss 2.927753210067749\n",
      "\t Params: tensor([  5.3616, -17.2699])\n",
      "\t Grad: tensor([-0.0010,  0.0059])\n",
      "Epoch 3658, Loss 2.9277517795562744\n",
      "\t Params: tensor([  5.3616, -17.2699])\n",
      "\t Grad: tensor([-0.0010,  0.0059])\n",
      "Epoch 3659, Loss 2.9277536869049072\n",
      "\t Params: tensor([  5.3616, -17.2700])\n",
      "\t Grad: tensor([-0.0010,  0.0059])\n",
      "Epoch 3660, Loss 2.9277520179748535\n",
      "\t Params: tensor([  5.3616, -17.2701])\n",
      "\t Grad: tensor([-0.0010,  0.0059])\n",
      "Epoch 3661, Loss 2.927751302719116\n",
      "\t Params: tensor([  5.3616, -17.2701])\n",
      "\t Grad: tensor([-0.0010,  0.0059])\n",
      "Epoch 3662, Loss 2.9277520179748535\n",
      "\t Params: tensor([  5.3616, -17.2702])\n",
      "\t Grad: tensor([-0.0010,  0.0059])\n",
      "Epoch 3663, Loss 2.9277503490448\n",
      "\t Params: tensor([  5.3616, -17.2702])\n",
      "\t Grad: tensor([-0.0011,  0.0059])\n",
      "Epoch 3664, Loss 2.9277493953704834\n",
      "\t Params: tensor([  5.3616, -17.2703])\n",
      "\t Grad: tensor([-0.0010,  0.0059])\n",
      "Epoch 3665, Loss 2.927751302719116\n",
      "\t Params: tensor([  5.3616, -17.2703])\n",
      "\t Grad: tensor([-0.0010,  0.0059])\n",
      "Epoch 3666, Loss 2.9277496337890625\n",
      "\t Params: tensor([  5.3616, -17.2704])\n",
      "\t Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch 3667, Loss 2.9277503490448\n",
      "\t Params: tensor([  5.3617, -17.2705])\n",
      "\t Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch 3668, Loss 2.9277472496032715\n",
      "\t Params: tensor([  5.3617, -17.2705])\n",
      "\t Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch 3669, Loss 2.927748680114746\n",
      "\t Params: tensor([  5.3617, -17.2706])\n",
      "\t Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch 3670, Loss 2.9277474880218506\n",
      "\t Params: tensor([  5.3617, -17.2706])\n",
      "\t Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch 3671, Loss 2.927747964859009\n",
      "\t Params: tensor([  5.3617, -17.2707])\n",
      "\t Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch 3672, Loss 2.927748203277588\n",
      "\t Params: tensor([  5.3617, -17.2708])\n",
      "\t Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch 3673, Loss 2.9277472496032715\n",
      "\t Params: tensor([  5.3617, -17.2708])\n",
      "\t Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch 3674, Loss 2.9277467727661133\n",
      "\t Params: tensor([  5.3617, -17.2709])\n",
      "\t Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch 3675, Loss 2.927747964859009\n",
      "\t Params: tensor([  5.3617, -17.2709])\n",
      "\t Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch 3676, Loss 2.9277474880218506\n",
      "\t Params: tensor([  5.3617, -17.2710])\n",
      "\t Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch 3677, Loss 2.927746534347534\n",
      "\t Params: tensor([  5.3618, -17.2710])\n",
      "\t Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch 3678, Loss 2.9277451038360596\n",
      "\t Params: tensor([  5.3618, -17.2711])\n",
      "\t Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch 3679, Loss 2.9277451038360596\n",
      "\t Params: tensor([  5.3618, -17.2712])\n",
      "\t Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch 3680, Loss 2.9277455806732178\n",
      "\t Params: tensor([  5.3618, -17.2712])\n",
      "\t Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch 3681, Loss 2.927744150161743\n",
      "\t Params: tensor([  5.3618, -17.2713])\n",
      "\t Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch 3682, Loss 2.927743434906006\n",
      "\t Params: tensor([  5.3618, -17.2713])\n",
      "\t Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch 3683, Loss 2.9277427196502686\n",
      "\t Params: tensor([  5.3618, -17.2714])\n",
      "\t Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch 3684, Loss 2.9277427196502686\n",
      "\t Params: tensor([  5.3618, -17.2714])\n",
      "\t Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch 3685, Loss 2.927743434906006\n",
      "\t Params: tensor([  5.3618, -17.2715])\n",
      "\t Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch 3686, Loss 2.9277429580688477\n",
      "\t Params: tensor([  5.3618, -17.2716])\n",
      "\t Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch 3687, Loss 2.927744150161743\n",
      "\t Params: tensor([  5.3619, -17.2716])\n",
      "\t Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch 3688, Loss 2.9277422428131104\n",
      "\t Params: tensor([  5.3619, -17.2717])\n",
      "\t Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch 3689, Loss 2.9277422428131104\n",
      "\t Params: tensor([  5.3619, -17.2717])\n",
      "\t Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch 3690, Loss 2.927741765975952\n",
      "\t Params: tensor([  5.3619, -17.2718])\n",
      "\t Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch 3691, Loss 2.9277420043945312\n",
      "\t Params: tensor([  5.3619, -17.2718])\n",
      "\t Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch 3692, Loss 2.927741289138794\n",
      "\t Params: tensor([  5.3619, -17.2719])\n",
      "\t Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch 3693, Loss 2.9277405738830566\n",
      "\t Params: tensor([  5.3619, -17.2719])\n",
      "\t Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch 3694, Loss 2.927741050720215\n",
      "\t Params: tensor([  5.3619, -17.2720])\n",
      "\t Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch 3695, Loss 2.927741765975952\n",
      "\t Params: tensor([  5.3619, -17.2721])\n",
      "\t Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch 3696, Loss 2.9277405738830566\n",
      "\t Params: tensor([  5.3619, -17.2721])\n",
      "\t Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch 3697, Loss 2.9277405738830566\n",
      "\t Params: tensor([  5.3620, -17.2722])\n",
      "\t Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch 3698, Loss 2.9277398586273193\n",
      "\t Params: tensor([  5.3620, -17.2722])\n",
      "\t Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch 3699, Loss 2.927738904953003\n",
      "\t Params: tensor([  5.3620, -17.2723])\n",
      "\t Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch 3700, Loss 2.9277384281158447\n",
      "\t Params: tensor([  5.3620, -17.2723])\n",
      "\t Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch 3701, Loss 2.9277381896972656\n",
      "\t Params: tensor([  5.3620, -17.2724])\n",
      "\t Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch 3702, Loss 2.927737236022949\n",
      "\t Params: tensor([  5.3620, -17.2724])\n",
      "\t Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch 3703, Loss 2.927736759185791\n",
      "\t Params: tensor([  5.3620, -17.2725])\n",
      "\t Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch 3704, Loss 2.9277379512786865\n",
      "\t Params: tensor([  5.3620, -17.2726])\n",
      "\t Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch 3705, Loss 2.9277374744415283\n",
      "\t Params: tensor([  5.3620, -17.2726])\n",
      "\t Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch 3706, Loss 2.9277360439300537\n",
      "\t Params: tensor([  5.3620, -17.2727])\n",
      "\t Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch 3707, Loss 2.927736520767212\n",
      "\t Params: tensor([  5.3621, -17.2727])\n",
      "\t Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch 3708, Loss 2.927736520767212\n",
      "\t Params: tensor([  5.3621, -17.2728])\n",
      "\t Grad: tensor([-0.0010,  0.0054])\n",
      "Epoch 3709, Loss 2.9277358055114746\n",
      "\t Params: tensor([  5.3621, -17.2728])\n",
      "\t Grad: tensor([-0.0010,  0.0054])\n",
      "Epoch 3710, Loss 2.927734136581421\n",
      "\t Params: tensor([  5.3621, -17.2729])\n",
      "\t Grad: tensor([-0.0010,  0.0054])\n",
      "Epoch 3711, Loss 2.927734613418579\n",
      "\t Params: tensor([  5.3621, -17.2729])\n",
      "\t Grad: tensor([-0.0010,  0.0054])\n",
      "Epoch 3712, Loss 2.9277353286743164\n",
      "\t Params: tensor([  5.3621, -17.2730])\n",
      "\t Grad: tensor([-0.0009,  0.0054])\n",
      "Epoch 3713, Loss 2.927734375\n",
      "\t Params: tensor([  5.3621, -17.2730])\n",
      "\t Grad: tensor([-0.0009,  0.0054])\n",
      "Epoch 3714, Loss 2.9277336597442627\n",
      "\t Params: tensor([  5.3621, -17.2731])\n",
      "\t Grad: tensor([-0.0009,  0.0054])\n",
      "Epoch 3715, Loss 2.9277334213256836\n",
      "\t Params: tensor([  5.3621, -17.2732])\n",
      "\t Grad: tensor([-0.0009,  0.0054])\n",
      "Epoch 3716, Loss 2.927734136581421\n",
      "\t Params: tensor([  5.3621, -17.2732])\n",
      "\t Grad: tensor([-0.0009,  0.0054])\n",
      "Epoch 3717, Loss 2.9277329444885254\n",
      "\t Params: tensor([  5.3622, -17.2733])\n",
      "\t Grad: tensor([-0.0009,  0.0054])\n",
      "Epoch 3718, Loss 2.9277334213256836\n",
      "\t Params: tensor([  5.3622, -17.2733])\n",
      "\t Grad: tensor([-0.0009,  0.0054])\n",
      "Epoch 3719, Loss 2.9277327060699463\n",
      "\t Params: tensor([  5.3622, -17.2734])\n",
      "\t Grad: tensor([-0.0010,  0.0053])\n",
      "Epoch 3720, Loss 2.927732229232788\n",
      "\t Params: tensor([  5.3622, -17.2734])\n",
      "\t Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch 3721, Loss 2.9277312755584717\n",
      "\t Params: tensor([  5.3622, -17.2735])\n",
      "\t Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch 3722, Loss 2.9277307987213135\n",
      "\t Params: tensor([  5.3622, -17.2735])\n",
      "\t Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch 3723, Loss 2.9277327060699463\n",
      "\t Params: tensor([  5.3622, -17.2736])\n",
      "\t Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch 3724, Loss 2.9277303218841553\n",
      "\t Params: tensor([  5.3622, -17.2736])\n",
      "\t Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch 3725, Loss 2.927729845046997\n",
      "\t Params: tensor([  5.3622, -17.2737])\n",
      "\t Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch 3726, Loss 2.927731513977051\n",
      "\t Params: tensor([  5.3622, -17.2737])\n",
      "\t Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch 3727, Loss 2.9277303218841553\n",
      "\t Params: tensor([  5.3622, -17.2738])\n",
      "\t Grad: tensor([-0.0010,  0.0053])\n",
      "Epoch 3728, Loss 2.927731513977051\n",
      "\t Params: tensor([  5.3623, -17.2738])\n",
      "\t Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch 3729, Loss 2.9277312755584717\n",
      "\t Params: tensor([  5.3623, -17.2739])\n",
      "\t Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch 3730, Loss 2.9277303218841553\n",
      "\t Params: tensor([  5.3623, -17.2740])\n",
      "\t Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch 3731, Loss 2.9277284145355225\n",
      "\t Params: tensor([  5.3623, -17.2740])\n",
      "\t Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch 3732, Loss 2.9277291297912598\n",
      "\t Params: tensor([  5.3623, -17.2741])\n",
      "\t Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch 3733, Loss 2.9277291297912598\n",
      "\t Params: tensor([  5.3623, -17.2741])\n",
      "\t Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch 3734, Loss 2.9277284145355225\n",
      "\t Params: tensor([  5.3623, -17.2742])\n",
      "\t Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch 3735, Loss 2.927727699279785\n",
      "\t Params: tensor([  5.3623, -17.2742])\n",
      "\t Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch 3736, Loss 2.9277281761169434\n",
      "\t Params: tensor([  5.3623, -17.2743])\n",
      "\t Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch 3737, Loss 2.9277281761169434\n",
      "\t Params: tensor([  5.3623, -17.2743])\n",
      "\t Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch 3738, Loss 2.927727460861206\n",
      "\t Params: tensor([  5.3623, -17.2744])\n",
      "\t Grad: tensor([-0.0009,  0.0052])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3739, Loss 2.9277281761169434\n",
      "\t Params: tensor([  5.3624, -17.2744])\n",
      "\t Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch 3740, Loss 2.927727699279785\n",
      "\t Params: tensor([  5.3624, -17.2745])\n",
      "\t Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch 3741, Loss 2.9277265071868896\n",
      "\t Params: tensor([  5.3624, -17.2745])\n",
      "\t Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch 3742, Loss 2.927727460861206\n",
      "\t Params: tensor([  5.3624, -17.2746])\n",
      "\t Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch 3743, Loss 2.9277257919311523\n",
      "\t Params: tensor([  5.3624, -17.2746])\n",
      "\t Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch 3744, Loss 2.9277260303497314\n",
      "\t Params: tensor([  5.3624, -17.2747])\n",
      "\t Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch 3745, Loss 2.927725076675415\n",
      "\t Params: tensor([  5.3624, -17.2747])\n",
      "\t Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch 3746, Loss 2.927725076675415\n",
      "\t Params: tensor([  5.3624, -17.2748])\n",
      "\t Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch 3747, Loss 2.927725076675415\n",
      "\t Params: tensor([  5.3624, -17.2748])\n",
      "\t Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch 3748, Loss 2.9277231693267822\n",
      "\t Params: tensor([  5.3624, -17.2749])\n",
      "\t Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch 3749, Loss 2.9277243614196777\n",
      "\t Params: tensor([  5.3624, -17.2749])\n",
      "\t Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch 3750, Loss 2.9277236461639404\n",
      "\t Params: tensor([  5.3625, -17.2750])\n",
      "\t Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch 3751, Loss 2.927725315093994\n",
      "\t Params: tensor([  5.3625, -17.2750])\n",
      "\t Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch 3752, Loss 2.9277238845825195\n",
      "\t Params: tensor([  5.3625, -17.2751])\n",
      "\t Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch 3753, Loss 2.9277238845825195\n",
      "\t Params: tensor([  5.3625, -17.2751])\n",
      "\t Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch 3754, Loss 2.9277231693267822\n",
      "\t Params: tensor([  5.3625, -17.2752])\n",
      "\t Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch 3755, Loss 2.927722692489624\n",
      "\t Params: tensor([  5.3625, -17.2752])\n",
      "\t Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch 3756, Loss 2.927722692489624\n",
      "\t Params: tensor([  5.3625, -17.2753])\n",
      "\t Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch 3757, Loss 2.9277215003967285\n",
      "\t Params: tensor([  5.3625, -17.2753])\n",
      "\t Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch 3758, Loss 2.927722930908203\n",
      "\t Params: tensor([  5.3625, -17.2754])\n",
      "\t Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch 3759, Loss 2.9277215003967285\n",
      "\t Params: tensor([  5.3625, -17.2754])\n",
      "\t Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch 3760, Loss 2.927722692489624\n",
      "\t Params: tensor([  5.3625, -17.2755])\n",
      "\t Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch 3761, Loss 2.9277212619781494\n",
      "\t Params: tensor([  5.3626, -17.2755])\n",
      "\t Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch 3762, Loss 2.9277215003967285\n",
      "\t Params: tensor([  5.3626, -17.2756])\n",
      "\t Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch 3763, Loss 2.927719831466675\n",
      "\t Params: tensor([  5.3626, -17.2756])\n",
      "\t Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch 3764, Loss 2.927720069885254\n",
      "\t Params: tensor([  5.3626, -17.2757])\n",
      "\t Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch 3765, Loss 2.9277191162109375\n",
      "\t Params: tensor([  5.3626, -17.2757])\n",
      "\t Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch 3766, Loss 2.927720785140991\n",
      "\t Params: tensor([  5.3626, -17.2758])\n",
      "\t Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch 3767, Loss 2.9277193546295166\n",
      "\t Params: tensor([  5.3626, -17.2758])\n",
      "\t Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch 3768, Loss 2.9277191162109375\n",
      "\t Params: tensor([  5.3626, -17.2759])\n",
      "\t Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch 3769, Loss 2.9277191162109375\n",
      "\t Params: tensor([  5.3626, -17.2759])\n",
      "\t Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch 3770, Loss 2.9277188777923584\n",
      "\t Params: tensor([  5.3626, -17.2760])\n",
      "\t Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch 3771, Loss 2.9277188777923584\n",
      "\t Params: tensor([  5.3626, -17.2760])\n",
      "\t Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch 3772, Loss 2.927719831466675\n",
      "\t Params: tensor([  5.3626, -17.2761])\n",
      "\t Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch 3773, Loss 2.927718162536621\n",
      "\t Params: tensor([  5.3627, -17.2761])\n",
      "\t Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch 3774, Loss 2.9277184009552\n",
      "\t Params: tensor([  5.3627, -17.2762])\n",
      "\t Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch 3775, Loss 2.927717447280884\n",
      "\t Params: tensor([  5.3627, -17.2762])\n",
      "\t Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch 3776, Loss 2.927718162536621\n",
      "\t Params: tensor([  5.3627, -17.2763])\n",
      "\t Grad: tensor([-0.0008,  0.0049])\n",
      "Epoch 3777, Loss 2.9277167320251465\n",
      "\t Params: tensor([  5.3627, -17.2763])\n",
      "\t Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch 3778, Loss 2.9277169704437256\n",
      "\t Params: tensor([  5.3627, -17.2764])\n",
      "\t Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch 3779, Loss 2.927716016769409\n",
      "\t Params: tensor([  5.3627, -17.2764])\n",
      "\t Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch 3780, Loss 2.9277162551879883\n",
      "\t Params: tensor([  5.3627, -17.2765])\n",
      "\t Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch 3781, Loss 2.9277167320251465\n",
      "\t Params: tensor([  5.3627, -17.2765])\n",
      "\t Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch 3782, Loss 2.9277167320251465\n",
      "\t Params: tensor([  5.3627, -17.2766])\n",
      "\t Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch 3783, Loss 2.927716016769409\n",
      "\t Params: tensor([  5.3627, -17.2766])\n",
      "\t Grad: tensor([-0.0009,  0.0048])\n",
      "Epoch 3784, Loss 2.9277150630950928\n",
      "\t Params: tensor([  5.3627, -17.2767])\n",
      "\t Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch 3785, Loss 2.927715301513672\n",
      "\t Params: tensor([  5.3628, -17.2767])\n",
      "\t Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch 3786, Loss 2.927715301513672\n",
      "\t Params: tensor([  5.3628, -17.2767])\n",
      "\t Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch 3787, Loss 2.9277150630950928\n",
      "\t Params: tensor([  5.3628, -17.2768])\n",
      "\t Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch 3788, Loss 2.9277145862579346\n",
      "\t Params: tensor([  5.3628, -17.2768])\n",
      "\t Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch 3789, Loss 2.9277150630950928\n",
      "\t Params: tensor([  5.3628, -17.2769])\n",
      "\t Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3790, Loss 2.9277138710021973\n",
      "\t Params: tensor([  5.3628, -17.2769])\n",
      "\t Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3791, Loss 2.927713632583618\n",
      "\t Params: tensor([  5.3628, -17.2770])\n",
      "\t Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3792, Loss 2.9277138710021973\n",
      "\t Params: tensor([  5.3628, -17.2770])\n",
      "\t Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3793, Loss 2.927713632583618\n",
      "\t Params: tensor([  5.3628, -17.2771])\n",
      "\t Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3794, Loss 2.92771315574646\n",
      "\t Params: tensor([  5.3628, -17.2771])\n",
      "\t Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3795, Loss 2.9277143478393555\n",
      "\t Params: tensor([  5.3628, -17.2772])\n",
      "\t Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3796, Loss 2.927712917327881\n",
      "\t Params: tensor([  5.3629, -17.2772])\n",
      "\t Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3797, Loss 2.9277122020721436\n",
      "\t Params: tensor([  5.3629, -17.2773])\n",
      "\t Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3798, Loss 2.9277124404907227\n",
      "\t Params: tensor([  5.3629, -17.2773])\n",
      "\t Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3799, Loss 2.927712917327881\n",
      "\t Params: tensor([  5.3629, -17.2774])\n",
      "\t Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3800, Loss 2.927711248397827\n",
      "\t Params: tensor([  5.3629, -17.2774])\n",
      "\t Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3801, Loss 2.9277117252349854\n",
      "\t Params: tensor([  5.3629, -17.2775])\n",
      "\t Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3802, Loss 2.9277124404907227\n",
      "\t Params: tensor([  5.3629, -17.2775])\n",
      "\t Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch 3803, Loss 2.9277114868164062\n",
      "\t Params: tensor([  5.3629, -17.2775])\n",
      "\t Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch 3804, Loss 2.9277122020721436\n",
      "\t Params: tensor([  5.3629, -17.2776])\n",
      "\t Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch 3805, Loss 2.927710771560669\n",
      "\t Params: tensor([  5.3629, -17.2776])\n",
      "\t Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch 3806, Loss 2.92771053314209\n",
      "\t Params: tensor([  5.3629, -17.2777])\n",
      "\t Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch 3807, Loss 2.927710771560669\n",
      "\t Params: tensor([  5.3629, -17.2777])\n",
      "\t Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch 3808, Loss 2.9277093410491943\n",
      "\t Params: tensor([  5.3629, -17.2778])\n",
      "\t Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch 3809, Loss 2.9277100563049316\n",
      "\t Params: tensor([  5.3630, -17.2778])\n",
      "\t Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch 3810, Loss 2.9277100563049316\n",
      "\t Params: tensor([  5.3630, -17.2779])\n",
      "\t Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch 3811, Loss 2.927708387374878\n",
      "\t Params: tensor([  5.3630, -17.2779])\n",
      "\t Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch 3812, Loss 2.9277079105377197\n",
      "\t Params: tensor([  5.3630, -17.2780])\n",
      "\t Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch 3813, Loss 2.927708625793457\n",
      "\t Params: tensor([  5.3630, -17.2780])\n",
      "\t Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch 3814, Loss 2.9277093410491943\n",
      "\t Params: tensor([  5.3630, -17.2781])\n",
      "\t Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3815, Loss 2.9277098178863525\n",
      "\t Params: tensor([  5.3630, -17.2781])\n",
      "\t Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3816, Loss 2.9277079105377197\n",
      "\t Params: tensor([  5.3630, -17.2781])\n",
      "\t Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3817, Loss 2.9277079105377197\n",
      "\t Params: tensor([  5.3630, -17.2782])\n",
      "\t Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3818, Loss 2.927706241607666\n",
      "\t Params: tensor([  5.3630, -17.2782])\n",
      "\t Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3819, Loss 2.927706718444824\n",
      "\t Params: tensor([  5.3630, -17.2783])\n",
      "\t Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3820, Loss 2.9277076721191406\n",
      "\t Params: tensor([  5.3630, -17.2783])\n",
      "\t Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3821, Loss 2.9277074337005615\n",
      "\t Params: tensor([  5.3631, -17.2784])\n",
      "\t Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3822, Loss 2.9277074337005615\n",
      "\t Params: tensor([  5.3631, -17.2784])\n",
      "\t Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3823, Loss 2.927706718444824\n",
      "\t Params: tensor([  5.3631, -17.2785])\n",
      "\t Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3824, Loss 2.9277079105377197\n",
      "\t Params: tensor([  5.3631, -17.2785])\n",
      "\t Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3825, Loss 2.9277074337005615\n",
      "\t Params: tensor([  5.3631, -17.2786])\n",
      "\t Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3826, Loss 2.927706003189087\n",
      "\t Params: tensor([  5.3631, -17.2786])\n",
      "\t Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3827, Loss 2.9277069568634033\n",
      "\t Params: tensor([  5.3631, -17.2786])\n",
      "\t Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3828, Loss 2.9277052879333496\n",
      "\t Params: tensor([  5.3631, -17.2787])\n",
      "\t Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3829, Loss 2.927706003189087\n",
      "\t Params: tensor([  5.3631, -17.2787])\n",
      "\t Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3830, Loss 2.927706003189087\n",
      "\t Params: tensor([  5.3631, -17.2788])\n",
      "\t Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3831, Loss 2.9277052879333496\n",
      "\t Params: tensor([  5.3631, -17.2788])\n",
      "\t Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3832, Loss 2.9277048110961914\n",
      "\t Params: tensor([  5.3631, -17.2789])\n",
      "\t Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3833, Loss 2.9277052879333496\n",
      "\t Params: tensor([  5.3631, -17.2789])\n",
      "\t Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3834, Loss 2.9277048110961914\n",
      "\t Params: tensor([  5.3632, -17.2789])\n",
      "\t Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3835, Loss 2.9277048110961914\n",
      "\t Params: tensor([  5.3632, -17.2790])\n",
      "\t Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3836, Loss 2.9277045726776123\n",
      "\t Params: tensor([  5.3632, -17.2790])\n",
      "\t Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3837, Loss 2.927703619003296\n",
      "\t Params: tensor([  5.3632, -17.2791])\n",
      "\t Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3838, Loss 2.927703619003296\n",
      "\t Params: tensor([  5.3632, -17.2791])\n",
      "\t Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3839, Loss 2.927703619003296\n",
      "\t Params: tensor([  5.3632, -17.2792])\n",
      "\t Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3840, Loss 2.9277031421661377\n",
      "\t Params: tensor([  5.3632, -17.2792])\n",
      "\t Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3841, Loss 2.9277021884918213\n",
      "\t Params: tensor([  5.3632, -17.2793])\n",
      "\t Grad: tensor([-0.0008,  0.0043])\n",
      "Epoch 3842, Loss 2.9277029037475586\n",
      "\t Params: tensor([  5.3632, -17.2793])\n",
      "\t Grad: tensor([-0.0008,  0.0043])\n",
      "Epoch 3843, Loss 2.9277029037475586\n",
      "\t Params: tensor([  5.3632, -17.2793])\n",
      "\t Grad: tensor([-0.0008,  0.0043])\n",
      "Epoch 3844, Loss 2.927703619003296\n",
      "\t Params: tensor([  5.3632, -17.2794])\n",
      "\t Grad: tensor([-0.0008,  0.0043])\n",
      "Epoch 3845, Loss 2.9277024269104004\n",
      "\t Params: tensor([  5.3632, -17.2794])\n",
      "\t Grad: tensor([-0.0008,  0.0043])\n",
      "Epoch 3846, Loss 2.9277007579803467\n",
      "\t Params: tensor([  5.3632, -17.2795])\n",
      "\t Grad: tensor([-0.0008,  0.0043])\n",
      "Epoch 3847, Loss 2.9277029037475586\n",
      "\t Params: tensor([  5.3633, -17.2795])\n",
      "\t Grad: tensor([-0.0008,  0.0043])\n",
      "Epoch 3848, Loss 2.927701711654663\n",
      "\t Params: tensor([  5.3633, -17.2796])\n",
      "\t Grad: tensor([-0.0008,  0.0043])\n",
      "Epoch 3849, Loss 2.927700996398926\n",
      "\t Params: tensor([  5.3633, -17.2796])\n",
      "\t Grad: tensor([-0.0007,  0.0043])\n",
      "Epoch 3850, Loss 2.927700996398926\n",
      "\t Params: tensor([  5.3633, -17.2796])\n",
      "\t Grad: tensor([-0.0007,  0.0043])\n",
      "Epoch 3851, Loss 2.9277029037475586\n",
      "\t Params: tensor([  5.3633, -17.2797])\n",
      "\t Grad: tensor([-0.0007,  0.0043])\n",
      "Epoch 3852, Loss 2.9277002811431885\n",
      "\t Params: tensor([  5.3633, -17.2797])\n",
      "\t Grad: tensor([-0.0007,  0.0043])\n",
      "Epoch 3853, Loss 2.927700996398926\n",
      "\t Params: tensor([  5.3633, -17.2798])\n",
      "\t Grad: tensor([-0.0007,  0.0043])\n",
      "Epoch 3854, Loss 2.927700996398926\n",
      "\t Params: tensor([  5.3633, -17.2798])\n",
      "\t Grad: tensor([-0.0007,  0.0043])\n",
      "Epoch 3855, Loss 2.9277000427246094\n",
      "\t Params: tensor([  5.3633, -17.2799])\n",
      "\t Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3856, Loss 2.9277000427246094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params: tensor([  5.3633, -17.2799])\n",
      "\t Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3857, Loss 2.9276998043060303\n",
      "\t Params: tensor([  5.3633, -17.2799])\n",
      "\t Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3858, Loss 2.9277007579803467\n",
      "\t Params: tensor([  5.3633, -17.2800])\n",
      "\t Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3859, Loss 2.927699327468872\n",
      "\t Params: tensor([  5.3633, -17.2800])\n",
      "\t Grad: tensor([-0.0008,  0.0042])\n",
      "Epoch 3860, Loss 2.927699327468872\n",
      "\t Params: tensor([  5.3634, -17.2801])\n",
      "\t Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3861, Loss 2.9277002811431885\n",
      "\t Params: tensor([  5.3634, -17.2801])\n",
      "\t Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3862, Loss 2.927699327468872\n",
      "\t Params: tensor([  5.3634, -17.2801])\n",
      "\t Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3863, Loss 2.9276983737945557\n",
      "\t Params: tensor([  5.3634, -17.2802])\n",
      "\t Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3864, Loss 2.927699327468872\n",
      "\t Params: tensor([  5.3634, -17.2802])\n",
      "\t Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3865, Loss 2.927696943283081\n",
      "\t Params: tensor([  5.3634, -17.2803])\n",
      "\t Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3866, Loss 2.9277000427246094\n",
      "\t Params: tensor([  5.3634, -17.2803])\n",
      "\t Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3867, Loss 2.927699327468872\n",
      "\t Params: tensor([  5.3634, -17.2804])\n",
      "\t Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3868, Loss 2.9276976585388184\n",
      "\t Params: tensor([  5.3634, -17.2804])\n",
      "\t Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3869, Loss 2.927696943283081\n",
      "\t Params: tensor([  5.3634, -17.2804])\n",
      "\t Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3870, Loss 2.9276983737945557\n",
      "\t Params: tensor([  5.3634, -17.2805])\n",
      "\t Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3871, Loss 2.9276955127716064\n",
      "\t Params: tensor([  5.3634, -17.2805])\n",
      "\t Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3872, Loss 2.9276986122131348\n",
      "\t Params: tensor([  5.3634, -17.2806])\n",
      "\t Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3873, Loss 2.9276976585388184\n",
      "\t Params: tensor([  5.3634, -17.2806])\n",
      "\t Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3874, Loss 2.9276962280273438\n",
      "\t Params: tensor([  5.3635, -17.2806])\n",
      "\t Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3875, Loss 2.9276983737945557\n",
      "\t Params: tensor([  5.3635, -17.2807])\n",
      "\t Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3876, Loss 2.92769718170166\n",
      "\t Params: tensor([  5.3635, -17.2807])\n",
      "\t Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3877, Loss 2.9276962280273438\n",
      "\t Params: tensor([  5.3635, -17.2808])\n",
      "\t Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3878, Loss 2.92769718170166\n",
      "\t Params: tensor([  5.3635, -17.2808])\n",
      "\t Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3879, Loss 2.927696466445923\n",
      "\t Params: tensor([  5.3635, -17.2808])\n",
      "\t Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3880, Loss 2.9276959896087646\n",
      "\t Params: tensor([  5.3635, -17.2809])\n",
      "\t Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3881, Loss 2.9276959896087646\n",
      "\t Params: tensor([  5.3635, -17.2809])\n",
      "\t Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3882, Loss 2.9276955127716064\n",
      "\t Params: tensor([  5.3635, -17.2810])\n",
      "\t Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3883, Loss 2.9276955127716064\n",
      "\t Params: tensor([  5.3635, -17.2810])\n",
      "\t Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3884, Loss 2.92769455909729\n",
      "\t Params: tensor([  5.3635, -17.2810])\n",
      "\t Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3885, Loss 2.927696466445923\n",
      "\t Params: tensor([  5.3635, -17.2811])\n",
      "\t Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3886, Loss 2.9276959896087646\n",
      "\t Params: tensor([  5.3635, -17.2811])\n",
      "\t Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3887, Loss 2.92769455909729\n",
      "\t Params: tensor([  5.3635, -17.2812])\n",
      "\t Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3888, Loss 2.92769455909729\n",
      "\t Params: tensor([  5.3636, -17.2812])\n",
      "\t Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3889, Loss 2.927694082260132\n",
      "\t Params: tensor([  5.3636, -17.2812])\n",
      "\t Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3890, Loss 2.9276933670043945\n",
      "\t Params: tensor([  5.3636, -17.2813])\n",
      "\t Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3891, Loss 2.9276926517486572\n",
      "\t Params: tensor([  5.3636, -17.2813])\n",
      "\t Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3892, Loss 2.9276952743530273\n",
      "\t Params: tensor([  5.3636, -17.2814])\n",
      "\t Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3893, Loss 2.927694797515869\n",
      "\t Params: tensor([  5.3636, -17.2814])\n",
      "\t Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3894, Loss 2.9276938438415527\n",
      "\t Params: tensor([  5.3636, -17.2815])\n",
      "\t Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3895, Loss 2.9276955127716064\n",
      "\t Params: tensor([  5.3636, -17.2815])\n",
      "\t Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3896, Loss 2.9276926517486572\n",
      "\t Params: tensor([  5.3636, -17.2815])\n",
      "\t Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3897, Loss 2.9276926517486572\n",
      "\t Params: tensor([  5.3636, -17.2816])\n",
      "\t Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3898, Loss 2.927694082260132\n",
      "\t Params: tensor([  5.3636, -17.2816])\n",
      "\t Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3899, Loss 2.9276931285858154\n",
      "\t Params: tensor([  5.3636, -17.2817])\n",
      "\t Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3900, Loss 2.927692174911499\n",
      "\t Params: tensor([  5.3636, -17.2817])\n",
      "\t Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3901, Loss 2.927694082260132\n",
      "\t Params: tensor([  5.3636, -17.2817])\n",
      "\t Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3902, Loss 2.927692413330078\n",
      "\t Params: tensor([  5.3637, -17.2818])\n",
      "\t Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3903, Loss 2.9276933670043945\n",
      "\t Params: tensor([  5.3637, -17.2818])\n",
      "\t Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3904, Loss 2.9276914596557617\n",
      "\t Params: tensor([  5.3637, -17.2818])\n",
      "\t Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3905, Loss 2.927692174911499\n",
      "\t Params: tensor([  5.3637, -17.2819])\n",
      "\t Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3906, Loss 2.927692174911499\n",
      "\t Params: tensor([  5.3637, -17.2819])\n",
      "\t Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3907, Loss 2.927692413330078\n",
      "\t Params: tensor([  5.3637, -17.2820])\n",
      "\t Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3908, Loss 2.9276914596557617\n",
      "\t Params: tensor([  5.3637, -17.2820])\n",
      "\t Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3909, Loss 2.927692174911499\n",
      "\t Params: tensor([  5.3637, -17.2820])\n",
      "\t Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3910, Loss 2.927690267562866\n",
      "\t Params: tensor([  5.3637, -17.2821])\n",
      "\t Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3911, Loss 2.9276914596557617\n",
      "\t Params: tensor([  5.3637, -17.2821])\n",
      "\t Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3912, Loss 2.9276907444000244\n",
      "\t Params: tensor([  5.3637, -17.2822])\n",
      "\t Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3913, Loss 2.9276914596557617\n",
      "\t Params: tensor([  5.3637, -17.2822])\n",
      "\t Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3914, Loss 2.927689552307129\n",
      "\t Params: tensor([  5.3637, -17.2822])\n",
      "\t Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3915, Loss 2.9276909828186035\n",
      "\t Params: tensor([  5.3637, -17.2823])\n",
      "\t Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3916, Loss 2.9276907444000244\n",
      "\t Params: tensor([  5.3637, -17.2823])\n",
      "\t Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3917, Loss 2.92768931388855\n",
      "\t Params: tensor([  5.3638, -17.2823])\n",
      "\t Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3918, Loss 2.927690267562866\n",
      "\t Params: tensor([  5.3638, -17.2824])\n",
      "\t Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3919, Loss 2.927689552307129\n",
      "\t Params: tensor([  5.3638, -17.2824])\n",
      "\t Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3920, Loss 2.927689552307129\n",
      "\t Params: tensor([  5.3638, -17.2825])\n",
      "\t Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3921, Loss 2.927689552307129\n",
      "\t Params: tensor([  5.3638, -17.2825])\n",
      "\t Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3922, Loss 2.9276888370513916\n",
      "\t Params: tensor([  5.3638, -17.2825])\n",
      "\t Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3923, Loss 2.9276885986328125\n",
      "\t Params: tensor([  5.3638, -17.2826])\n",
      "\t Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3924, Loss 2.9276888370513916\n",
      "\t Params: tensor([  5.3638, -17.2826])\n",
      "\t Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3925, Loss 2.927687644958496\n",
      "\t Params: tensor([  5.3638, -17.2826])\n",
      "\t Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3926, Loss 2.9276885986328125\n",
      "\t Params: tensor([  5.3638, -17.2827])\n",
      "\t Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3927, Loss 2.92768931388855\n",
      "\t Params: tensor([  5.3638, -17.2827])\n",
      "\t Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3928, Loss 2.9276883602142334\n",
      "\t Params: tensor([  5.3638, -17.2828])\n",
      "\t Grad: tensor([-0.0007,  0.0037])\n",
      "Epoch 3929, Loss 2.9276883602142334\n",
      "\t Params: tensor([  5.3638, -17.2828])\n",
      "\t Grad: tensor([-0.0007,  0.0037])\n",
      "Epoch 3930, Loss 2.927687883377075\n",
      "\t Params: tensor([  5.3638, -17.2828])\n",
      "\t Grad: tensor([-0.0007,  0.0037])\n",
      "Epoch 3931, Loss 2.927687644958496\n",
      "\t Params: tensor([  5.3638, -17.2829])\n",
      "\t Grad: tensor([-0.0007,  0.0037])\n",
      "Epoch 3932, Loss 2.927686929702759\n",
      "\t Params: tensor([  5.3639, -17.2829])\n",
      "\t Grad: tensor([-0.0007,  0.0037])\n",
      "Epoch 3933, Loss 2.9276888370513916\n",
      "\t Params: tensor([  5.3639, -17.2829])\n",
      "\t Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch 3934, Loss 2.927687883377075\n",
      "\t Params: tensor([  5.3639, -17.2830])\n",
      "\t Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch 3935, Loss 2.927686929702759\n",
      "\t Params: tensor([  5.3639, -17.2830])\n",
      "\t Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch 3936, Loss 2.927687168121338\n",
      "\t Params: tensor([  5.3639, -17.2831])\n",
      "\t Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch 3937, Loss 2.927686929702759\n",
      "\t Params: tensor([  5.3639, -17.2831])\n",
      "\t Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch 3938, Loss 2.9276864528656006\n",
      "\t Params: tensor([  5.3639, -17.2831])\n",
      "\t Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch 3939, Loss 2.9276864528656006\n",
      "\t Params: tensor([  5.3639, -17.2832])\n",
      "\t Grad: tensor([-0.0007,  0.0037])\n",
      "Epoch 3940, Loss 2.927686929702759\n",
      "\t Params: tensor([  5.3639, -17.2832])\n",
      "\t Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch 3941, Loss 2.9276864528656006\n",
      "\t Params: tensor([  5.3639, -17.2832])\n",
      "\t Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch 3942, Loss 2.9276862144470215\n",
      "\t Params: tensor([  5.3639, -17.2833])\n",
      "\t Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch 3943, Loss 2.9276864528656006\n",
      "\t Params: tensor([  5.3639, -17.2833])\n",
      "\t Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch 3944, Loss 2.9276862144470215\n",
      "\t Params: tensor([  5.3639, -17.2833])\n",
      "\t Grad: tensor([-0.0007,  0.0036])\n",
      "Epoch 3945, Loss 2.927685022354126\n",
      "\t Params: tensor([  5.3639, -17.2834])\n",
      "\t Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch 3946, Loss 2.927685499191284\n",
      "\t Params: tensor([  5.3639, -17.2834])\n",
      "\t Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch 3947, Loss 2.9276857376098633\n",
      "\t Params: tensor([  5.3640, -17.2835])\n",
      "\t Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch 3948, Loss 2.927685499191284\n",
      "\t Params: tensor([  5.3640, -17.2835])\n",
      "\t Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch 3949, Loss 2.9276864528656006\n",
      "\t Params: tensor([  5.3640, -17.2835])\n",
      "\t Grad: tensor([-0.0007,  0.0036])\n",
      "Epoch 3950, Loss 2.9276864528656006\n",
      "\t Params: tensor([  5.3640, -17.2836])\n",
      "\t Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch 3951, Loss 2.927686929702759\n",
      "\t Params: tensor([  5.3640, -17.2836])\n",
      "\t Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch 3952, Loss 2.927685022354126\n",
      "\t Params: tensor([  5.3640, -17.2836])\n",
      "\t Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch 3953, Loss 2.9276857376098633\n",
      "\t Params: tensor([  5.3640, -17.2837])\n",
      "\t Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch 3954, Loss 2.9276857376098633\n",
      "\t Params: tensor([  5.3640, -17.2837])\n",
      "\t Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch 3955, Loss 2.9276845455169678\n",
      "\t Params: tensor([  5.3640, -17.2837])\n",
      "\t Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch 3956, Loss 2.927683115005493\n",
      "\t Params: tensor([  5.3640, -17.2838])\n",
      "\t Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch 3957, Loss 2.9276838302612305\n",
      "\t Params: tensor([  5.3640, -17.2838])\n",
      "\t Grad: tensor([-0.0007,  0.0036])\n",
      "Epoch 3958, Loss 2.927684783935547\n",
      "\t Params: tensor([  5.3640, -17.2839])\n",
      "\t Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch 3959, Loss 2.9276840686798096\n",
      "\t Params: tensor([  5.3640, -17.2839])\n",
      "\t Grad: tensor([-0.0007,  0.0036])\n",
      "Epoch 3960, Loss 2.9276840686798096\n",
      "\t Params: tensor([  5.3640, -17.2839])\n",
      "\t Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3961, Loss 2.9276840686798096\n",
      "\t Params: tensor([  5.3640, -17.2840])\n",
      "\t Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3962, Loss 2.927685499191284\n",
      "\t Params: tensor([  5.3640, -17.2840])\n",
      "\t Grad: tensor([-0.0007,  0.0035])\n",
      "Epoch 3963, Loss 2.9276833534240723\n",
      "\t Params: tensor([  5.3641, -17.2840])\n",
      "\t Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3964, Loss 2.927684783935547\n",
      "\t Params: tensor([  5.3641, -17.2841])\n",
      "\t Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3965, Loss 2.927684783935547\n",
      "\t Params: tensor([  5.3641, -17.2841])\n",
      "\t Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3966, Loss 2.9276840686798096\n",
      "\t Params: tensor([  5.3641, -17.2841])\n",
      "\t Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3967, Loss 2.927682638168335\n",
      "\t Params: tensor([  5.3641, -17.2842])\n",
      "\t Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3968, Loss 2.927682638168335\n",
      "\t Params: tensor([  5.3641, -17.2842])\n",
      "\t Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3969, Loss 2.927682399749756\n",
      "\t Params: tensor([  5.3641, -17.2842])\n",
      "\t Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3970, Loss 2.927683115005493\n",
      "\t Params: tensor([  5.3641, -17.2843])\n",
      "\t Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3971, Loss 2.9276838302612305\n",
      "\t Params: tensor([  5.3641, -17.2843])\n",
      "\t Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3972, Loss 2.9276819229125977\n",
      "\t Params: tensor([  5.3641, -17.2843])\n",
      "\t Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3973, Loss 2.927683115005493\n",
      "\t Params: tensor([  5.3641, -17.2844])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3974, Loss 2.9276833534240723\n",
      "\t Params: tensor([  5.3641, -17.2844])\n",
      "\t Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3975, Loss 2.927683115005493\n",
      "\t Params: tensor([  5.3641, -17.2844])\n",
      "\t Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3976, Loss 2.927682399749756\n",
      "\t Params: tensor([  5.3641, -17.2845])\n",
      "\t Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3977, Loss 2.9276819229125977\n",
      "\t Params: tensor([  5.3641, -17.2845])\n",
      "\t Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3978, Loss 2.9276816844940186\n",
      "\t Params: tensor([  5.3641, -17.2845])\n",
      "\t Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3979, Loss 2.9276809692382812\n",
      "\t Params: tensor([  5.3642, -17.2846])\n",
      "\t Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3980, Loss 2.9276816844940186\n",
      "\t Params: tensor([  5.3642, -17.2846])\n",
      "\t Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3981, Loss 2.9276819229125977\n",
      "\t Params: tensor([  5.3642, -17.2847])\n",
      "\t Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3982, Loss 2.9276816844940186\n",
      "\t Params: tensor([  5.3642, -17.2847])\n",
      "\t Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3983, Loss 2.9276816844940186\n",
      "\t Params: tensor([  5.3642, -17.2847])\n",
      "\t Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3984, Loss 2.927682399749756\n",
      "\t Params: tensor([  5.3642, -17.2848])\n",
      "\t Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3985, Loss 2.927682399749756\n",
      "\t Params: tensor([  5.3642, -17.2848])\n",
      "\t Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3986, Loss 2.927680253982544\n",
      "\t Params: tensor([  5.3642, -17.2848])\n",
      "\t Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3987, Loss 2.9276819229125977\n",
      "\t Params: tensor([  5.3642, -17.2849])\n",
      "\t Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3988, Loss 2.9276809692382812\n",
      "\t Params: tensor([  5.3642, -17.2849])\n",
      "\t Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3989, Loss 2.9276812076568604\n",
      "\t Params: tensor([  5.3642, -17.2849])\n",
      "\t Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3990, Loss 2.9276795387268066\n",
      "\t Params: tensor([  5.3642, -17.2850])\n",
      "\t Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3991, Loss 2.9276809692382812\n",
      "\t Params: tensor([  5.3642, -17.2850])\n",
      "\t Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3992, Loss 2.927680015563965\n",
      "\t Params: tensor([  5.3642, -17.2850])\n",
      "\t Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3993, Loss 2.927680015563965\n",
      "\t Params: tensor([  5.3642, -17.2851])\n",
      "\t Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3994, Loss 2.927680730819702\n",
      "\t Params: tensor([  5.3642, -17.2851])\n",
      "\t Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 3995, Loss 2.9276809692382812\n",
      "\t Params: tensor([  5.3642, -17.2851])\n",
      "\t Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 3996, Loss 2.9276788234710693\n",
      "\t Params: tensor([  5.3643, -17.2852])\n",
      "\t Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 3997, Loss 2.927680253982544\n",
      "\t Params: tensor([  5.3643, -17.2852])\n",
      "\t Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 3998, Loss 2.9276785850524902\n",
      "\t Params: tensor([  5.3643, -17.2852])\n",
      "\t Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 3999, Loss 2.927680253982544\n",
      "\t Params: tensor([  5.3643, -17.2853])\n",
      "\t Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4000, Loss 2.927680253982544\n",
      "\t Params: tensor([  5.3643, -17.2853])\n",
      "\t Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4001, Loss 2.927680730819702\n",
      "\t Params: tensor([  5.3643, -17.2853])\n",
      "\t Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4002, Loss 2.9276788234710693\n",
      "\t Params: tensor([  5.3643, -17.2854])\n",
      "\t Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4003, Loss 2.9276788234710693\n",
      "\t Params: tensor([  5.3643, -17.2854])\n",
      "\t Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4004, Loss 2.927680015563965\n",
      "\t Params: tensor([  5.3643, -17.2854])\n",
      "\t Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4005, Loss 2.9276795387268066\n",
      "\t Params: tensor([  5.3643, -17.2855])\n",
      "\t Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4006, Loss 2.9276771545410156\n",
      "\t Params: tensor([  5.3643, -17.2855])\n",
      "\t Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4007, Loss 2.927677869796753\n",
      "\t Params: tensor([  5.3643, -17.2855])\n",
      "\t Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4008, Loss 2.9276785850524902\n",
      "\t Params: tensor([  5.3643, -17.2856])\n",
      "\t Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4009, Loss 2.927678108215332\n",
      "\t Params: tensor([  5.3643, -17.2856])\n",
      "\t Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4010, Loss 2.9276785850524902\n",
      "\t Params: tensor([  5.3643, -17.2856])\n",
      "\t Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4011, Loss 2.9276785850524902\n",
      "\t Params: tensor([  5.3643, -17.2857])\n",
      "\t Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4012, Loss 2.9276788234710693\n",
      "\t Params: tensor([  5.3643, -17.2857])\n",
      "\t Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4013, Loss 2.9276769161224365\n",
      "\t Params: tensor([  5.3644, -17.2857])\n",
      "\t Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4014, Loss 2.9276769161224365\n",
      "\t Params: tensor([  5.3644, -17.2857])\n",
      "\t Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4015, Loss 2.9276769161224365\n",
      "\t Params: tensor([  5.3644, -17.2858])\n",
      "\t Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4016, Loss 2.9276788234710693\n",
      "\t Params: tensor([  5.3644, -17.2858])\n",
      "\t Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4017, Loss 2.9276773929595947\n",
      "\t Params: tensor([  5.3644, -17.2858])\n",
      "\t Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4018, Loss 2.927677869796753\n",
      "\t Params: tensor([  5.3644, -17.2859])\n",
      "\t Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4019, Loss 2.927677869796753\n",
      "\t Params: tensor([  5.3644, -17.2859])\n",
      "\t Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4020, Loss 2.9276773929595947\n",
      "\t Params: tensor([  5.3644, -17.2859])\n",
      "\t Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4021, Loss 2.9276771545410156\n",
      "\t Params: tensor([  5.3644, -17.2860])\n",
      "\t Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4022, Loss 2.927677869796753\n",
      "\t Params: tensor([  5.3644, -17.2860])\n",
      "\t Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4023, Loss 2.9276771545410156\n",
      "\t Params: tensor([  5.3644, -17.2860])\n",
      "\t Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4024, Loss 2.9276769161224365\n",
      "\t Params: tensor([  5.3644, -17.2861])\n",
      "\t Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4025, Loss 2.9276764392852783\n",
      "\t Params: tensor([  5.3644, -17.2861])\n",
      "\t Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4026, Loss 2.9276764392852783\n",
      "\t Params: tensor([  5.3644, -17.2861])\n",
      "\t Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4027, Loss 2.927675485610962\n",
      "\t Params: tensor([  5.3644, -17.2862])\n",
      "\t Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4028, Loss 2.9276769161224365\n",
      "\t Params: tensor([  5.3644, -17.2862])\n",
      "\t Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4029, Loss 2.9276742935180664\n",
      "\t Params: tensor([  5.3644, -17.2862])\n",
      "\t Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4030, Loss 2.9276764392852783\n",
      "\t Params: tensor([  5.3644, -17.2863])\n",
      "\t Grad: tensor([-0.0006,  0.0031])\n",
      "Epoch 4031, Loss 2.9276750087738037\n",
      "\t Params: tensor([  5.3645, -17.2863])\n",
      "\t Grad: tensor([-0.0006,  0.0031])\n",
      "Epoch 4032, Loss 2.927675485610962\n",
      "\t Params: tensor([  5.3645, -17.2863])\n",
      "\t Grad: tensor([-0.0006,  0.0031])\n",
      "Epoch 4033, Loss 2.927675485610962\n",
      "\t Params: tensor([  5.3645, -17.2864])\n",
      "\t Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4034, Loss 2.927675485610962\n",
      "\t Params: tensor([  5.3645, -17.2864])\n",
      "\t Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4035, Loss 2.9276742935180664\n",
      "\t Params: tensor([  5.3645, -17.2864])\n",
      "\t Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4036, Loss 2.9276742935180664\n",
      "\t Params: tensor([  5.3645, -17.2865])\n",
      "\t Grad: tensor([-0.0006,  0.0031])\n",
      "Epoch 4037, Loss 2.9276769161224365\n",
      "\t Params: tensor([  5.3645, -17.2865])\n",
      "\t Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4038, Loss 2.9276742935180664\n",
      "\t Params: tensor([  5.3645, -17.2865])\n",
      "\t Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4039, Loss 2.927675724029541\n",
      "\t Params: tensor([  5.3645, -17.2865])\n",
      "\t Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4040, Loss 2.9276750087738037\n",
      "\t Params: tensor([  5.3645, -17.2866])\n",
      "\t Grad: tensor([-0.0006,  0.0031])\n",
      "Epoch 4041, Loss 2.9276747703552246\n",
      "\t Params: tensor([  5.3645, -17.2866])\n",
      "\t Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4042, Loss 2.9276750087738037\n",
      "\t Params: tensor([  5.3645, -17.2866])\n",
      "\t Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4043, Loss 2.9276740550994873\n",
      "\t Params: tensor([  5.3645, -17.2867])\n",
      "\t Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4044, Loss 2.927673578262329\n",
      "\t Params: tensor([  5.3645, -17.2867])\n",
      "\t Grad: tensor([-0.0006,  0.0031])\n",
      "Epoch 4045, Loss 2.9276747703552246\n",
      "\t Params: tensor([  5.3645, -17.2867])\n",
      "\t Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4046, Loss 2.9276742935180664\n",
      "\t Params: tensor([  5.3645, -17.2868])\n",
      "\t Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4047, Loss 2.927673578262329\n",
      "\t Params: tensor([  5.3645, -17.2868])\n",
      "\t Grad: tensor([-0.0006,  0.0031])\n",
      "Epoch 4048, Loss 2.9276747703552246\n",
      "\t Params: tensor([  5.3645, -17.2868])\n",
      "\t Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4049, Loss 2.9276723861694336\n",
      "\t Params: tensor([  5.3646, -17.2868])\n",
      "\t Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4050, Loss 2.9276747703552246\n",
      "\t Params: tensor([  5.3646, -17.2869])\n",
      "\t Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4051, Loss 2.9276750087738037\n",
      "\t Params: tensor([  5.3646, -17.2869])\n",
      "\t Grad: tensor([-0.0006,  0.0030])\n",
      "Epoch 4052, Loss 2.927673101425171\n",
      "\t Params: tensor([  5.3646, -17.2869])\n",
      "\t Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4053, Loss 2.92767333984375\n",
      "\t Params: tensor([  5.3646, -17.2870])\n",
      "\t Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4054, Loss 2.9276742935180664\n",
      "\t Params: tensor([  5.3646, -17.2870])\n",
      "\t Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4055, Loss 2.927673101425171\n",
      "\t Params: tensor([  5.3646, -17.2870])\n",
      "\t Grad: tensor([-0.0006,  0.0030])\n",
      "Epoch 4056, Loss 2.927673578262329\n",
      "\t Params: tensor([  5.3646, -17.2871])\n",
      "\t Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4057, Loss 2.9276723861694336\n",
      "\t Params: tensor([  5.3646, -17.2871])\n",
      "\t Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4058, Loss 2.9276740550994873\n",
      "\t Params: tensor([  5.3646, -17.2871])\n",
      "\t Grad: tensor([-0.0006,  0.0030])\n",
      "Epoch 4059, Loss 2.9276750087738037\n",
      "\t Params: tensor([  5.3646, -17.2872])\n",
      "\t Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4060, Loss 2.9276723861694336\n",
      "\t Params: tensor([  5.3646, -17.2872])\n",
      "\t Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4061, Loss 2.927673101425171\n",
      "\t Params: tensor([  5.3646, -17.2872])\n",
      "\t Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4062, Loss 2.9276747703552246\n",
      "\t Params: tensor([  5.3646, -17.2872])\n",
      "\t Grad: tensor([-0.0006,  0.0030])\n",
      "Epoch 4063, Loss 2.9276726245880127\n",
      "\t Params: tensor([  5.3646, -17.2873])\n",
      "\t Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4064, Loss 2.927673578262329\n",
      "\t Params: tensor([  5.3646, -17.2873])\n",
      "\t Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4065, Loss 2.9276716709136963\n",
      "\t Params: tensor([  5.3646, -17.2873])\n",
      "\t Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4066, Loss 2.9276726245880127\n",
      "\t Params: tensor([  5.3646, -17.2874])\n",
      "\t Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4067, Loss 2.927673101425171\n",
      "\t Params: tensor([  5.3646, -17.2874])\n",
      "\t Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4068, Loss 2.9276716709136963\n",
      "\t Params: tensor([  5.3647, -17.2874])\n",
      "\t Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4069, Loss 2.9276719093322754\n",
      "\t Params: tensor([  5.3647, -17.2875])\n",
      "\t Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4070, Loss 2.92767333984375\n",
      "\t Params: tensor([  5.3647, -17.2875])\n",
      "\t Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4071, Loss 2.927673101425171\n",
      "\t Params: tensor([  5.3647, -17.2875])\n",
      "\t Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4072, Loss 2.927670955657959\n",
      "\t Params: tensor([  5.3647, -17.2875])\n",
      "\t Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4073, Loss 2.9276726245880127\n",
      "\t Params: tensor([  5.3647, -17.2876])\n",
      "\t Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4074, Loss 2.9276723861694336\n",
      "\t Params: tensor([  5.3647, -17.2876])\n",
      "\t Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4075, Loss 2.9276719093322754\n",
      "\t Params: tensor([  5.3647, -17.2876])\n",
      "\t Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4076, Loss 2.9276719093322754\n",
      "\t Params: tensor([  5.3647, -17.2877])\n",
      "\t Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4077, Loss 2.927670478820801\n",
      "\t Params: tensor([  5.3647, -17.2877])\n",
      "\t Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4078, Loss 2.9276723861694336\n",
      "\t Params: tensor([  5.3647, -17.2877])\n",
      "\t Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4079, Loss 2.9276716709136963\n",
      "\t Params: tensor([  5.3647, -17.2877])\n",
      "\t Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4080, Loss 2.927671194076538\n",
      "\t Params: tensor([  5.3647, -17.2878])\n",
      "\t Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4081, Loss 2.9276702404022217\n",
      "\t Params: tensor([  5.3647, -17.2878])\n",
      "\t Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4082, Loss 2.92767333984375\n",
      "\t Params: tensor([  5.3647, -17.2878])\n",
      "\t Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4083, Loss 2.9276716709136963\n",
      "\t Params: tensor([  5.3647, -17.2879])\n",
      "\t Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4084, Loss 2.927670478820801\n",
      "\t Params: tensor([  5.3647, -17.2879])\n",
      "\t Grad: tensor([-0.0005,  0.0029])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4085, Loss 2.9276702404022217\n",
      "\t Params: tensor([  5.3647, -17.2879])\n",
      "\t Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4086, Loss 2.9276702404022217\n",
      "\t Params: tensor([  5.3647, -17.2879])\n",
      "\t Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4087, Loss 2.9276723861694336\n",
      "\t Params: tensor([  5.3647, -17.2880])\n",
      "\t Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4088, Loss 2.9276702404022217\n",
      "\t Params: tensor([  5.3648, -17.2880])\n",
      "\t Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4089, Loss 2.9276702404022217\n",
      "\t Params: tensor([  5.3648, -17.2880])\n",
      "\t Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4090, Loss 2.9276702404022217\n",
      "\t Params: tensor([  5.3648, -17.2881])\n",
      "\t Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4091, Loss 2.9276702404022217\n",
      "\t Params: tensor([  5.3648, -17.2881])\n",
      "\t Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4092, Loss 2.927670478820801\n",
      "\t Params: tensor([  5.3648, -17.2881])\n",
      "\t Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4093, Loss 2.9276702404022217\n",
      "\t Params: tensor([  5.3648, -17.2881])\n",
      "\t Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4094, Loss 2.9276692867279053\n",
      "\t Params: tensor([  5.3648, -17.2882])\n",
      "\t Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4095, Loss 2.927670478820801\n",
      "\t Params: tensor([  5.3648, -17.2882])\n",
      "\t Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4096, Loss 2.9276695251464844\n",
      "\t Params: tensor([  5.3648, -17.2882])\n",
      "\t Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4097, Loss 2.927670955657959\n",
      "\t Params: tensor([  5.3648, -17.2883])\n",
      "\t Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4098, Loss 2.9276695251464844\n",
      "\t Params: tensor([  5.3648, -17.2883])\n",
      "\t Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4099, Loss 2.927671194076538\n",
      "\t Params: tensor([  5.3648, -17.2883])\n",
      "\t Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4100, Loss 2.9276695251464844\n",
      "\t Params: tensor([  5.3648, -17.2883])\n",
      "\t Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4101, Loss 2.9276695251464844\n",
      "\t Params: tensor([  5.3648, -17.2884])\n",
      "\t Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4102, Loss 2.927671194076538\n",
      "\t Params: tensor([  5.3648, -17.2884])\n",
      "\t Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4103, Loss 2.927668809890747\n",
      "\t Params: tensor([  5.3648, -17.2884])\n",
      "\t Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4104, Loss 2.9276697635650635\n",
      "\t Params: tensor([  5.3648, -17.2885])\n",
      "\t Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4105, Loss 2.9276702404022217\n",
      "\t Params: tensor([  5.3648, -17.2885])\n",
      "\t Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4106, Loss 2.927670478820801\n",
      "\t Params: tensor([  5.3648, -17.2885])\n",
      "\t Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4107, Loss 2.9276695251464844\n",
      "\t Params: tensor([  5.3648, -17.2885])\n",
      "\t Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4108, Loss 2.9276680946350098\n",
      "\t Params: tensor([  5.3649, -17.2886])\n",
      "\t Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4109, Loss 2.927668571472168\n",
      "\t Params: tensor([  5.3649, -17.2886])\n",
      "\t Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4110, Loss 2.927668571472168\n",
      "\t Params: tensor([  5.3649, -17.2886])\n",
      "\t Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4111, Loss 2.9276695251464844\n",
      "\t Params: tensor([  5.3649, -17.2886])\n",
      "\t Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4112, Loss 2.9276697635650635\n",
      "\t Params: tensor([  5.3649, -17.2887])\n",
      "\t Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4113, Loss 2.9276695251464844\n",
      "\t Params: tensor([  5.3649, -17.2887])\n",
      "\t Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4114, Loss 2.9276695251464844\n",
      "\t Params: tensor([  5.3649, -17.2887])\n",
      "\t Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4115, Loss 2.927668809890747\n",
      "\t Params: tensor([  5.3649, -17.2887])\n",
      "\t Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4116, Loss 2.927668809890747\n",
      "\t Params: tensor([  5.3649, -17.2888])\n",
      "\t Grad: tensor([-0.0004,  0.0027])\n",
      "Epoch 4117, Loss 2.9276702404022217\n",
      "\t Params: tensor([  5.3649, -17.2888])\n",
      "\t Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4118, Loss 2.9276692867279053\n",
      "\t Params: tensor([  5.3649, -17.2888])\n",
      "\t Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4119, Loss 2.9276697635650635\n",
      "\t Params: tensor([  5.3649, -17.2889])\n",
      "\t Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4120, Loss 2.9276678562164307\n",
      "\t Params: tensor([  5.3649, -17.2889])\n",
      "\t Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4121, Loss 2.9276680946350098\n",
      "\t Params: tensor([  5.3649, -17.2889])\n",
      "\t Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4122, Loss 2.9276692867279053\n",
      "\t Params: tensor([  5.3649, -17.2889])\n",
      "\t Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4123, Loss 2.9276680946350098\n",
      "\t Params: tensor([  5.3649, -17.2890])\n",
      "\t Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4124, Loss 2.9276695251464844\n",
      "\t Params: tensor([  5.3649, -17.2890])\n",
      "\t Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4125, Loss 2.927666425704956\n",
      "\t Params: tensor([  5.3649, -17.2890])\n",
      "\t Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4126, Loss 2.927668571472168\n",
      "\t Params: tensor([  5.3649, -17.2890])\n",
      "\t Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4127, Loss 2.9276678562164307\n",
      "\t Params: tensor([  5.3649, -17.2891])\n",
      "\t Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4128, Loss 2.927668571472168\n",
      "\t Params: tensor([  5.3649, -17.2891])\n",
      "\t Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4129, Loss 2.927666664123535\n",
      "\t Params: tensor([  5.3650, -17.2891])\n",
      "\t Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4130, Loss 2.9276673793792725\n",
      "\t Params: tensor([  5.3650, -17.2892])\n",
      "\t Grad: tensor([-0.0004,  0.0027])\n",
      "Epoch 4131, Loss 2.9276680946350098\n",
      "\t Params: tensor([  5.3650, -17.2892])\n",
      "\t Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4132, Loss 2.9276673793792725\n",
      "\t Params: tensor([  5.3650, -17.2892])\n",
      "\t Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4133, Loss 2.9276671409606934\n",
      "\t Params: tensor([  5.3650, -17.2892])\n",
      "\t Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4134, Loss 2.927666425704956\n",
      "\t Params: tensor([  5.3650, -17.2893])\n",
      "\t Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4135, Loss 2.927665948867798\n",
      "\t Params: tensor([  5.3650, -17.2893])\n",
      "\t Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4136, Loss 2.927668809890747\n",
      "\t Params: tensor([  5.3650, -17.2893])\n",
      "\t Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4137, Loss 2.927665948867798\n",
      "\t Params: tensor([  5.3650, -17.2893])\n",
      "\t Grad: tensor([-0.0004,  0.0026])\n",
      "Epoch 4138, Loss 2.9276678562164307\n",
      "\t Params: tensor([  5.3650, -17.2894])\n",
      "\t Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4139, Loss 2.927665948867798\n",
      "\t Params: tensor([  5.3650, -17.2894])\n",
      "\t Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4140, Loss 2.9276671409606934\n",
      "\t Params: tensor([  5.3650, -17.2894])\n",
      "\t Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4141, Loss 2.9276673793792725\n",
      "\t Params: tensor([  5.3650, -17.2894])\n",
      "\t Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4142, Loss 2.927666425704956\n",
      "\t Params: tensor([  5.3650, -17.2895])\n",
      "\t Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4143, Loss 2.9276671409606934\n",
      "\t Params: tensor([  5.3650, -17.2895])\n",
      "\t Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4144, Loss 2.927665948867798\n",
      "\t Params: tensor([  5.3650, -17.2895])\n",
      "\t Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4145, Loss 2.9276671409606934\n",
      "\t Params: tensor([  5.3650, -17.2896])\n",
      "\t Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4146, Loss 2.9276671409606934\n",
      "\t Params: tensor([  5.3650, -17.2896])\n",
      "\t Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4147, Loss 2.927666664123535\n",
      "\t Params: tensor([  5.3650, -17.2896])\n",
      "\t Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4148, Loss 2.9276671409606934\n",
      "\t Params: tensor([  5.3650, -17.2896])\n",
      "\t Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4149, Loss 2.9276654720306396\n",
      "\t Params: tensor([  5.3650, -17.2897])\n",
      "\t Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4150, Loss 2.927666425704956\n",
      "\t Params: tensor([  5.3651, -17.2897])\n",
      "\t Grad: tensor([-0.0004,  0.0026])\n",
      "Epoch 4151, Loss 2.927666425704956\n",
      "\t Params: tensor([  5.3651, -17.2897])\n",
      "\t Grad: tensor([-0.0004,  0.0026])\n",
      "Epoch 4152, Loss 2.927665948867798\n",
      "\t Params: tensor([  5.3651, -17.2897])\n",
      "\t Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4153, Loss 2.927665948867798\n",
      "\t Params: tensor([  5.3651, -17.2898])\n",
      "\t Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4154, Loss 2.927665948867798\n",
      "\t Params: tensor([  5.3651, -17.2898])\n",
      "\t Grad: tensor([-0.0004,  0.0026])\n",
      "Epoch 4155, Loss 2.9276657104492188\n",
      "\t Params: tensor([  5.3651, -17.2898])\n",
      "\t Grad: tensor([-0.0004,  0.0026])\n",
      "Epoch 4156, Loss 2.9276657104492188\n",
      "\t Params: tensor([  5.3651, -17.2898])\n",
      "\t Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4157, Loss 2.9276654720306396\n",
      "\t Params: tensor([  5.3651, -17.2899])\n",
      "\t Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4158, Loss 2.927665948867798\n",
      "\t Params: tensor([  5.3651, -17.2899])\n",
      "\t Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4159, Loss 2.9276654720306396\n",
      "\t Params: tensor([  5.3651, -17.2899])\n",
      "\t Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4160, Loss 2.927663564682007\n",
      "\t Params: tensor([  5.3651, -17.2899])\n",
      "\t Grad: tensor([-0.0005,  0.0025])\n",
      "Epoch 4161, Loss 2.927665948867798\n",
      "\t Params: tensor([  5.3651, -17.2900])\n",
      "\t Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4162, Loss 2.9276649951934814\n",
      "\t Params: tensor([  5.3651, -17.2900])\n",
      "\t Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4163, Loss 2.927665948867798\n",
      "\t Params: tensor([  5.3651, -17.2900])\n",
      "\t Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4164, Loss 2.927663564682007\n",
      "\t Params: tensor([  5.3651, -17.2900])\n",
      "\t Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4165, Loss 2.9276649951934814\n",
      "\t Params: tensor([  5.3651, -17.2901])\n",
      "\t Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4166, Loss 2.9276654720306396\n",
      "\t Params: tensor([  5.3651, -17.2901])\n",
      "\t Grad: tensor([-0.0005,  0.0025])\n",
      "Epoch 4167, Loss 2.9276649951934814\n",
      "\t Params: tensor([  5.3651, -17.2901])\n",
      "\t Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4168, Loss 2.9276657104492188\n",
      "\t Params: tensor([  5.3651, -17.2901])\n",
      "\t Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4169, Loss 2.927664279937744\n",
      "\t Params: tensor([  5.3651, -17.2902])\n",
      "\t Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4170, Loss 2.9276654720306396\n",
      "\t Params: tensor([  5.3651, -17.2902])\n",
      "\t Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4171, Loss 2.927665948867798\n",
      "\t Params: tensor([  5.3651, -17.2902])\n",
      "\t Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4172, Loss 2.9276633262634277\n",
      "\t Params: tensor([  5.3651, -17.2902])\n",
      "\t Grad: tensor([-0.0005,  0.0025])\n",
      "Epoch 4173, Loss 2.927664041519165\n",
      "\t Params: tensor([  5.3652, -17.2903])\n",
      "\t Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4174, Loss 2.927664041519165\n",
      "\t Params: tensor([  5.3652, -17.2903])\n",
      "\t Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4175, Loss 2.9276649951934814\n",
      "\t Params: tensor([  5.3652, -17.2903])\n",
      "\t Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4176, Loss 2.9276628494262695\n",
      "\t Params: tensor([  5.3652, -17.2903])\n",
      "\t Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4177, Loss 2.927664041519165\n",
      "\t Params: tensor([  5.3652, -17.2903])\n",
      "\t Grad: tensor([-0.0005,  0.0025])\n",
      "Epoch 4178, Loss 2.927664041519165\n",
      "\t Params: tensor([  5.3652, -17.2904])\n",
      "\t Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4179, Loss 2.9276633262634277\n",
      "\t Params: tensor([  5.3652, -17.2904])\n",
      "\t Grad: tensor([-0.0005,  0.0024])\n",
      "Epoch 4180, Loss 2.927663564682007\n",
      "\t Params: tensor([  5.3652, -17.2904])\n",
      "\t Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4181, Loss 2.927664041519165\n",
      "\t Params: tensor([  5.3652, -17.2904])\n",
      "\t Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4182, Loss 2.9276633262634277\n",
      "\t Params: tensor([  5.3652, -17.2905])\n",
      "\t Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4183, Loss 2.927664279937744\n",
      "\t Params: tensor([  5.3652, -17.2905])\n",
      "\t Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4184, Loss 2.927664041519165\n",
      "\t Params: tensor([  5.3652, -17.2905])\n",
      "\t Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4185, Loss 2.927661657333374\n",
      "\t Params: tensor([  5.3652, -17.2905])\n",
      "\t Grad: tensor([-0.0005,  0.0024])\n",
      "Epoch 4186, Loss 2.9276649951934814\n",
      "\t Params: tensor([  5.3652, -17.2906])\n",
      "\t Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4187, Loss 2.9276633262634277\n",
      "\t Params: tensor([  5.3652, -17.2906])\n",
      "\t Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4188, Loss 2.927661657333374\n",
      "\t Params: tensor([  5.3652, -17.2906])\n",
      "\t Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4189, Loss 2.9276633262634277\n",
      "\t Params: tensor([  5.3652, -17.2906])\n",
      "\t Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4190, Loss 2.927664279937744\n",
      "\t Params: tensor([  5.3652, -17.2907])\n",
      "\t Grad: tensor([-0.0005,  0.0024])\n",
      "Epoch 4191, Loss 2.927664041519165\n",
      "\t Params: tensor([  5.3652, -17.2907])\n",
      "\t Grad: tensor([-0.0005,  0.0024])\n",
      "Epoch 4192, Loss 2.9276621341705322\n",
      "\t Params: tensor([  5.3652, -17.2907])\n",
      "\t Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4193, Loss 2.9276633262634277\n",
      "\t Params: tensor([  5.3652, -17.2907])\n",
      "\t Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4194, Loss 2.9276628494262695\n",
      "\t Params: tensor([  5.3652, -17.2908])\n",
      "\t Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4195, Loss 2.9276649951934814\n",
      "\t Params: tensor([  5.3652, -17.2908])\n",
      "\t Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4196, Loss 2.927664041519165\n",
      "\t Params: tensor([  5.3653, -17.2908])\n",
      "\t Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4197, Loss 2.9276628494262695\n",
      "\t Params: tensor([  5.3653, -17.2908])\n",
      "\t Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4198, Loss 2.9276621341705322\n",
      "\t Params: tensor([  5.3653, -17.2909])\n",
      "\t Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4199, Loss 2.927663564682007\n",
      "\t Params: tensor([  5.3653, -17.2909])\n",
      "\t Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4200, Loss 2.9276633262634277\n",
      "\t Params: tensor([  5.3653, -17.2909])\n",
      "\t Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4201, Loss 2.927661180496216\n",
      "\t Params: tensor([  5.3653, -17.2909])\n",
      "\t Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4202, Loss 2.9276621341705322\n",
      "\t Params: tensor([  5.3653, -17.2910])\n",
      "\t Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4203, Loss 2.927661895751953\n",
      "\t Params: tensor([  5.3653, -17.2910])\n",
      "\t Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4204, Loss 2.9276628494262695\n",
      "\t Params: tensor([  5.3653, -17.2910])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4205, Loss 2.9276628494262695\n",
      "\t Params: tensor([  5.3653, -17.2910])\n",
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4206, Loss 2.927661895751953\n",
      "\t Params: tensor([  5.3653, -17.2910])\n",
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4207, Loss 2.9276621341705322\n",
      "\t Params: tensor([  5.3653, -17.2911])\n",
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4208, Loss 2.9276628494262695\n",
      "\t Params: tensor([  5.3653, -17.2911])\n",
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4209, Loss 2.927663564682007\n",
      "\t Params: tensor([  5.3653, -17.2911])\n",
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4210, Loss 2.927661657333374\n",
      "\t Params: tensor([  5.3653, -17.2911])\n",
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4211, Loss 2.9276604652404785\n",
      "\t Params: tensor([  5.3653, -17.2912])\n",
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4212, Loss 2.927661657333374\n",
      "\t Params: tensor([  5.3653, -17.2912])\n",
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4213, Loss 2.9276621341705322\n",
      "\t Params: tensor([  5.3653, -17.2912])\n",
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4214, Loss 2.9276621341705322\n",
      "\t Params: tensor([  5.3653, -17.2912])\n",
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4215, Loss 2.927661180496216\n",
      "\t Params: tensor([  5.3653, -17.2913])\n",
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4216, Loss 2.9276604652404785\n",
      "\t Params: tensor([  5.3653, -17.2913])\n",
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4217, Loss 2.927661657333374\n",
      "\t Params: tensor([  5.3653, -17.2913])\n",
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4218, Loss 2.9276621341705322\n",
      "\t Params: tensor([  5.3653, -17.2913])\n",
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4219, Loss 2.9276628494262695\n",
      "\t Params: tensor([  5.3653, -17.2913])\n",
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4220, Loss 2.9276621341705322\n",
      "\t Params: tensor([  5.3653, -17.2914])\n",
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4221, Loss 2.927661895751953\n",
      "\t Params: tensor([  5.3654, -17.2914])\n",
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4222, Loss 2.9276621341705322\n",
      "\t Params: tensor([  5.3654, -17.2914])\n",
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4223, Loss 2.9276626110076904\n",
      "\t Params: tensor([  5.3654, -17.2914])\n",
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4224, Loss 2.9276604652404785\n",
      "\t Params: tensor([  5.3654, -17.2915])\n",
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4225, Loss 2.927661895751953\n",
      "\t Params: tensor([  5.3654, -17.2915])\n",
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4226, Loss 2.9276604652404785\n",
      "\t Params: tensor([  5.3654, -17.2915])\n",
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4227, Loss 2.927661180496216\n",
      "\t Params: tensor([  5.3654, -17.2915])\n",
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4228, Loss 2.9276609420776367\n",
      "\t Params: tensor([  5.3654, -17.2915])\n",
      "\t Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4229, Loss 2.9276604652404785\n",
      "\t Params: tensor([  5.3654, -17.2916])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4230, Loss 2.927661895751953\n",
      "\t Params: tensor([  5.3654, -17.2916])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4231, Loss 2.927661657333374\n",
      "\t Params: tensor([  5.3654, -17.2916])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4232, Loss 2.9276602268218994\n",
      "\t Params: tensor([  5.3654, -17.2916])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4233, Loss 2.927661657333374\n",
      "\t Params: tensor([  5.3654, -17.2917])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4234, Loss 2.9276609420776367\n",
      "\t Params: tensor([  5.3654, -17.2917])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4235, Loss 2.927661657333374\n",
      "\t Params: tensor([  5.3654, -17.2917])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4236, Loss 2.927661180496216\n",
      "\t Params: tensor([  5.3654, -17.2917])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4237, Loss 2.9276602268218994\n",
      "\t Params: tensor([  5.3654, -17.2918])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4238, Loss 2.9276609420776367\n",
      "\t Params: tensor([  5.3654, -17.2918])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4239, Loss 2.9276602268218994\n",
      "\t Params: tensor([  5.3654, -17.2918])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4240, Loss 2.927661895751953\n",
      "\t Params: tensor([  5.3654, -17.2918])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4241, Loss 2.9276604652404785\n",
      "\t Params: tensor([  5.3654, -17.2918])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4242, Loss 2.927659034729004\n",
      "\t Params: tensor([  5.3654, -17.2919])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4243, Loss 2.9276609420776367\n",
      "\t Params: tensor([  5.3654, -17.2919])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4244, Loss 2.927661180496216\n",
      "\t Params: tensor([  5.3654, -17.2919])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4245, Loss 2.927661657333374\n",
      "\t Params: tensor([  5.3654, -17.2919])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4246, Loss 2.927659511566162\n",
      "\t Params: tensor([  5.3655, -17.2920])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4247, Loss 2.927659034729004\n",
      "\t Params: tensor([  5.3655, -17.2920])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4248, Loss 2.927659749984741\n",
      "\t Params: tensor([  5.3655, -17.2920])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4249, Loss 2.9276604652404785\n",
      "\t Params: tensor([  5.3655, -17.2920])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4250, Loss 2.927661895751953\n",
      "\t Params: tensor([  5.3655, -17.2920])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4251, Loss 2.9276602268218994\n",
      "\t Params: tensor([  5.3655, -17.2921])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4252, Loss 2.9276602268218994\n",
      "\t Params: tensor([  5.3655, -17.2921])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4253, Loss 2.9276602268218994\n",
      "\t Params: tensor([  5.3655, -17.2921])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4254, Loss 2.9276604652404785\n",
      "\t Params: tensor([  5.3655, -17.2921])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4255, Loss 2.927659749984741\n",
      "\t Params: tensor([  5.3655, -17.2921])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4256, Loss 2.9276609420776367\n",
      "\t Params: tensor([  5.3655, -17.2922])\n",
      "\t Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4257, Loss 2.927658796310425\n",
      "\t Params: tensor([  5.3655, -17.2922])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4258, Loss 2.927659511566162\n",
      "\t Params: tensor([  5.3655, -17.2922])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4259, Loss 2.927659034729004\n",
      "\t Params: tensor([  5.3655, -17.2922])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4260, Loss 2.927658796310425\n",
      "\t Params: tensor([  5.3655, -17.2922])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4261, Loss 2.927661657333374\n",
      "\t Params: tensor([  5.3655, -17.2923])\n",
      "\t Grad: tensor([-0.0003,  0.0021])\n",
      "Epoch 4262, Loss 2.9276583194732666\n",
      "\t Params: tensor([  5.3655, -17.2923])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4263, Loss 2.927659034729004\n",
      "\t Params: tensor([  5.3655, -17.2923])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4264, Loss 2.9276602268218994\n",
      "\t Params: tensor([  5.3655, -17.2923])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4265, Loss 2.927658796310425\n",
      "\t Params: tensor([  5.3655, -17.2924])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4266, Loss 2.9276604652404785\n",
      "\t Params: tensor([  5.3655, -17.2924])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4267, Loss 2.927659511566162\n",
      "\t Params: tensor([  5.3655, -17.2924])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4268, Loss 2.927659034729004\n",
      "\t Params: tensor([  5.3655, -17.2924])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4269, Loss 2.927659511566162\n",
      "\t Params: tensor([  5.3655, -17.2924])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4270, Loss 2.9276602268218994\n",
      "\t Params: tensor([  5.3655, -17.2925])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4271, Loss 2.927659749984741\n",
      "\t Params: tensor([  5.3655, -17.2925])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4272, Loss 2.927659511566162\n",
      "\t Params: tensor([  5.3655, -17.2925])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4273, Loss 2.9276578426361084\n",
      "\t Params: tensor([  5.3656, -17.2925])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4274, Loss 2.927659034729004\n",
      "\t Params: tensor([  5.3656, -17.2925])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4275, Loss 2.927659511566162\n",
      "\t Params: tensor([  5.3656, -17.2926])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4276, Loss 2.927659034729004\n",
      "\t Params: tensor([  5.3656, -17.2926])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4277, Loss 2.927659034729004\n",
      "\t Params: tensor([  5.3656, -17.2926])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4278, Loss 2.9276583194732666\n",
      "\t Params: tensor([  5.3656, -17.2926])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4279, Loss 2.9276578426361084\n",
      "\t Params: tensor([  5.3656, -17.2926])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4280, Loss 2.927658796310425\n",
      "\t Params: tensor([  5.3656, -17.2927])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4281, Loss 2.927658796310425\n",
      "\t Params: tensor([  5.3656, -17.2927])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4282, Loss 2.9276602268218994\n",
      "\t Params: tensor([  5.3656, -17.2927])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4283, Loss 2.927659749984741\n",
      "\t Params: tensor([  5.3656, -17.2927])\n",
      "\t Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4284, Loss 2.9276578426361084\n",
      "\t Params: tensor([  5.3656, -17.2927])\n",
      "\t Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4285, Loss 2.927658796310425\n",
      "\t Params: tensor([  5.3656, -17.2928])\n",
      "\t Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4286, Loss 2.9276580810546875\n",
      "\t Params: tensor([  5.3656, -17.2928])\n",
      "\t Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4287, Loss 2.927658796310425\n",
      "\t Params: tensor([  5.3656, -17.2928])\n",
      "\t Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4288, Loss 2.9276583194732666\n",
      "\t Params: tensor([  5.3656, -17.2928])\n",
      "\t Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4289, Loss 2.92765736579895\n",
      "\t Params: tensor([  5.3656, -17.2929])\n",
      "\t Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4290, Loss 2.927659034729004\n",
      "\t Params: tensor([  5.3656, -17.2929])\n",
      "\t Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4291, Loss 2.927659034729004\n",
      "\t Params: tensor([  5.3656, -17.2929])\n",
      "\t Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4292, Loss 2.927658796310425\n",
      "\t Params: tensor([  5.3656, -17.2929])\n",
      "\t Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4293, Loss 2.927659034729004\n",
      "\t Params: tensor([  5.3656, -17.2929])\n",
      "\t Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4294, Loss 2.927658796310425\n",
      "\t Params: tensor([  5.3656, -17.2930])\n",
      "\t Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4295, Loss 2.92765736579895\n",
      "\t Params: tensor([  5.3656, -17.2930])\n",
      "\t Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4296, Loss 2.927657127380371\n",
      "\t Params: tensor([  5.3656, -17.2930])\n",
      "\t Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4297, Loss 2.92765736579895\n",
      "\t Params: tensor([  5.3656, -17.2930])\n",
      "\t Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4298, Loss 2.9276580810546875\n",
      "\t Params: tensor([  5.3656, -17.2930])\n",
      "\t Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4299, Loss 2.927659034729004\n",
      "\t Params: tensor([  5.3656, -17.2931])\n",
      "\t Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4300, Loss 2.927659511566162\n",
      "\t Params: tensor([  5.3657, -17.2931])\n",
      "\t Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4301, Loss 2.927657127380371\n",
      "\t Params: tensor([  5.3657, -17.2931])\n",
      "\t Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4302, Loss 2.9276578426361084\n",
      "\t Params: tensor([  5.3657, -17.2931])\n",
      "\t Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4303, Loss 2.9276580810546875\n",
      "\t Params: tensor([  5.3657, -17.2931])\n",
      "\t Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4304, Loss 2.9276583194732666\n",
      "\t Params: tensor([  5.3657, -17.2932])\n",
      "\t Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4305, Loss 2.9276578426361084\n",
      "\t Params: tensor([  5.3657, -17.2932])\n",
      "\t Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4306, Loss 2.92765736579895\n",
      "\t Params: tensor([  5.3657, -17.2932])\n",
      "\t Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4307, Loss 2.92765736579895\n",
      "\t Params: tensor([  5.3657, -17.2932])\n",
      "\t Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4308, Loss 2.9276580810546875\n",
      "\t Params: tensor([  5.3657, -17.2932])\n",
      "\t Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4309, Loss 2.927657127380371\n",
      "\t Params: tensor([  5.3657, -17.2932])\n",
      "\t Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4310, Loss 2.927659511566162\n",
      "\t Params: tensor([  5.3657, -17.2933])\n",
      "\t Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4311, Loss 2.927658796310425\n",
      "\t Params: tensor([  5.3657, -17.2933])\n",
      "\t Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4312, Loss 2.9276580810546875\n",
      "\t Params: tensor([  5.3657, -17.2933])\n",
      "\t Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4313, Loss 2.9276556968688965\n",
      "\t Params: tensor([  5.3657, -17.2933])\n",
      "\t Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4314, Loss 2.9276580810546875\n",
      "\t Params: tensor([  5.3657, -17.2933])\n",
      "\t Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4315, Loss 2.92765736579895\n",
      "\t Params: tensor([  5.3657, -17.2934])\n",
      "\t Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4316, Loss 2.92765736579895\n",
      "\t Params: tensor([  5.3657, -17.2934])\n",
      "\t Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4317, Loss 2.9276583194732666\n",
      "\t Params: tensor([  5.3657, -17.2934])\n",
      "\t Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4318, Loss 2.9276578426361084\n",
      "\t Params: tensor([  5.3657, -17.2934])\n",
      "\t Grad: tensor([-0.0004,  0.0019])\n",
      "Epoch 4319, Loss 2.927656650543213\n",
      "\t Params: tensor([  5.3657, -17.2934])\n",
      "\t Grad: tensor([-0.0004,  0.0019])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4320, Loss 2.9276556968688965\n",
      "\t Params: tensor([  5.3657, -17.2935])\n",
      "\t Grad: tensor([-0.0004,  0.0019])\n",
      "Epoch 4321, Loss 2.92765736579895\n",
      "\t Params: tensor([  5.3657, -17.2935])\n",
      "\t Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4322, Loss 2.927657127380371\n",
      "\t Params: tensor([  5.3657, -17.2935])\n",
      "\t Grad: tensor([-0.0004,  0.0019])\n",
      "Epoch 4323, Loss 2.9276578426361084\n",
      "\t Params: tensor([  5.3657, -17.2935])\n",
      "\t Grad: tensor([-0.0004,  0.0019])\n",
      "Epoch 4324, Loss 2.9276583194732666\n",
      "\t Params: tensor([  5.3657, -17.2935])\n",
      "\t Grad: tensor([-0.0004,  0.0019])\n",
      "Epoch 4325, Loss 2.927656650543213\n",
      "\t Params: tensor([  5.3657, -17.2936])\n",
      "\t Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4326, Loss 2.927656650543213\n",
      "\t Params: tensor([  5.3657, -17.2936])\n",
      "\t Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4327, Loss 2.927656650543213\n",
      "\t Params: tensor([  5.3657, -17.2936])\n",
      "\t Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4328, Loss 2.9276556968688965\n",
      "\t Params: tensor([  5.3657, -17.2936])\n",
      "\t Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4329, Loss 2.927656650543213\n",
      "\t Params: tensor([  5.3657, -17.2936])\n",
      "\t Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4330, Loss 2.9276559352874756\n",
      "\t Params: tensor([  5.3658, -17.2936])\n",
      "\t Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4331, Loss 2.9276556968688965\n",
      "\t Params: tensor([  5.3658, -17.2937])\n",
      "\t Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4332, Loss 2.9276559352874756\n",
      "\t Params: tensor([  5.3658, -17.2937])\n",
      "\t Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4333, Loss 2.927656412124634\n",
      "\t Params: tensor([  5.3658, -17.2937])\n",
      "\t Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4334, Loss 2.9276556968688965\n",
      "\t Params: tensor([  5.3658, -17.2937])\n",
      "\t Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4335, Loss 2.927657127380371\n",
      "\t Params: tensor([  5.3658, -17.2937])\n",
      "\t Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4336, Loss 2.9276559352874756\n",
      "\t Params: tensor([  5.3658, -17.2938])\n",
      "\t Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4337, Loss 2.927656412124634\n",
      "\t Params: tensor([  5.3658, -17.2938])\n",
      "\t Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4338, Loss 2.9276556968688965\n",
      "\t Params: tensor([  5.3658, -17.2938])\n",
      "\t Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4339, Loss 2.92765736579895\n",
      "\t Params: tensor([  5.3658, -17.2938])\n",
      "\t Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4340, Loss 2.927656412124634\n",
      "\t Params: tensor([  5.3658, -17.2938])\n",
      "\t Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4341, Loss 2.927657127380371\n",
      "\t Params: tensor([  5.3658, -17.2939])\n",
      "\t Grad: tensor([-0.0004,  0.0019])\n",
      "Epoch 4342, Loss 2.9276556968688965\n",
      "\t Params: tensor([  5.3658, -17.2939])\n",
      "\t Grad: tensor([-0.0004,  0.0019])\n",
      "Epoch 4343, Loss 2.927656412124634\n",
      "\t Params: tensor([  5.3658, -17.2939])\n",
      "\t Grad: tensor([-0.0004,  0.0018])\n",
      "Epoch 4344, Loss 2.927656650543213\n",
      "\t Params: tensor([  5.3658, -17.2939])\n",
      "\t Grad: tensor([-0.0004,  0.0018])\n",
      "Epoch 4345, Loss 2.9276556968688965\n",
      "\t Params: tensor([  5.3658, -17.2939])\n",
      "\t Grad: tensor([-0.0004,  0.0018])\n",
      "Epoch 4346, Loss 2.9276559352874756\n",
      "\t Params: tensor([  5.3658, -17.2940])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4347, Loss 2.92765736579895\n",
      "\t Params: tensor([  5.3658, -17.2940])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4348, Loss 2.927656650543213\n",
      "\t Params: tensor([  5.3658, -17.2940])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4349, Loss 2.9276556968688965\n",
      "\t Params: tensor([  5.3658, -17.2940])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4350, Loss 2.92765736579895\n",
      "\t Params: tensor([  5.3658, -17.2940])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4351, Loss 2.92765736579895\n",
      "\t Params: tensor([  5.3658, -17.2941])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4352, Loss 2.927656650543213\n",
      "\t Params: tensor([  5.3658, -17.2941])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4353, Loss 2.9276552200317383\n",
      "\t Params: tensor([  5.3658, -17.2941])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4354, Loss 2.9276559352874756\n",
      "\t Params: tensor([  5.3658, -17.2941])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4355, Loss 2.927656412124634\n",
      "\t Params: tensor([  5.3658, -17.2941])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4356, Loss 2.9276559352874756\n",
      "\t Params: tensor([  5.3658, -17.2941])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4357, Loss 2.927656650543213\n",
      "\t Params: tensor([  5.3658, -17.2942])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4358, Loss 2.9276556968688965\n",
      "\t Params: tensor([  5.3658, -17.2942])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4359, Loss 2.927656412124634\n",
      "\t Params: tensor([  5.3658, -17.2942])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4360, Loss 2.9276559352874756\n",
      "\t Params: tensor([  5.3659, -17.2942])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4361, Loss 2.927654504776001\n",
      "\t Params: tensor([  5.3659, -17.2942])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4362, Loss 2.927657127380371\n",
      "\t Params: tensor([  5.3659, -17.2942])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4363, Loss 2.927654504776001\n",
      "\t Params: tensor([  5.3659, -17.2943])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4364, Loss 2.9276556968688965\n",
      "\t Params: tensor([  5.3659, -17.2943])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4365, Loss 2.9276556968688965\n",
      "\t Params: tensor([  5.3659, -17.2943])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4366, Loss 2.927656412124634\n",
      "\t Params: tensor([  5.3659, -17.2943])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4367, Loss 2.927654266357422\n",
      "\t Params: tensor([  5.3659, -17.2943])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4368, Loss 2.927654981613159\n",
      "\t Params: tensor([  5.3659, -17.2943])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4369, Loss 2.927656412124634\n",
      "\t Params: tensor([  5.3659, -17.2944])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4370, Loss 2.927654504776001\n",
      "\t Params: tensor([  5.3659, -17.2944])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4371, Loss 2.927656650543213\n",
      "\t Params: tensor([  5.3659, -17.2944])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4372, Loss 2.9276552200317383\n",
      "\t Params: tensor([  5.3659, -17.2944])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4373, Loss 2.927657127380371\n",
      "\t Params: tensor([  5.3659, -17.2944])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4374, Loss 2.9276559352874756\n",
      "\t Params: tensor([  5.3659, -17.2945])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4375, Loss 2.9276552200317383\n",
      "\t Params: tensor([  5.3659, -17.2945])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4376, Loss 2.9276540279388428\n",
      "\t Params: tensor([  5.3659, -17.2945])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4377, Loss 2.927654504776001\n",
      "\t Params: tensor([  5.3659, -17.2945])\n",
      "\t Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4378, Loss 2.9276552200317383\n",
      "\t Params: tensor([  5.3659, -17.2945])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4379, Loss 2.9276540279388428\n",
      "\t Params: tensor([  5.3659, -17.2945])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4380, Loss 2.9276556968688965\n",
      "\t Params: tensor([  5.3659, -17.2946])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4381, Loss 2.927654504776001\n",
      "\t Params: tensor([  5.3659, -17.2946])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4382, Loss 2.927654504776001\n",
      "\t Params: tensor([  5.3659, -17.2946])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4383, Loss 2.927654504776001\n",
      "\t Params: tensor([  5.3659, -17.2946])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4384, Loss 2.9276556968688965\n",
      "\t Params: tensor([  5.3659, -17.2946])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4385, Loss 2.9276556968688965\n",
      "\t Params: tensor([  5.3659, -17.2946])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4386, Loss 2.9276552200317383\n",
      "\t Params: tensor([  5.3659, -17.2947])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4387, Loss 2.9276533126831055\n",
      "\t Params: tensor([  5.3659, -17.2947])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4388, Loss 2.9276535511016846\n",
      "\t Params: tensor([  5.3659, -17.2947])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4389, Loss 2.9276540279388428\n",
      "\t Params: tensor([  5.3659, -17.2947])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4390, Loss 2.927654504776001\n",
      "\t Params: tensor([  5.3659, -17.2947])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4391, Loss 2.9276559352874756\n",
      "\t Params: tensor([  5.3659, -17.2947])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4392, Loss 2.927654504776001\n",
      "\t Params: tensor([  5.3659, -17.2948])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4393, Loss 2.9276559352874756\n",
      "\t Params: tensor([  5.3660, -17.2948])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4394, Loss 2.927654504776001\n",
      "\t Params: tensor([  5.3660, -17.2948])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4395, Loss 2.927654504776001\n",
      "\t Params: tensor([  5.3660, -17.2948])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4396, Loss 2.927654504776001\n",
      "\t Params: tensor([  5.3660, -17.2948])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4397, Loss 2.9276552200317383\n",
      "\t Params: tensor([  5.3660, -17.2948])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4398, Loss 2.9276559352874756\n",
      "\t Params: tensor([  5.3660, -17.2949])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4399, Loss 2.9276540279388428\n",
      "\t Params: tensor([  5.3660, -17.2949])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4400, Loss 2.927654981613159\n",
      "\t Params: tensor([  5.3660, -17.2949])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4401, Loss 2.9276535511016846\n",
      "\t Params: tensor([  5.3660, -17.2949])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4402, Loss 2.927654981613159\n",
      "\t Params: tensor([  5.3660, -17.2949])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4403, Loss 2.9276552200317383\n",
      "\t Params: tensor([  5.3660, -17.2949])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4404, Loss 2.9276556968688965\n",
      "\t Params: tensor([  5.3660, -17.2950])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4405, Loss 2.927654981613159\n",
      "\t Params: tensor([  5.3660, -17.2950])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4406, Loss 2.927654981613159\n",
      "\t Params: tensor([  5.3660, -17.2950])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4407, Loss 2.927654266357422\n",
      "\t Params: tensor([  5.3660, -17.2950])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4408, Loss 2.9276533126831055\n",
      "\t Params: tensor([  5.3660, -17.2950])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4409, Loss 2.927654981613159\n",
      "\t Params: tensor([  5.3660, -17.2951])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4410, Loss 2.927654504776001\n",
      "\t Params: tensor([  5.3660, -17.2951])\n",
      "\t Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4411, Loss 2.927654266357422\n",
      "\t Params: tensor([  5.3660, -17.2951])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4412, Loss 2.927654266357422\n",
      "\t Params: tensor([  5.3660, -17.2951])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4413, Loss 2.927654504776001\n",
      "\t Params: tensor([  5.3660, -17.2951])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4414, Loss 2.9276533126831055\n",
      "\t Params: tensor([  5.3660, -17.2951])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4415, Loss 2.9276540279388428\n",
      "\t Params: tensor([  5.3660, -17.2952])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4416, Loss 2.9276540279388428\n",
      "\t Params: tensor([  5.3660, -17.2952])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4417, Loss 2.9276540279388428\n",
      "\t Params: tensor([  5.3660, -17.2952])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4418, Loss 2.927654266357422\n",
      "\t Params: tensor([  5.3660, -17.2952])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4419, Loss 2.927654266357422\n",
      "\t Params: tensor([  5.3660, -17.2952])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4420, Loss 2.9276540279388428\n",
      "\t Params: tensor([  5.3660, -17.2952])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4421, Loss 2.9276533126831055\n",
      "\t Params: tensor([  5.3660, -17.2953])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4422, Loss 2.9276552200317383\n",
      "\t Params: tensor([  5.3660, -17.2953])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4423, Loss 2.9276533126831055\n",
      "\t Params: tensor([  5.3660, -17.2953])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4424, Loss 2.9276535511016846\n",
      "\t Params: tensor([  5.3660, -17.2953])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4425, Loss 2.927654504776001\n",
      "\t Params: tensor([  5.3660, -17.2953])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4426, Loss 2.927654266357422\n",
      "\t Params: tensor([  5.3660, -17.2953])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4427, Loss 2.9276535511016846\n",
      "\t Params: tensor([  5.3661, -17.2953])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4428, Loss 2.9276533126831055\n",
      "\t Params: tensor([  5.3661, -17.2954])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4429, Loss 2.9276535511016846\n",
      "\t Params: tensor([  5.3661, -17.2954])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4430, Loss 2.927652597427368\n",
      "\t Params: tensor([  5.3661, -17.2954])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4431, Loss 2.927654266357422\n",
      "\t Params: tensor([  5.3661, -17.2954])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4432, Loss 2.9276552200317383\n",
      "\t Params: tensor([  5.3661, -17.2954])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4433, Loss 2.9276540279388428\n",
      "\t Params: tensor([  5.3661, -17.2954])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4434, Loss 2.9276552200317383\n",
      "\t Params: tensor([  5.3661, -17.2955])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4435, Loss 2.927651882171631\n",
      "\t Params: tensor([  5.3661, -17.2955])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4436, Loss 2.9276528358459473\n",
      "\t Params: tensor([  5.3661, -17.2955])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4437, Loss 2.9276540279388428\n",
      "\t Params: tensor([  5.3661, -17.2955])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4438, Loss 2.9276535511016846\n",
      "\t Params: tensor([  5.3661, -17.2955])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4439, Loss 2.927654266357422\n",
      "\t Params: tensor([  5.3661, -17.2955])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4440, Loss 2.927654266357422\n",
      "\t Params: tensor([  5.3661, -17.2955])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4441, Loss 2.9276528358459473\n",
      "\t Params: tensor([  5.3661, -17.2956])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4442, Loss 2.9276533126831055\n",
      "\t Params: tensor([  5.3661, -17.2956])\n",
      "\t Grad: tensor([-0.0002,  0.0016])\n",
      "Epoch 4443, Loss 2.9276528358459473\n",
      "\t Params: tensor([  5.3661, -17.2956])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4444, Loss 2.9276528358459473\n",
      "\t Params: tensor([  5.3661, -17.2956])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4445, Loss 2.9276540279388428\n",
      "\t Params: tensor([  5.3661, -17.2956])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4446, Loss 2.92765212059021\n",
      "\t Params: tensor([  5.3661, -17.2956])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4447, Loss 2.927654266357422\n",
      "\t Params: tensor([  5.3661, -17.2957])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4448, Loss 2.9276540279388428\n",
      "\t Params: tensor([  5.3661, -17.2957])\n",
      "\t Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4449, Loss 2.9276535511016846\n",
      "\t Params: tensor([  5.3661, -17.2957])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4450, Loss 2.9276533126831055\n",
      "\t Params: tensor([  5.3661, -17.2957])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4451, Loss 2.9276514053344727\n",
      "\t Params: tensor([  5.3661, -17.2957])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4452, Loss 2.9276533126831055\n",
      "\t Params: tensor([  5.3661, -17.2957])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4453, Loss 2.9276540279388428\n",
      "\t Params: tensor([  5.3661, -17.2957])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4454, Loss 2.9276533126831055\n",
      "\t Params: tensor([  5.3661, -17.2958])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4455, Loss 2.927654504776001\n",
      "\t Params: tensor([  5.3661, -17.2958])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4456, Loss 2.9276535511016846\n",
      "\t Params: tensor([  5.3661, -17.2958])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4457, Loss 2.92765212059021\n",
      "\t Params: tensor([  5.3661, -17.2958])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4458, Loss 2.9276533126831055\n",
      "\t Params: tensor([  5.3661, -17.2958])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4459, Loss 2.9276528358459473\n",
      "\t Params: tensor([  5.3661, -17.2958])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4460, Loss 2.9276535511016846\n",
      "\t Params: tensor([  5.3661, -17.2959])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4461, Loss 2.9276533126831055\n",
      "\t Params: tensor([  5.3661, -17.2959])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4462, Loss 2.927651882171631\n",
      "\t Params: tensor([  5.3661, -17.2959])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4463, Loss 2.9276528358459473\n",
      "\t Params: tensor([  5.3661, -17.2959])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4464, Loss 2.9276535511016846\n",
      "\t Params: tensor([  5.3662, -17.2959])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4465, Loss 2.9276535511016846\n",
      "\t Params: tensor([  5.3662, -17.2959])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4466, Loss 2.9276535511016846\n",
      "\t Params: tensor([  5.3662, -17.2959])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4467, Loss 2.927651882171631\n",
      "\t Params: tensor([  5.3662, -17.2960])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4468, Loss 2.9276533126831055\n",
      "\t Params: tensor([  5.3662, -17.2960])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4469, Loss 2.9276528358459473\n",
      "\t Params: tensor([  5.3662, -17.2960])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4470, Loss 2.9276533126831055\n",
      "\t Params: tensor([  5.3662, -17.2960])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4471, Loss 2.9276533126831055\n",
      "\t Params: tensor([  5.3662, -17.2960])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4472, Loss 2.92765212059021\n",
      "\t Params: tensor([  5.3662, -17.2960])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4473, Loss 2.92765212059021\n",
      "\t Params: tensor([  5.3662, -17.2960])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4474, Loss 2.927651882171631\n",
      "\t Params: tensor([  5.3662, -17.2961])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4475, Loss 2.927652597427368\n",
      "\t Params: tensor([  5.3662, -17.2961])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4476, Loss 2.9276533126831055\n",
      "\t Params: tensor([  5.3662, -17.2961])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4477, Loss 2.92765212059021\n",
      "\t Params: tensor([  5.3662, -17.2961])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4478, Loss 2.9276528358459473\n",
      "\t Params: tensor([  5.3662, -17.2961])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4479, Loss 2.927652597427368\n",
      "\t Params: tensor([  5.3662, -17.2961])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4480, Loss 2.927652597427368\n",
      "\t Params: tensor([  5.3662, -17.2962])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4481, Loss 2.9276528358459473\n",
      "\t Params: tensor([  5.3662, -17.2962])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4482, Loss 2.9276535511016846\n",
      "\t Params: tensor([  5.3662, -17.2962])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4483, Loss 2.927652597427368\n",
      "\t Params: tensor([  5.3662, -17.2962])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4484, Loss 2.927651882171631\n",
      "\t Params: tensor([  5.3662, -17.2962])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4485, Loss 2.927652597427368\n",
      "\t Params: tensor([  5.3662, -17.2962])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4486, Loss 2.927652597427368\n",
      "\t Params: tensor([  5.3662, -17.2962])\n",
      "\t Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4487, Loss 2.92765212059021\n",
      "\t Params: tensor([  5.3662, -17.2963])\n",
      "\t Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch 4488, Loss 2.927652597427368\n",
      "\t Params: tensor([  5.3662, -17.2963])\n",
      "\t Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch 4489, Loss 2.927651882171631\n",
      "\t Params: tensor([  5.3662, -17.2963])\n",
      "\t Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch 4490, Loss 2.9276514053344727\n",
      "\t Params: tensor([  5.3662, -17.2963])\n",
      "\t Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch 4491, Loss 2.9276514053344727\n",
      "\t Params: tensor([  5.3662, -17.2963])\n",
      "\t Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch 4492, Loss 2.9276528358459473\n",
      "\t Params: tensor([  5.3662, -17.2963])\n",
      "\t Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch 4493, Loss 2.927652597427368\n",
      "\t Params: tensor([  5.3662, -17.2964])\n",
      "\t Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch 4494, Loss 2.927652597427368\n",
      "\t Params: tensor([  5.3662, -17.2964])\n",
      "\t Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch 4495, Loss 2.9276511669158936\n",
      "\t Params: tensor([  5.3662, -17.2964])\n",
      "\t Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch 4496, Loss 2.92765212059021\n",
      "\t Params: tensor([  5.3662, -17.2964])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4497, Loss 2.92765212059021\n",
      "\t Params: tensor([  5.3662, -17.2964])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4498, Loss 2.9276533126831055\n",
      "\t Params: tensor([  5.3662, -17.2964])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4499, Loss 2.9276506900787354\n",
      "\t Params: tensor([  5.3662, -17.2964])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4500, Loss 2.92765212059021\n",
      "\t Params: tensor([  5.3662, -17.2964])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4501, Loss 2.927652597427368\n",
      "\t Params: tensor([  5.3662, -17.2965])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4502, Loss 2.9276528358459473\n",
      "\t Params: tensor([  5.3663, -17.2965])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4503, Loss 2.927651882171631\n",
      "\t Params: tensor([  5.3663, -17.2965])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4504, Loss 2.9276514053344727\n",
      "\t Params: tensor([  5.3663, -17.2965])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4505, Loss 2.927651882171631\n",
      "\t Params: tensor([  5.3663, -17.2965])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4506, Loss 2.9276514053344727\n",
      "\t Params: tensor([  5.3663, -17.2965])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4507, Loss 2.927651882171631\n",
      "\t Params: tensor([  5.3663, -17.2965])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4508, Loss 2.9276511669158936\n",
      "\t Params: tensor([  5.3663, -17.2966])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4509, Loss 2.92765212059021\n",
      "\t Params: tensor([  5.3663, -17.2966])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4510, Loss 2.9276504516601562\n",
      "\t Params: tensor([  5.3663, -17.2966])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4511, Loss 2.9276511669158936\n",
      "\t Params: tensor([  5.3663, -17.2966])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4512, Loss 2.92765212059021\n",
      "\t Params: tensor([  5.3663, -17.2966])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4513, Loss 2.92765212059021\n",
      "\t Params: tensor([  5.3663, -17.2966])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4514, Loss 2.927650213241577\n",
      "\t Params: tensor([  5.3663, -17.2966])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4515, Loss 2.9276514053344727\n",
      "\t Params: tensor([  5.3663, -17.2966])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4516, Loss 2.927652597427368\n",
      "\t Params: tensor([  5.3663, -17.2967])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4517, Loss 2.92765212059021\n",
      "\t Params: tensor([  5.3663, -17.2967])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4518, Loss 2.9276504516601562\n",
      "\t Params: tensor([  5.3663, -17.2967])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4519, Loss 2.9276514053344727\n",
      "\t Params: tensor([  5.3663, -17.2967])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4520, Loss 2.9276511669158936\n",
      "\t Params: tensor([  5.3663, -17.2967])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4521, Loss 2.92765212059021\n",
      "\t Params: tensor([  5.3663, -17.2967])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4522, Loss 2.927651882171631\n",
      "\t Params: tensor([  5.3663, -17.2967])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4523, Loss 2.927652597427368\n",
      "\t Params: tensor([  5.3663, -17.2968])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4524, Loss 2.927651882171631\n",
      "\t Params: tensor([  5.3663, -17.2968])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4525, Loss 2.92765212059021\n",
      "\t Params: tensor([  5.3663, -17.2968])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4526, Loss 2.9276528358459473\n",
      "\t Params: tensor([  5.3663, -17.2968])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4527, Loss 2.927651882171631\n",
      "\t Params: tensor([  5.3663, -17.2968])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4528, Loss 2.9276514053344727\n",
      "\t Params: tensor([  5.3663, -17.2968])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4529, Loss 2.9276506900787354\n",
      "\t Params: tensor([  5.3663, -17.2968])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4530, Loss 2.9276514053344727\n",
      "\t Params: tensor([  5.3663, -17.2969])\n",
      "\t Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4531, Loss 2.9276504516601562\n",
      "\t Params: tensor([  5.3663, -17.2969])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4532, Loss 2.927651882171631\n",
      "\t Params: tensor([  5.3663, -17.2969])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4533, Loss 2.9276506900787354\n",
      "\t Params: tensor([  5.3663, -17.2969])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4534, Loss 2.9276511669158936\n",
      "\t Params: tensor([  5.3663, -17.2969])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4535, Loss 2.9276514053344727\n",
      "\t Params: tensor([  5.3663, -17.2969])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4536, Loss 2.927652597427368\n",
      "\t Params: tensor([  5.3663, -17.2969])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4537, Loss 2.9276511669158936\n",
      "\t Params: tensor([  5.3663, -17.2969])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4538, Loss 2.927650213241577\n",
      "\t Params: tensor([  5.3663, -17.2970])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4539, Loss 2.9276506900787354\n",
      "\t Params: tensor([  5.3663, -17.2970])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4540, Loss 2.9276511669158936\n",
      "\t Params: tensor([  5.3663, -17.2970])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4541, Loss 2.9276506900787354\n",
      "\t Params: tensor([  5.3663, -17.2970])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4542, Loss 2.9276511669158936\n",
      "\t Params: tensor([  5.3663, -17.2970])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4543, Loss 2.9276504516601562\n",
      "\t Params: tensor([  5.3663, -17.2970])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4544, Loss 2.9276506900787354\n",
      "\t Params: tensor([  5.3664, -17.2970])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4545, Loss 2.92765212059021\n",
      "\t Params: tensor([  5.3664, -17.2971])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4546, Loss 2.9276506900787354\n",
      "\t Params: tensor([  5.3664, -17.2971])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4547, Loss 2.9276504516601562\n",
      "\t Params: tensor([  5.3664, -17.2971])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4548, Loss 2.9276506900787354\n",
      "\t Params: tensor([  5.3664, -17.2971])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4549, Loss 2.927651882171631\n",
      "\t Params: tensor([  5.3664, -17.2971])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4550, Loss 2.927651882171631\n",
      "\t Params: tensor([  5.3664, -17.2971])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4551, Loss 2.927652597427368\n",
      "\t Params: tensor([  5.3664, -17.2971])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4552, Loss 2.9276511669158936\n",
      "\t Params: tensor([  5.3664, -17.2971])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4553, Loss 2.927651882171631\n",
      "\t Params: tensor([  5.3664, -17.2972])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4554, Loss 2.9276506900787354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params: tensor([  5.3664, -17.2972])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4555, Loss 2.9276506900787354\n",
      "\t Params: tensor([  5.3664, -17.2972])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4556, Loss 2.927651882171631\n",
      "\t Params: tensor([  5.3664, -17.2972])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4557, Loss 2.9276514053344727\n",
      "\t Params: tensor([  5.3664, -17.2972])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4558, Loss 2.9276511669158936\n",
      "\t Params: tensor([  5.3664, -17.2972])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4559, Loss 2.927650213241577\n",
      "\t Params: tensor([  5.3664, -17.2972])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4560, Loss 2.9276514053344727\n",
      "\t Params: tensor([  5.3664, -17.2973])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4561, Loss 2.92765212059021\n",
      "\t Params: tensor([  5.3664, -17.2973])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4562, Loss 2.9276514053344727\n",
      "\t Params: tensor([  5.3664, -17.2973])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4563, Loss 2.927650213241577\n",
      "\t Params: tensor([  5.3664, -17.2973])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4564, Loss 2.9276511669158936\n",
      "\t Params: tensor([  5.3664, -17.2973])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4565, Loss 2.927651882171631\n",
      "\t Params: tensor([  5.3664, -17.2973])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4566, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3664, -17.2973])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4567, Loss 2.927650213241577\n",
      "\t Params: tensor([  5.3664, -17.2973])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4568, Loss 2.9276514053344727\n",
      "\t Params: tensor([  5.3664, -17.2974])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4569, Loss 2.927651882171631\n",
      "\t Params: tensor([  5.3664, -17.2974])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4570, Loss 2.9276504516601562\n",
      "\t Params: tensor([  5.3664, -17.2974])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4571, Loss 2.92764949798584\n",
      "\t Params: tensor([  5.3664, -17.2974])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4572, Loss 2.9276504516601562\n",
      "\t Params: tensor([  5.3664, -17.2974])\n",
      "\t Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4573, Loss 2.927650213241577\n",
      "\t Params: tensor([  5.3664, -17.2974])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4574, Loss 2.927650213241577\n",
      "\t Params: tensor([  5.3664, -17.2974])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4575, Loss 2.9276511669158936\n",
      "\t Params: tensor([  5.3664, -17.2975])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4576, Loss 2.927650213241577\n",
      "\t Params: tensor([  5.3664, -17.2975])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4577, Loss 2.9276511669158936\n",
      "\t Params: tensor([  5.3664, -17.2975])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4578, Loss 2.9276506900787354\n",
      "\t Params: tensor([  5.3664, -17.2975])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4579, Loss 2.927651882171631\n",
      "\t Params: tensor([  5.3664, -17.2975])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4580, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3664, -17.2975])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4581, Loss 2.927652597427368\n",
      "\t Params: tensor([  5.3664, -17.2975])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4582, Loss 2.9276506900787354\n",
      "\t Params: tensor([  5.3664, -17.2975])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4583, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3664, -17.2976])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4584, Loss 2.927650213241577\n",
      "\t Params: tensor([  5.3664, -17.2976])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4585, Loss 2.9276504516601562\n",
      "\t Params: tensor([  5.3664, -17.2976])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4586, Loss 2.9276506900787354\n",
      "\t Params: tensor([  5.3664, -17.2976])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4587, Loss 2.9276506900787354\n",
      "\t Params: tensor([  5.3664, -17.2976])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4588, Loss 2.9276504516601562\n",
      "\t Params: tensor([  5.3665, -17.2976])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4589, Loss 2.9276504516601562\n",
      "\t Params: tensor([  5.3665, -17.2976])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4590, Loss 2.927651882171631\n",
      "\t Params: tensor([  5.3665, -17.2976])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4591, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3665, -17.2976])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4592, Loss 2.9276506900787354\n",
      "\t Params: tensor([  5.3665, -17.2977])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4593, Loss 2.9276511669158936\n",
      "\t Params: tensor([  5.3665, -17.2977])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4594, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3665, -17.2977])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4595, Loss 2.9276514053344727\n",
      "\t Params: tensor([  5.3665, -17.2977])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4596, Loss 2.9276504516601562\n",
      "\t Params: tensor([  5.3665, -17.2977])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4597, Loss 2.9276514053344727\n",
      "\t Params: tensor([  5.3665, -17.2977])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4598, Loss 2.927651882171631\n",
      "\t Params: tensor([  5.3665, -17.2977])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4599, Loss 2.9276504516601562\n",
      "\t Params: tensor([  5.3665, -17.2977])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4600, Loss 2.9276511669158936\n",
      "\t Params: tensor([  5.3665, -17.2977])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4601, Loss 2.9276504516601562\n",
      "\t Params: tensor([  5.3665, -17.2978])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4602, Loss 2.9276504516601562\n",
      "\t Params: tensor([  5.3665, -17.2978])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4603, Loss 2.927650213241577\n",
      "\t Params: tensor([  5.3665, -17.2978])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4604, Loss 2.9276511669158936\n",
      "\t Params: tensor([  5.3665, -17.2978])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4605, Loss 2.9276514053344727\n",
      "\t Params: tensor([  5.3665, -17.2978])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4606, Loss 2.9276506900787354\n",
      "\t Params: tensor([  5.3665, -17.2978])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4607, Loss 2.9276506900787354\n",
      "\t Params: tensor([  5.3665, -17.2978])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4608, Loss 2.9276514053344727\n",
      "\t Params: tensor([  5.3665, -17.2978])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4609, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3665, -17.2978])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4610, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3665, -17.2979])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4611, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3665, -17.2979])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4612, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3665, -17.2979])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4613, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3665, -17.2979])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4614, Loss 2.927650213241577\n",
      "\t Params: tensor([  5.3665, -17.2979])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4615, Loss 2.9276504516601562\n",
      "\t Params: tensor([  5.3665, -17.2979])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4616, Loss 2.9276504516601562\n",
      "\t Params: tensor([  5.3665, -17.2979])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4617, Loss 2.9276504516601562\n",
      "\t Params: tensor([  5.3665, -17.2979])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4618, Loss 2.9276511669158936\n",
      "\t Params: tensor([  5.3665, -17.2980])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4619, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3665, -17.2980])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4620, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3665, -17.2980])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4621, Loss 2.9276511669158936\n",
      "\t Params: tensor([  5.3665, -17.2980])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4622, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3665, -17.2980])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4623, Loss 2.9276506900787354\n",
      "\t Params: tensor([  5.3665, -17.2980])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4624, Loss 2.9276514053344727\n",
      "\t Params: tensor([  5.3665, -17.2980])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4625, Loss 2.9276506900787354\n",
      "\t Params: tensor([  5.3665, -17.2980])\n",
      "\t Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4626, Loss 2.9276511669158936\n",
      "\t Params: tensor([  5.3665, -17.2980])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4627, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3665, -17.2981])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4628, Loss 2.9276506900787354\n",
      "\t Params: tensor([  5.3665, -17.2981])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4629, Loss 2.9276506900787354\n",
      "\t Params: tensor([  5.3665, -17.2981])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4630, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3665, -17.2981])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4631, Loss 2.9276506900787354\n",
      "\t Params: tensor([  5.3665, -17.2981])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4632, Loss 2.9276506900787354\n",
      "\t Params: tensor([  5.3665, -17.2981])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4633, Loss 2.927650213241577\n",
      "\t Params: tensor([  5.3665, -17.2981])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4634, Loss 2.927650213241577\n",
      "\t Params: tensor([  5.3665, -17.2981])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4635, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3665, -17.2981])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4636, Loss 2.92764949798584\n",
      "\t Params: tensor([  5.3665, -17.2982])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4637, Loss 2.9276504516601562\n",
      "\t Params: tensor([  5.3665, -17.2982])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4638, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3666, -17.2982])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4639, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3666, -17.2982])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4640, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3666, -17.2982])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4641, Loss 2.92764949798584\n",
      "\t Params: tensor([  5.3666, -17.2982])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4642, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3666, -17.2982])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4643, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3666, -17.2982])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4644, Loss 2.92764949798584\n",
      "\t Params: tensor([  5.3666, -17.2982])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4645, Loss 2.92764949798584\n",
      "\t Params: tensor([  5.3666, -17.2983])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4646, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3666, -17.2983])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4647, Loss 2.9276504516601562\n",
      "\t Params: tensor([  5.3666, -17.2983])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4648, Loss 2.92764949798584\n",
      "\t Params: tensor([  5.3666, -17.2983])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4649, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3666, -17.2983])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4650, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3666, -17.2983])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4651, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3666, -17.2983])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4652, Loss 2.9276506900787354\n",
      "\t Params: tensor([  5.3666, -17.2983])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4653, Loss 2.927650213241577\n",
      "\t Params: tensor([  5.3666, -17.2984])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4654, Loss 2.9276504516601562\n",
      "\t Params: tensor([  5.3666, -17.2984])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4655, Loss 2.9276511669158936\n",
      "\t Params: tensor([  5.3666, -17.2984])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4656, Loss 2.927650213241577\n",
      "\t Params: tensor([  5.3666, -17.2984])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4657, Loss 2.9276514053344727\n",
      "\t Params: tensor([  5.3666, -17.2984])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4658, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3666, -17.2984])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4659, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3666, -17.2984])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4660, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3666, -17.2984])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4661, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3666, -17.2984])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4662, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3666, -17.2985])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4663, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3666, -17.2985])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4664, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3666, -17.2985])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4665, Loss 2.92764949798584\n",
      "\t Params: tensor([  5.3666, -17.2985])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4666, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3666, -17.2985])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4667, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3666, -17.2985])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4668, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3666, -17.2985])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4669, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3666, -17.2985])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4670, Loss 2.92764949798584\n",
      "\t Params: tensor([  5.3666, -17.2985])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4671, Loss 2.92764949798584\n",
      "\t Params: tensor([  5.3666, -17.2986])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4672, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3666, -17.2986])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4673, Loss 2.927650213241577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params: tensor([  5.3666, -17.2986])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4674, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3666, -17.2986])\n",
      "\t Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4675, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3666, -17.2986])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4676, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3666, -17.2986])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4677, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3666, -17.2986])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4678, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3666, -17.2986])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4679, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3666, -17.2986])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4680, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3666, -17.2987])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4681, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3666, -17.2987])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4682, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3666, -17.2987])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4683, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3666, -17.2987])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4684, Loss 2.9276504516601562\n",
      "\t Params: tensor([  5.3666, -17.2987])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4685, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3666, -17.2987])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4686, Loss 2.9276514053344727\n",
      "\t Params: tensor([  5.3666, -17.2987])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4687, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3666, -17.2987])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4688, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3666, -17.2987])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4689, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3667, -17.2987])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4690, Loss 2.9276504516601562\n",
      "\t Params: tensor([  5.3667, -17.2987])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4691, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3667, -17.2988])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4692, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3667, -17.2988])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4693, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3667, -17.2988])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4694, Loss 2.927650213241577\n",
      "\t Params: tensor([  5.3667, -17.2988])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4695, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3667, -17.2988])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4696, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3667, -17.2988])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4697, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3667, -17.2988])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4698, Loss 2.9276504516601562\n",
      "\t Params: tensor([  5.3667, -17.2988])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4699, Loss 2.92764949798584\n",
      "\t Params: tensor([  5.3667, -17.2988])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4700, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3667, -17.2988])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4701, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3667, -17.2989])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4702, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3667, -17.2989])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4703, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3667, -17.2989])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4704, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3667, -17.2989])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4705, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3667, -17.2989])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4706, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3667, -17.2989])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4707, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3667, -17.2989])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4708, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3667, -17.2989])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4709, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3667, -17.2989])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4710, Loss 2.927650213241577\n",
      "\t Params: tensor([  5.3667, -17.2989])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4711, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3667, -17.2989])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4712, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3667, -17.2990])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4713, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3667, -17.2990])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4714, Loss 2.9276504516601562\n",
      "\t Params: tensor([  5.3667, -17.2990])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4715, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3667, -17.2990])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4716, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3667, -17.2990])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4717, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3667, -17.2990])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4718, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3667, -17.2990])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4719, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3667, -17.2990])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4720, Loss 2.92764949798584\n",
      "\t Params: tensor([  5.3667, -17.2990])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4721, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3667, -17.2990])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4722, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3667, -17.2991])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4723, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3667, -17.2991])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4724, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3667, -17.2991])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4725, Loss 2.927650213241577\n",
      "\t Params: tensor([  5.3667, -17.2991])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4726, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3667, -17.2991])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4727, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3667, -17.2991])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4728, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3667, -17.2991])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4729, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3667, -17.2991])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4730, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3667, -17.2991])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4731, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3667, -17.2991])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4732, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3667, -17.2991])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4733, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3667, -17.2992])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4734, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3667, -17.2992])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4735, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3667, -17.2992])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4736, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3667, -17.2992])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4737, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3667, -17.2992])\n",
      "\t Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4738, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3667, -17.2992])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4739, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3667, -17.2992])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4740, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3667, -17.2992])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4741, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3667, -17.2992])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4742, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3667, -17.2992])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4743, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3667, -17.2993])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4744, Loss 2.92764949798584\n",
      "\t Params: tensor([  5.3667, -17.2993])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4745, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3667, -17.2993])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4746, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3667, -17.2993])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4747, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3667, -17.2993])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4748, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3667, -17.2993])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4749, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3668, -17.2993])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4750, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3668, -17.2993])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4751, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3668, -17.2993])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4752, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3668, -17.2993])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4753, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3668, -17.2993])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4754, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3668, -17.2994])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4755, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3668, -17.2994])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4756, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3668, -17.2994])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4757, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3668, -17.2994])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4758, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3668, -17.2994])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4759, Loss 2.92764949798584\n",
      "\t Params: tensor([  5.3668, -17.2994])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4760, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3668, -17.2994])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4761, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3668, -17.2994])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4762, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3668, -17.2994])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4763, Loss 2.9276468753814697\n",
      "\t Params: tensor([  5.3668, -17.2994])\n",
      "\t Grad: tensor([-0.0001,  0.0009])\n",
      "Epoch 4764, Loss 2.92764949798584\n",
      "\t Params: tensor([  5.3668, -17.2995])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4765, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3668, -17.2995])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4766, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3668, -17.2995])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4767, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3668, -17.2995])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4768, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3668, -17.2995])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4769, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3668, -17.2995])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4770, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3668, -17.2995])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4771, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3668, -17.2995])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4772, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3668, -17.2995])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4773, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3668, -17.2995])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4774, Loss 2.927650213241577\n",
      "\t Params: tensor([  5.3668, -17.2995])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4775, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3668, -17.2996])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4776, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3668, -17.2996])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4777, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3668, -17.2996])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4778, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3668, -17.2996])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4779, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3668, -17.2996])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4780, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3668, -17.2996])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4781, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3668, -17.2996])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4782, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3668, -17.2996])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4783, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3668, -17.2996])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4784, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3668, -17.2996])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4785, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3668, -17.2997])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4786, Loss 2.927650213241577\n",
      "\t Params: tensor([  5.3668, -17.2997])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4787, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3668, -17.2997])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4788, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3668, -17.2997])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4789, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3668, -17.2997])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4790, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3668, -17.2997])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4791, Loss 2.9276468753814697\n",
      "\t Params: tensor([  5.3668, -17.2997])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4792, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3668, -17.2997])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4793, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3668, -17.2997])\n",
      "\t Grad: tensor([-0.0001,  0.0009])\n",
      "Epoch 4794, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3668, -17.2997])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4795, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3668, -17.2997])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4796, Loss 2.9276483058929443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Params: tensor([  5.3668, -17.2997])\n",
      "\t Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4797, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3668, -17.2998])\n",
      "\t Grad: tensor([-0.0001,  0.0009])\n",
      "Epoch 4798, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3668, -17.2998])\n",
      "\t Grad: tensor([-0.0001,  0.0009])\n",
      "Epoch 4799, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3668, -17.2998])\n",
      "\t Grad: tensor([-0.0001,  0.0009])\n",
      "Epoch 4800, Loss 2.9276463985443115\n",
      "\t Params: tensor([  5.3668, -17.2998])\n",
      "\t Grad: tensor([-0.0001,  0.0009])\n",
      "Epoch 4801, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3668, -17.2998])\n",
      "\t Grad: tensor([-0.0001,  0.0009])\n",
      "Epoch 4802, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3668, -17.2998])\n",
      "\t Grad: tensor([-0.0001,  0.0009])\n",
      "Epoch 4803, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3668, -17.2998])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4804, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3668, -17.2998])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4805, Loss 2.9276466369628906\n",
      "\t Params: tensor([  5.3668, -17.2998])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4806, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3668, -17.2998])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4807, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3668, -17.2998])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4808, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3668, -17.2998])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4809, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3668, -17.2998])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4810, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3668, -17.2999])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4811, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3668, -17.2999])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4812, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3669, -17.2999])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4813, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3669, -17.2999])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4814, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3669, -17.2999])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4815, Loss 2.9276466369628906\n",
      "\t Params: tensor([  5.3669, -17.2999])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4816, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3669, -17.2999])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4817, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3669, -17.2999])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4818, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3669, -17.2999])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4819, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3669, -17.2999])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4820, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3669, -17.2999])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4821, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3669, -17.2999])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4822, Loss 2.9276468753814697\n",
      "\t Params: tensor([  5.3669, -17.2999])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4823, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3669, -17.3000])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4824, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3669, -17.3000])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4825, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3669, -17.3000])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4826, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3669, -17.3000])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4827, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3669, -17.3000])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4828, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3669, -17.3000])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4829, Loss 2.9276468753814697\n",
      "\t Params: tensor([  5.3669, -17.3000])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4830, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3669, -17.3000])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4831, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3669, -17.3000])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4832, Loss 2.9276463985443115\n",
      "\t Params: tensor([  5.3669, -17.3000])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4833, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3669, -17.3000])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4834, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3669, -17.3000])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4835, Loss 2.9276468753814697\n",
      "\t Params: tensor([  5.3669, -17.3000])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4836, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3669, -17.3001])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4837, Loss 2.9276468753814697\n",
      "\t Params: tensor([  5.3669, -17.3001])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4838, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3669, -17.3001])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4839, Loss 2.9276468753814697\n",
      "\t Params: tensor([  5.3669, -17.3001])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4840, Loss 2.927649736404419\n",
      "\t Params: tensor([  5.3669, -17.3001])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4841, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3669, -17.3001])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4842, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3669, -17.3001])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4843, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3669, -17.3001])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4844, Loss 2.9276468753814697\n",
      "\t Params: tensor([  5.3669, -17.3001])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4845, Loss 2.9276468753814697\n",
      "\t Params: tensor([  5.3669, -17.3001])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4846, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3669, -17.3001])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4847, Loss 2.9276468753814697\n",
      "\t Params: tensor([  5.3669, -17.3001])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4848, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3669, -17.3001])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4849, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3669, -17.3002])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4850, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3669, -17.3002])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4851, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3669, -17.3002])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4852, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3669, -17.3002])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4853, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3669, -17.3002])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4854, Loss 2.9276468753814697\n",
      "\t Params: tensor([  5.3669, -17.3002])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4855, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3669, -17.3002])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4856, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3669, -17.3002])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4857, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3669, -17.3002])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4858, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3669, -17.3002])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4859, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3669, -17.3002])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4860, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3669, -17.3002])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4861, Loss 2.927645206451416\n",
      "\t Params: tensor([  5.3669, -17.3002])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4862, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3669, -17.3003])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4863, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3669, -17.3003])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4864, Loss 2.9276466369628906\n",
      "\t Params: tensor([  5.3669, -17.3003])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4865, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3669, -17.3003])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4866, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3669, -17.3003])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4867, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3669, -17.3003])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4868, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3669, -17.3003])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4869, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3669, -17.3003])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4870, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3669, -17.3003])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4871, Loss 2.9276463985443115\n",
      "\t Params: tensor([  5.3669, -17.3003])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4872, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3669, -17.3003])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4873, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3669, -17.3003])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4874, Loss 2.9276468753814697\n",
      "\t Params: tensor([  5.3669, -17.3003])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4875, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3669, -17.3004])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4876, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3669, -17.3004])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4877, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3669, -17.3004])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4878, Loss 2.9276468753814697\n",
      "\t Params: tensor([  5.3669, -17.3004])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4879, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3669, -17.3004])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4880, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3669, -17.3004])\n",
      "\t Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4881, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3669, -17.3004])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4882, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3669, -17.3004])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4883, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3669, -17.3004])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4884, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3669, -17.3004])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4885, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3669, -17.3004])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4886, Loss 2.9276468753814697\n",
      "\t Params: tensor([  5.3670, -17.3004])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4887, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3670, -17.3004])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4888, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3670, -17.3005])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4889, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3670, -17.3005])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4890, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3670, -17.3005])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4891, Loss 2.9276466369628906\n",
      "\t Params: tensor([  5.3670, -17.3005])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4892, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3670, -17.3005])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4893, Loss 2.9276459217071533\n",
      "\t Params: tensor([  5.3670, -17.3005])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4894, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3670, -17.3005])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4895, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3670, -17.3005])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4896, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3670, -17.3005])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4897, Loss 2.9276468753814697\n",
      "\t Params: tensor([  5.3670, -17.3005])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4898, Loss 2.9276466369628906\n",
      "\t Params: tensor([  5.3670, -17.3005])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4899, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3670, -17.3005])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4900, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3670, -17.3005])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4901, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3670, -17.3006])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4902, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3670, -17.3006])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4903, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3670, -17.3006])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4904, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3670, -17.3006])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4905, Loss 2.9276459217071533\n",
      "\t Params: tensor([  5.3670, -17.3006])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4906, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3670, -17.3006])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4907, Loss 2.9276463985443115\n",
      "\t Params: tensor([  5.3670, -17.3006])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4908, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3670, -17.3006])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4909, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3670, -17.3006])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4910, Loss 2.9276466369628906\n",
      "\t Params: tensor([  5.3670, -17.3006])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4911, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3670, -17.3006])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4912, Loss 2.9276466369628906\n",
      "\t Params: tensor([  5.3670, -17.3006])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4913, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3670, -17.3006])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4914, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3670, -17.3006])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4915, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3670, -17.3007])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4916, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3670, -17.3007])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4917, Loss 2.9276490211486816\n",
      "\t Params: tensor([  5.3670, -17.3007])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4918, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3670, -17.3007])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4919, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3670, -17.3007])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4920, Loss 2.9276466369628906\n",
      "\t Params: tensor([  5.3670, -17.3007])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4921, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3670, -17.3007])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4922, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3670, -17.3007])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4923, Loss 2.9276468753814697\n",
      "\t Params: tensor([  5.3670, -17.3007])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4924, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3670, -17.3007])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4925, Loss 2.9276459217071533\n",
      "\t Params: tensor([  5.3670, -17.3007])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4926, Loss 2.9276466369628906\n",
      "\t Params: tensor([  5.3670, -17.3007])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4927, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3670, -17.3007])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4928, Loss 2.9276468753814697\n",
      "\t Params: tensor([  5.3670, -17.3008])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4929, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3670, -17.3008])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4930, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3670, -17.3008])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4931, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3670, -17.3008])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4932, Loss 2.9276463985443115\n",
      "\t Params: tensor([  5.3670, -17.3008])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4933, Loss 2.92764949798584\n",
      "\t Params: tensor([  5.3670, -17.3008])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4934, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3670, -17.3008])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4935, Loss 2.9276463985443115\n",
      "\t Params: tensor([  5.3670, -17.3008])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4936, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3670, -17.3008])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4937, Loss 2.9276463985443115\n",
      "\t Params: tensor([  5.3670, -17.3008])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4938, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3670, -17.3008])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4939, Loss 2.9276463985443115\n",
      "\t Params: tensor([  5.3670, -17.3008])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4940, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3670, -17.3008])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4941, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3670, -17.3009])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4942, Loss 2.9276463985443115\n",
      "\t Params: tensor([  5.3670, -17.3009])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4943, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3670, -17.3009])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4944, Loss 2.9276463985443115\n",
      "\t Params: tensor([  5.3670, -17.3009])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4945, Loss 2.9276468753814697\n",
      "\t Params: tensor([  5.3670, -17.3009])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4946, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3670, -17.3009])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4947, Loss 2.9276487827301025\n",
      "\t Params: tensor([  5.3670, -17.3009])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4948, Loss 2.9276459217071533\n",
      "\t Params: tensor([  5.3670, -17.3009])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4949, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3670, -17.3009])\n",
      "\t Grad: tensor([-9.9361e-05,  6.6355e-04])\n",
      "Epoch 4950, Loss 2.9276463985443115\n",
      "\t Params: tensor([  5.3670, -17.3009])\n",
      "\t Grad: tensor([-9.5367e-05,  6.6292e-04])\n",
      "Epoch 4951, Loss 2.9276463985443115\n",
      "\t Params: tensor([  5.3670, -17.3009])\n",
      "\t Grad: tensor([-9.6858e-05,  6.6188e-04])\n",
      "Epoch 4952, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3670, -17.3009])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4953, Loss 2.9276466369628906\n",
      "\t Params: tensor([  5.3670, -17.3009])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4954, Loss 2.9276466369628906\n",
      "\t Params: tensor([  5.3670, -17.3009])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4955, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3670, -17.3009])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4956, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3670, -17.3009])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4957, Loss 2.927645683288574\n",
      "\t Params: tensor([  5.3670, -17.3009])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4958, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3670, -17.3010])\n",
      "\t Grad: tensor([-9.8228e-05,  6.5479e-04])\n",
      "Epoch 4959, Loss 2.9276468753814697\n",
      "\t Params: tensor([  5.3670, -17.3010])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4960, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3670, -17.3010])\n",
      "\t Grad: tensor([-9.8288e-05,  6.5270e-04])\n",
      "Epoch 4961, Loss 2.9276463985443115\n",
      "\t Params: tensor([  5.3670, -17.3010])\n",
      "\t Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4962, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3670, -17.3010])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4963, Loss 2.9276468753814697\n",
      "\t Params: tensor([  5.3670, -17.3010])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4964, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3670, -17.3010])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4965, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3670, -17.3010])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4966, Loss 2.927645683288574\n",
      "\t Params: tensor([  5.3671, -17.3010])\n",
      "\t Grad: tensor([-9.7990e-05,  6.4683e-04])\n",
      "Epoch 4967, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3671, -17.3010])\n",
      "\t Grad: tensor([-9.7573e-05,  6.4588e-04])\n",
      "Epoch 4968, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3671, -17.3010])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4969, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3671, -17.3010])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4970, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3671, -17.3010])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4971, Loss 2.9276466369628906\n",
      "\t Params: tensor([  5.3671, -17.3010])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4972, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3671, -17.3010])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4973, Loss 2.9276463985443115\n",
      "\t Params: tensor([  5.3671, -17.3010])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4974, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3671, -17.3010])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4975, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3671, -17.3011])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4976, Loss 2.9276468753814697\n",
      "\t Params: tensor([  5.3671, -17.3011])\n",
      "\t Grad: tensor([-9.5606e-05,  6.3694e-04])\n",
      "Epoch 4977, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3671, -17.3011])\n",
      "\t Grad: tensor([-9.8169e-05,  6.3545e-04])\n",
      "Epoch 4978, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3671, -17.3011])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4979, Loss 2.9276466369628906\n",
      "\t Params: tensor([  5.3671, -17.3011])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4980, Loss 2.9276463985443115\n",
      "\t Params: tensor([  5.3671, -17.3011])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4981, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3671, -17.3011])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4982, Loss 2.9276466369628906\n",
      "\t Params: tensor([  5.3671, -17.3011])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4983, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3671, -17.3011])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4984, Loss 2.927645683288574\n",
      "\t Params: tensor([  5.3671, -17.3011])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4985, Loss 2.9276480674743652\n",
      "\t Params: tensor([  5.3671, -17.3011])\n",
      "\t Grad: tensor([-9.6440e-05,  6.2808e-04])\n",
      "Epoch 4986, Loss 2.9276466369628906\n",
      "\t Params: tensor([  5.3671, -17.3011])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4987, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3671, -17.3011])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4988, Loss 2.9276466369628906\n",
      "\t Params: tensor([  5.3671, -17.3011])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4989, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3671, -17.3011])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4990, Loss 2.9276463985443115\n",
      "\t Params: tensor([  5.3671, -17.3011])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4991, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3671, -17.3011])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4992, Loss 2.9276468753814697\n",
      "\t Params: tensor([  5.3671, -17.3011])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4993, Loss 2.927645683288574\n",
      "\t Params: tensor([  5.3671, -17.3012])\n",
      "\t Grad: tensor([-9.2626e-05,  6.2042e-04])\n",
      "Epoch 4994, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3671, -17.3012])\n",
      "\t Grad: tensor([-9.8884e-05,  6.1837e-04])\n",
      "Epoch 4995, Loss 2.9276483058929443\n",
      "\t Params: tensor([  5.3671, -17.3012])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4996, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3671, -17.3012])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4997, Loss 2.9276468753814697\n",
      "\t Params: tensor([  5.3671, -17.3012])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4998, Loss 2.927647352218628\n",
      "\t Params: tensor([  5.3671, -17.3012])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4999, Loss 2.927647590637207\n",
      "\t Params: tensor([  5.3671, -17.3012])\n",
      "\t Grad: tensor([-0.0001,  0.0006])\n"
     ]
    }
   ],
   "source": [
    "params = training_loop(5000, 1e-2, torch.tensor([1.0, 0.0]), t_un, t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the values for w and b represent those to convert Celsius to Fahrenheit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x294cbd5d3c8>]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t_p = model(t_un, *params) # make sure to use normalized data to predict as that's what model was trained on\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"Temperature (Fahrenheit)\")\n",
    "ax.set_ylabel(\"Temperature (Celsius)\")\n",
    "ax.plot(t_u.numpy(), t_c.numpy(), 'o') # raw values\n",
    "ax.plot(t_u.numpy(), t_p.detach().numpy()) # fitted values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch's Autograd\n",
    "PyTorch remembers operations about tensors that lets it provide the chain of derivatives of those operations with respect to their inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True) \n",
    "# track entire family tree of tensors resulting from operations on params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.grad is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4517.2969,   82.6000])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(model(t_u, *params), t_c)\n",
    "loss.backward()\n",
    "params.grad # contains derivative of loss wrt each param, i.e., the gradient of the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that calling backward will lead derivatives to accumulate at leaf nodes. We need to *zero the gradient* after using it for parameter updates so it doesn't add on top of itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.grad is not None:\n",
    "    params.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "            \n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        loss.backward()\n",
    "        \n",
    "        # autograd mechanism should \"look away\" meaning it won't add edges to the forward graph\n",
    "        with torch.no_grad():\n",
    "            params -= learning_rate * params.grad\n",
    "        \n",
    "        if epoch % 500 == 0:\n",
    "            print(f'Epoch {epoch}, Loss {float(loss)}')\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860115051269531\n",
      "Epoch 1000, Loss 3.828537940979004\n",
      "Epoch 1500, Loss 3.092191219329834\n",
      "Epoch 2000, Loss 2.957697868347168\n",
      "Epoch 2500, Loss 2.933133840560913\n",
      "Epoch 3000, Loss 2.9286484718322754\n",
      "Epoch 3500, Loss 2.9278297424316406\n",
      "Epoch 4000, Loss 2.9276793003082275\n",
      "Epoch 4500, Loss 2.927651882171631\n",
      "Epoch 5000, Loss 2.9276468753814697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = training_loop(5000, 1e-2, torch.tensor([1.0, 0.0], requires_grad=True), t_un, t_c)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use built-in optimization strategy instead of training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'NAdam',\n",
       " 'Optimizer',\n",
       " 'RAdam',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_functional',\n",
       " '_multi_tensor',\n",
       " 'lr_scheduler',\n",
       " 'swa_utils']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "dir(optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.SGD([params], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.5483e-01, -8.2600e-04], requires_grad=True)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p = model(t_u, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step() # automatically applied SGD\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now replace entire training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 500 == 0:\n",
    "            print(f'Epoch {epoch}, Loss {float(loss)}')\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860115051269531\n",
      "Epoch 1000, Loss 3.828537940979004\n",
      "Epoch 1500, Loss 3.092191219329834\n",
      "Epoch 2000, Loss 2.957697868347168\n",
      "Epoch 2500, Loss 2.933133840560913\n",
      "Epoch 3000, Loss 2.9286484718322754\n",
      "Epoch 3500, Loss 2.9278297424316406\n",
      "Epoch 4000, Loss 2.9276793003082275\n",
      "Epoch 4500, Loss 2.927651882171631\n",
      "Epoch 5000, Loss 2.9276468753814697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "params = training_loop(5000, optimizer, params, t_un, t_c)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use different optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.612900257110596\n",
      "Epoch 1000, Loss 3.086700439453125\n",
      "Epoch 1500, Loss 2.928579092025757\n",
      "Epoch 2000, Loss 2.9276442527770996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  0.5367, -17.3021], requires_grad=True)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-1\n",
    "optimizer = optim.Adam([params], lr=learning_rate)\n",
    "# can use original t_u as learning rate set adaptively and less sensitive to scaling of params\n",
    "params = training_loop(2000, optimizer, params, t_u, t_c)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3,  9,  6,  4,  2,  0, 10,  1,  8]), tensor([5, 7]))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples) # random permutation of ints from 0 to n_samples - 1\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t_u = t_u[train_indices]\n",
    "train_t_c = t_c[train_indices]\n",
    "\n",
    "val_t_u = t_u[val_indices]\n",
    "val_t_c = t_c[val_indices]\n",
    "\n",
    "train_t_un = 0.1 * train_t_u\n",
    "val_t_un = 0.1 * val_t_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define new training loop with validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        loss = loss_fn(train_t_p, train_t_c)\n",
    "        \n",
    "        with torch.no_grad(): # do not track autograd graph on validation tensor\n",
    "            val_t_p = model(val_t_u, *params)\n",
    "            val_loss = loss_fn(val_t_p, val_t_c)\n",
    "            assert val_loss.requires_grad == False\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch <= 3 or epoch % 500 == 0:\n",
    "            print(f'Epoch {epoch}, Loss {float(loss)}')\n",
    "            print(f'\\tValidation loss {val_loss:.4f}')\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 92.90481567382812\n",
      "Validation loss 23.9323\n",
      "Epoch 2, Loss 32.46635818481445\n",
      "Validation loss 36.4383\n",
      "Epoch 3, Loss 25.994890213012695\n",
      "Validation loss 46.3535\n",
      "Epoch 500, Loss 9.8406400680542\n",
      "Validation loss 11.4031\n",
      "Epoch 1000, Loss 5.113063335418701\n",
      "Validation loss 2.0225\n",
      "Epoch 1500, Loss 3.6757562160491943\n",
      "Validation loss 0.9233\n",
      "Epoch 2000, Loss 3.238779067993164\n",
      "Validation loss 1.5555\n",
      "Epoch 2500, Loss 3.1059255599975586\n",
      "Validation loss 2.2806\n",
      "Epoch 3000, Loss 3.0655362606048584\n",
      "Validation loss 2.7949\n"
     ]
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "params = training_loop(3000, optimizer, params, train_t_un, val_t_un, train_t_c, val_t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  5.5415, -18.3099], requires_grad=True)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "1. Note the loss function stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_model(t_u, w2, w1, b):\n",
    "    return w2 * t_u ** 2 + w1 * t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_training_loop(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = new_model(train_t_u, *params)\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_t_p = new_model(val_t_u, *params)\n",
    "            val_loss = loss_fn(val_t_p, val_t_c)\n",
    "            assert val_loss.requires_grad == False\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch <= 3 or epoch % 500 == 0:\n",
    "            print(f'Epoch {epoch}, Loss {float(train_loss)}')\n",
    "            print(f'\\tValidation loss {val_loss:.4f}')\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 764.6103515625\n",
      "\tValidation loss 276.1225\n",
      "Epoch 2, Loss 409.0584716796875\n",
      "\tValidation loss 174.7018\n",
      "Epoch 3, Loss 221.0795440673828\n",
      "\tValidation loss 117.0025\n",
      "Epoch 500, Loss 8.26038646697998\n",
      "\tValidation loss 24.7391\n",
      "Epoch 1000, Loss 6.830669403076172\n",
      "\tValidation loss 21.1843\n",
      "Epoch 1500, Loss 5.794828414916992\n",
      "\tValidation loss 18.4018\n",
      "Epoch 2000, Loss 5.044313430786133\n",
      "\tValidation loss 16.2094\n",
      "Epoch 2500, Loss 4.500488758087158\n",
      "\tValidation loss 14.4706\n",
      "Epoch 3000, Loss 4.106388092041016\n",
      "\tValidation loss 13.0826\n",
      "Epoch 3500, Loss 3.820749044418335\n",
      "\tValidation loss 11.9676\n",
      "Epoch 4000, Loss 3.613677501678467\n",
      "\tValidation loss 11.0664\n",
      "Epoch 4500, Loss 3.46352481842041\n",
      "\tValidation loss 10.3339\n",
      "Epoch 5000, Loss 3.354599952697754\n",
      "\tValidation loss 9.7351\n",
      "Epoch 5500, Loss 3.2755422592163086\n",
      "\tValidation loss 9.2431\n",
      "Epoch 6000, Loss 3.2181215286254883\n",
      "\tValidation loss 8.8369\n",
      "Epoch 6500, Loss 3.17637300491333\n",
      "\tValidation loss 8.5000\n",
      "Epoch 7000, Loss 3.1459782123565674\n",
      "\tValidation loss 8.2195\n",
      "Epoch 7500, Loss 3.1238086223602295\n",
      "\tValidation loss 7.9849\n",
      "Epoch 8000, Loss 3.1075990200042725\n",
      "\tValidation loss 7.7880\n",
      "Epoch 8500, Loss 3.095702886581421\n",
      "\tValidation loss 7.6222\n",
      "Epoch 9000, Loss 3.0869364738464355\n",
      "\tValidation loss 7.4821\n",
      "Epoch 9500, Loss 3.0804312229156494\n",
      "\tValidation loss 7.3633\n",
      "Epoch 10000, Loss 3.0755701065063477\n",
      "\tValidation loss 7.2623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5673, -0.9551, -0.8040], requires_grad=True)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True) # holds [w2, w1, b]\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "params = new_training_loop(10000, optimizer, params, train_t_un, val_t_un, train_t_c, val_t_c)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the validation loss is higher, here the result is worse with the more complex model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learn",
   "language": "python",
   "name": "deep_learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
